##### Improving Language Understanding by Generative Pre-Training
# 1 Introduction
#### GPT의 전략
1.  unsupervised pre-training -> language modeling objective 사용, 신경망 초기 파라미터 학습
2. supervised fine-tuning -> 이후 각 task의 supervised objective에 맞게 fine-tuning
> language modeling ?
> 1. Causal Language Modeling 
> 	$k$ 개의 단어가 주어졌을 때, 그 다음 단어는?
> 	입력: `w₁, w₂, ..., wₖ`
> 	출력: `P(wₖ₊₁ | w₁, ..., wₖ)`
> 2. Masked Language Modeling
> 	입력: `w₁, w₂, [MASK], w₄`
> 	출력: `[MASK]`에 들어갈 단어 예측 → `P(w₃ | w₁, w₂, w₄)`

#### GPT의 목표
적은 수정으로 다양한 과제에 전이할 수 있는 보편적인 표현을 학습하기
(Our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks.)

#### GPT의 가정
대규모 unlabeled text와, 수동으로 라벨링된 데이터를 사용할 수 있다고 가정.

#### GPT의 구조
Transformer Decoder를 사용
전이 학습을 할 때, task-specific input adaptation을 활용.
> task-specific input adaptation ?
> : 구조화된 입력을 하나의 연속된 토큰 시퀀스로 처리
> : 아래 Figure 1 오른쪽 그림 참고
> : 기존 모델들은, task 별로 모델을 설계하여 task가 바뀌면 모델도 바뀌었음.
> 하지만 GPT는 input만 바꿔서 처리하도록 함.

#### GPT의 평가
Natural Language Inference (NLI), 두 문장의 논리 관계를 판단. 전제 -> 가설 판단
Question Answering
Semantic Similarity, 문장 유사도 평가.
Text classification

4가지의 task로 평가. -> GPT-1이 다양한 task에 전이 가능한지 확인
매우 잘했다고 합니다~

# 2 Related Work
사전 연구들

# 3 Framework
1. 엄청 많은 텍스트 데이터를 가지고 언어 모델을 먼저 학습
2. Fine-tuning, labeled data를 가지고 모델을 특정 태스크에 맞게 조정

## 3.1 Unsupervised pre-training
unsupervised corpus of token $\mathcal{U} = \{ u_1, \ldots, u_n \}$이 주어졌을 때,
아래 Likelihood를 최대화하는 방향으로 표준 언어 모델을 학습.
> 이때 토큰은, **Byte Pair Encoding**으로 토크나이징 함. 

$$
L_1(\mathcal{U}) = \sum_i \log P(u_i \mid u_{i-k}, \ldots, u_{i-1}; \Theta)
$$이때 $k$ : context window의 크기, $\Theta$는 신경망의 파리미터(SGD로 학습). 
> 앞의 $k$개의 토큰을 보고, 다음 토큰을 예측하는 신경망.
> GPT-1에서는, $k=512$. 즉, 각 토큰을 예측할 때 앞의 512개의 토큰만 봄.
> 이후 2, 3, 4로 넘어가면서 확장. GPT-4o는 128k개의 토큰을 봄.

언어 모델로는 **multi-layer Transformer decoder** 구조를 사용.
$$
\begin{aligned}
h_0 &= U W_e + W_p \\
h_i &= \mathrm{transformer\_block}(h_{i-1}) \quad \forall i \in [1, n] \\
P(u) &= \mathrm{softmax}(h_n W_e^{\mathsf{T}})
\end{aligned}
$$
	$U=(u_{-k},...,u_{-1})$ : 예측하고자 하는 토큰 앞에 있는 토큰들.
	$n$은 layer의 수 ($n$개의 transformer layer)
	$W_{e}$는 token embedding matrix
	$W_{p}$는 position embedding matrix
	아래 Figure 1 왼쪽 그림 참고

#### Transformer Decoder(transformer_block)의 구조
input context 
-> multi-headed self-attention
-> position-wise feedforward layers
> self-attention을 통과한 각 토큰 벡터마다, 똑같은 FFN이 독립적으로 적용,

## 3.2 Supervised fine-tuning
labeled dataset $\mathcal{C}$ : 각 데이터는 **input tokens의 sequence $x^{1},...,x^{m}$** 와 **label $y$** 로 구성.

이 데이터셋의 input들은, 우리가 3.1에서 훈련 시켜놓은 **pre-trained model에 입력**되어서,
**the final transformer block's activation $h^{m}_{l}$** 을 얻음.
그리고나서 아래와 같이 $y$를 예측하는 과정을 거침.
$$
P(y \mid x^1, \ldots, x^m) = \mathrm{softmax}(h_\ell^m W_y)
$$

따라서 자연스럽게, objective function은 아래와 같음.
$$
L_2(\mathcal{C}) = \sum_{(x, y)} \log P(y \mid x^1, \ldots, x^m)
$$
#### objective function의 개선
파인튜닝 과정에서, 언어 모델링을 보조 목적함수로 추가하면 학습에 도움이 됨.
왜냐하면,
(a) supervised model의 일반화 성능 향상
(b) 수렴 속도 향상

따라서 최종적으로 weight $\lambda$가 들어간 objective function을 아래와 같이 작성.
$$L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda * L_{1}(\mathcal{C})$$

#### 파인튜닝 과정에서 추가적으로 생기는 파라미터는,
1. $W_{y}$
2. delimiter token에 대한 embedding

![[Figure 1.png]]
## 3.3 Task-specific input transformations
text classification를 포함한 일부 task는, 바로 파인 튜닝을 할 수 있음.
하지만 **질의응답**이나, **텍스트 함의(Entailment, 전제를 주고 가설 확인)** 처럼 **정해진 구조를 가진 입력**을 필요로 하는 작업이 있음. ex) 문장이 순서대로 짝지어진 입력, 문서-질문-정답 순으로 구성된 입력
우리의 pre-trained model은, 연속된 텍스트 시퀀스만 학습하였기에, 위와 같은 작업에 적용하려면 수정이 필요.
> 기존 방법에서는, task 별로 아키텍쳐를 얻는 방법을 제안하였음.
> 하지만 이 방법은,
> 1. 작업마다 구조를 만들어야함.
> 2. 그 추가 구조에는 전이학습이 안됨.

논문에서는 **traversal-style 접근법**을 제안
### Traversal-style Approach
: **정해진 구조를 가진 입력을 pre-trained model이 처리할 수 있는 순서 있는 시퀀스로 변환.**
: [[1. 대학원/사전준비/Improving Language Understanding by Generative Pre-Training/images/Figure 1.png|Figure 1]] 오른쪽 그림 참고. 다양한 상황에 대한 traversal-style 입력을 정리
: ex) Textual Entailment
- 전제(Premise): "A man is eating pizza."
- 가설(Hypothesis): "Someone is having food."
- traversal-style 입력
	`[s] A man is eating pizza. [SEP] Someone is having food. [e]`
	`[s]` : 문장의 시작을 알리는 토큰 (randomly initialized)
	`[SEP]` : 구분자 토큰
	`[e]` : 문장의 끝을 알리는 토큰 (randomly initialized)
	**이렇게 입력을 바꿔주면, 일반적인 언어 모델 입력처럼 다룰 수 있음.**
	*근데 이렇게 하면, 모델이 이 입력이 어떤 task인지 확인이 불가능하지 않나?*

#### 예제 1: Textual entailment
전제문 $p$와 가설문 $h$ 사이에 delimiter `$` 를 넣는다.
#### 예제 2: Similarity
비교되는 두 문장 사이에 정해진 순서가 없음.
따라서 두 가지 가능한 문장 순서를 모두 포함한 입력을 만듦(두 문장 사이는 delimiter로 구분).
이 두 입력을 각각 모델에 넣어서 두 개의 $h^{m}_{l}$을 만들고,
이들을 element-wise sum을 한 후 linear layer 통과
#### 예제 3: Question Answering and Commonsense Reasoning
이 task에서는, 문서 $z$,  질문 $q$, 가능한 답변들 $\{a_{k}\}$가 주어짐.
각 정답 후보에 대해, 문서와 질문을 먼저 붙이고, 이후 구분자 `$`를 이용해 $[z;q;\$;a_{k}]$ 처럼 입력을 만듦.
이 입력들을 각각 모델에 넣어 출력을 구하고,
softmax 정규화를 통해 여러 답변의 확률 분포를 만듦.

# 4 Experiments
## 4.1 Setup
### Unsupervised pre-training
**BooksCorpus** Dataset 사용
https://paperswithcode.com/dataset/bookcorpus
(논문 기준) 모험, 판타지, 로맨스 등 다양한 분야에서 미출판된 7,000개 이상의 책 데이터.
또한, 이 데이터에는 긴 길이의 연속적인 문장 구조를 포함하고 있음. 따라서 모델이 long-range information을 학습할 수 있음.
> BookCorpus 데이터셋에서, 토큰 수준의 **perplexity 18.4**로 매우 낮은 값 달성.
#### perplexity ?
$$
\text{Perplexity} = \exp\left( -\frac{1}{N} \sum_{i=1}^{N} \log P(u_i \mid u_1, \ldots, u_{i-1}) \right)
$$
낮을 수록 더 나은 예측성능.

**1B Word Benchmark** Dataset이, 위 데이터셋의 대안으로 제시되지만, 이 데이터 셋은 문장 단위로 섞여있어서 long-range sturcture를 파괴함.

### Model Specifications
| 항목                    | 설명                                                                                                               |
| --------------------- | ---------------------------------------------------------------------------------------------------------------- |
| 모델 구조                 | **12-layer decoder-only Transformer**<br>**임베딩 차원 768** (768 dimensional states)<br>**12개의 Attention heads**     |
| position-wise FFN     | **3072** 차원<br>*(아마, 768 -> 3072 -> 768 일 듯?)*                                                                   |
| Activation            | **GELU** (Gaussian Error Linear Unit)<br>*GELU?*                                                                 |
| Optimization          | **Adam**, <br>첫 2000 step, **학습률** 0 -> **2.5e-4** linearly 증가 (**warm-up**)<br>이후 **cosine annealing** -> 0에 수렴 |
| Epoch                 | **100**                                                                                                          |
| 입력 시퀀스                | **minibatch size 64** (randomly sampled)<br>길이 512짜리 연속 토큰 (**contiguous sequence of 512 tokens**)               |
| Weight initialization | $\mathcal{N}(0, 0.02)$ 으로도 충분. (왜냐하면 layernorm을 하기 때문)                                                           |
| Tokenization          | **BPE**(Byte Pair Encoding), **40K** merges<br>*BPE?*                                                            |
| Dropout               | residual, embedding, attention 모두 dropout = 0.1                                                                  |
| 정규화                   | **LayerNorm**<br>modified version L2 정규화($w=0.01$) bias, gain weight 제외<br>*modified version of L2 Norm이 뭐지?*    |
| 위치 임베딩                | sinusoidal 대신 **학습 가능한 learned positional embedding**                                                            |
| 텍스트 정제                | `ftfy` 라이브러리로 문자 오류 수정<br>(standardize some punctuation and whitespace)<br>`spaCy` tokenizer로 토큰화                |

### Fine-tuning Details
기본적으로는, pre-training 과정에서 사용한 하이퍼파라미터 설정을 그대로 사용.
#### Classifier 에서는, 
**dropout(비율 0.1)** 적용

#### 대부분의 작업에서는,
**학습률 6.25e-5**
**minibatch size 32**

파인튜닝이 굉장히 빨라서, 3번의 epoch 만으로도 충분.

#### learning rate scheduling
**linear decay** 적용.
**첫 0.2%만 warm-up**에 사용

Loss 함수의 가중치 $\lambda$는 0.5로 설정

## 4.2 Supervised Fine-tuning
![[Pasted image 20250608015057.png]]
### Natural Language Inference (NLI, 자연어 함의)
문장 쌍을 읽고, 그들의 관계가 `entailment`, `contradiction`, `neutral` 중 무엇인지 확인
> `entailment` : 전제, 가설 쌍에서, 전제 문장이 참이면 가설도 참
> `contradiction` : 전제가 참인 경우, 가설이 거짓
> `neutral` : 전제가 참인 경우, 가설이 참인지 거짓인지 모름

![[Pasted image 20250608015800.png]]
### Question Answering and Commonsense reasoning
![[Pasted image 20250608015932.png]]
RACE 데이터셋은, 중고등학교 영어 지문 수준으로, 긴 지문 + 추론이 필요한 데이터셋.
Story Cloze 데이터셋은, 내용을 읽고 이야기의 결말을 두 개 중 하나 고르는 문제.
-> GPT-1이 state-of-the-art를 달성하면서, 긴 문맥을 효과적으로 처리할 수 있음을 보여줌.

### Semantic Similarity & Classification
![[Pasted image 20250608020249.png]]
# 5 Analysis
![[Figure 2.png]]
### Impact of number of layers transferred
위 그림의 왼쪽 그림에서, layer수가 늘어감에 따라, 모델의 성능이 향상되는 것을 보여줌.

### Zero-shot Behaviors
가설 : 트랜스포머 기반의 언어 모델이, 본인의 성능을 높이기 위해 다양한 태스크를 무의식적으로 학습함.
실험 : 사전학습 횟수를 늘려가면서, 각 태스크에 대해 파인튜닝을 하지 않고 zero-shot 성능을 확인해봤는데, 점진적으로 높아지는 것을 확인. 
따라서 태스크에 맞춰 별도의 파인튜닝을 하지 않아도, 어느정도 성능이 확보 됨.

#### 파인튜닝없이 각 태스크를 해결한 방법
- **CoLA (문법 수용성 판단)**  
    → 문장 전체에 대해 **토큰별 로그 확률 평균값**을 계산함.  
    → 이 값이 일정 기준(Threshold)을 넘으면 “문법적으로 맞다”고 판단.
- **SST-2 (감성 분석)**  
    → 문장 끝에 **"very"** 라는 단어를 붙이고,  
    → 모델이 예측할 단어를 **"positive" 또는 "negative"** 로 제한함.  
    → 둘 중 어떤 단어에 더 높은 확률을 주는지로 감성 판단.
- **RACE (질문 응답)**  
    → 지문과 질문을 조건으로 주고,  
    → 각 선택지에 대해 **토큰 로그 확률 평균**을 계산함.  
    → **가장 높은 확률**을 받은 선택지를 정답으로 예측.
- **DPRD (Winograd Schema, 대명사 해석)**  
    → 문장에서 **대명사(he, she 등)를 두 후보 중 하나로 치환**한 다음,  
    → 나머지 문장에 대해 생성 모델이 더 높은 평균 로그 확률을 주는 쪽을 정답으로 예측.

### Ablation studies (구성요소 제거 실험)
![[Table 5.png]]
#### Transformer w/o aux LM
파인튜닝 objective function에서, 보조 목적함수 $L_{2}$를 제거한 경우.
보조 목적함수가 NLI, QQP task에는 도움이 된다는 것을 확인.
또한 작은 데이터 셋보단, 큰 데이터 셋에서 보조 목적함수가 더 큰 도움이 됨.

#### LSTM w/ aux LM
같은 프레임워크로 실행했을 때, Avg. Score가 5.6점 하락
MRPC task에서만 LSTM의 성능이 더 좋았음.

#### Transformer w/o pre-training
사전학습없이 바로 지도학습으로 넘어가니까, 모든 task에서 점수가 크게 하락.


# 6 Conclusion
generative pre-training과 fine-tuning을 통해, 특정 태스크에 국한되지 않는 프레임워크 제안.

