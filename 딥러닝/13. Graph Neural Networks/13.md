이전 장들에서는 시퀸스나 이미지의 형태로 구조화된 데이터를 다뤘음
아래와 같은 **그래프 형태의 구조화된 데이터**도 존재
![[13.0.1.png]]
그래프 : **노드**라 불리는 객체들의 집합과 이를 연결하는 **엣지**로 구성됨
- (a) 분자 구조 : 노드 = 원자, 엣지 = 결합
- (b) 철도망 : 노드 = 도시, 엣지 = 평균 이동시간을 나타내는 연속 변수 (대칭적이므로 무방향)
- (c) WWW : 노드 = 페이지, 엣지 = 하이퍼링크 (방향성을 가짐)

이번 장에선 그래프 구조의 데이터에 딥러닝을 적용하는 방법을 알아봄
	이미지는 그래프의 특수한 형태로, CNN에서 영감을 받아 일반화된 접근법인 GNN을 구축
	핵심 : 그래프 노드 재정렬에 대해 **등변성(equivariance)** 또는 **불변성(invariance)** 를 보장> 노드 순서에 따라 모델의 출력이 영향을 받지 않고 동일해야함
>등변성 : 입력이 변하면 출력도 일관성 있게 변함
>불변성 : 입력이 변해도 출력이 변하지 않음 (등변성보다 하위개념)

# 13.1 Machine Learning on Graphs
크게 **3가지 목표**가 존재
	1. **노드 예측** : 
	   노드의 속성 예측 ex. 문서 분류
	2. **엣지 예측** : 
	   엣지의 존재 여부 예측 ex. 단백질 네트워크에서 일부만 알 때, 추가적인 엣지 탐색
	3. **그래프 전체 예측** : 
	   그래프 전체의 속성 예측 ex. 특정 분자가 물에 용해되는지 예측
	   그래프 회귀 또는 그래프 분류로 불림

그래프 예측 문제는 **2가지 방식** 존재
	1. **귀납적 학습** : 새로운 데이터에 대해 예측하도록 학습
	2. **전이적 학습** : 전체 구조와 일부 노드의 라벨이 주어지면 나머지 노드의 라벨을 예측하도록 학습 (반지도 학습의 한 형태)

그래프에서 직접적인 예측 대신, **임베딩을 학습해 후속 작업에 활용**할 수 있음
	**그래프 표현 학습 (Graph Representation Learning)**
	ex. 대규모 분자 구조 데이터셋을 사용해 딥러닝 모델을 학습해, 특정 과제에 맞게 미세 조정(fine-tuning)할 수 있는 기반 모델(foundation model)을 구축

### 13.1.1 Graph properties
앞으로 단순 그래프로 GNN의 핵심 개념을 살펴봄
단순 그래프 : 
	1. 노드간 엣지는 최대 하나
	2. 엣지는 무방향
	3. 자기 자신을 연결하는 self-edge는 없음

#### 그래프 기호
- 그래프는 $G = (V, E)$로, $V$는 노드의 집합, $E$는 엣지의 집합
- 노드는 $n = 1, \cdots, N$으로 인덱싱되며, 두 노드 사이 엣지는 $(n, m)$ 으로 표현
- 특정 노드 $n$과 연결된 노드들은 $n$의 이웃(neighbours)라고 하며, 이웃들의 집합을 $\mathcal{N}(n)$로 표현
#### 그래프에서 데이터 표현
노드와 엣지에 관측된 데이터를 포함할 수 있음
각 노드 $n$에 대해 $D$차원 열 백터 $\mathbf{x}_n$으로 해당 노드의 변수를 표현
모든 노드의 데이터를 전치시켜서 하나의 행렬 $\mathbf{X}$로 표현 ($N \times D$)
엣지에도 변수가 존재할 수 있지만 노드 변수에만 초점을 맞춤
### 13.1.2 Adjacency matrix
그래프의 엣지를 표현하는 간단하고 편리한 방법
#### 인접 행렬 정의
1. 노드의 순서 정의 ($n = 1,\cdots,N$)
2. $N \times N$ 크기의 인접행렬 $\mathbf{A}$를 생성

$A_{n,m} = 1$ : 노드 $n$에서 노드 $m$으로 엣지가 있음
$A_{n,m} = 0$ : 그렇지 않음

![[13.1.1.png]]
무방향 그래프에선 $A_{n,m}=A_{m,n}$이 항상 성립하므로 인접 행렬이 대칭임
(b)와 (c)는 노드 순서에 따라 행렬이 달라지는 문제점을 보여줌 (불변성을 가져야됨)

해결책 : 순열 불변성을 모델 설계에 반영 (inductive bias)

### 13.1.3 Permutation equivariance
순열은 노드의 순서를 바꾸는 작업으로, 순열 행렬 $P$로 표현
$$
\mathbf{P} = \begin{pmatrix} 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 & 0 \end{pmatrix}
$$
인접 행렬과 똑같은 크기이며 다음의 특징을 가짐
	각 행과 각 열에 1이 하나씩 존재하며 나머지는 모두 0
	$\mathbf{P}_{n,m}$이 1이면 노드 $n$이 노드 $m$으로 재배열됨을 의미

표준 단위 벡터를 $\mathbf{u}_n$으로 나타내면 순열 행렬 $\mathbf{P}$는 아래와 같음
$$
\mathbf{P} =
\begin{pmatrix}
\mathbf{u}_{\pi(1)}^{\mathsf{T}} \\
\mathbf{u}_{\pi(2)}^{\mathsf{T}} \\
\vdots \\
\mathbf{u}_{\pi(N)}^{\mathsf{T}}
\end{pmatrix}
$$
이때 $\pi(\cdot)$는 순열 함수로, 노드 $n$을 재배열 후의 노드 $m$으로 매핑함

- 재배열된 노드 데이터 행렬 : $\widetilde{\mathbf{X}} = \mathbf{P} \mathbf{X}$ (행의 순서가 바뀜)
- 노드 순서가 바뀐 인접 행렬 : $\widetilde{\mathbf{A}} = \mathbf{P} \mathbf{A} \mathbf{P}^{\mathsf{T}}$ (행과 열 모두 순열해야함)

#### GNN에서 불변성과 등변성
불변성(Invariance) : 
그래프의 **전역 속성**(예: 그래프 분류 결과)은 노드 순서와 무관해야 함
$$
y(\widetilde{\mathbf{X}}, \widetilde{\mathbf{A}}) = y(\mathbf{X}, \mathbf{A})
$$

등변성(Equivariance) : 
노드에 관한 **지역 속성**(예: 각 노드의 라벨)은 순열에 따라 동일한 방식으로 재배열되어야 함
$$
y(\widetilde{\mathbf{X}}, \widetilde{\mathbf{A}}) = \mathbf{P} y(\mathbf{X}, \mathbf{A})
$$

GNN 설계시, 위 불변성과 등변성을 inductive bias로 포함시켜야 함
# 13.2 Neural Message-Passing
각 신경망 층은 **노드 재배열에 대해 등변성**을 가져야 함 (여러 겹으로 쌓아도)
그래프 수준의 속성을 예측할 땐, **마지막 레이어가 불변성을 보장**하도록 설계함
>각 레이어는 매개변수에 대해 미분가능한 비선형 함수면서 출력이 등변성을 유지해야함

서로 다른 분자가 서로 다른 수의 원자를 가질 수 있는 것처럼 그래프의 크기가 다양할 수 있기에, **가변적인 그래프를 입력으로 처리**할 수 있어야 함 (트랜스포머처럼)
또한 대규모 그래프를 위해 스케일링이 되어어야 하므로, **매개변수 공유를 사용**함
매개변수 공유를 통해 불변성과 등변성도 모델에 내제 가능

### 13.2.1 Convolutional filters
이미지는 그래프 데이터의 일종
	노드 : 이미지의 각 픽셀
	엣지 : 인접한 픽셀 간의 관계 (대각선, 수평, 수직)
따라서 CNN에서 영감을 받고자 함

![[13.2.1.png]]
(a) 3x3 필터가 작용하는 방식
$$
z_i^{(l+1)} = f\left( \sum_j w_j z_j^{(l)} + b \right)
$$
좌항 : $l+1$번째 레이어에서 픽셀(노드) $i$의 값
우항 : 픽셀(노드) $i$ 주변 9개의 값에 대해 filter(가중치)를 주고 편향을 더한 값에 ReLU와 같은 비선형 함수를 취한 값
>픽셀(노드)의 순서를 재배열하면 등변성을 보장하지 못함 (필터의 가중치 때문)

(b) 필터를 그래프 관점에서 재구성
$$
z_i^{(l+1)} = f\left( w_{\text{neigh}} \sum_{j \in \mathcal{N}(i)} z_j^{(l)} + w_{\text{self}} z_i^{(l)} + b \right)
$$
이때 매개변수들(가중치랑 편향)은 노드에 관계없이 공유됨
>$w_{neigh}$는 이웃 노드에 공유되는 가중치로 순서가 바뀌어도 출력은 같음 (불변성)
모든 노드에 대해 이렇게 작동하므로 노드 순서가 바뀌어도 결과는 같은 방식으로 재배열 됨 (등변성)

자기 자신도 입력에 포함되는 이유? (by. GPT)
	CNN은 주변의 패턴이 중요한거지 개별 값은 큰 의미가 없음
	하지만 GNN에서는 노드 자신의 정보도 중요함
	ex. SNS에서 친구들의 특성뿐만 아니라 해당 사용자의 특성도 반영해야함
### 13.2.2 Graph convolution networks
CNN에서 영감을 받아 그래프 데이터를 처리하기 위한 DNN
목표 : 
	1. 노드 임베딩을 비선형적이고 유연하게 변환
	2. 학습 가능한 가중치와 편향 매개변수에 대해 미분 가능하도록 설계
	3. 그래프의 $l$번째 레이어에서 $l+1$번째 레이어로 정보를 매핑

크게 2가지 단계로 구성됨
	1. **Aggregation** : 불변성을 만족하며 이웃 노드에서 정보를 수집&결합
	2. **Update** : 이웃 정보와 노드 자체의 정보를 결합해 새로운 임베딩 벡터 계산

#### Aggregation
$$
\mathbf{z}_n^{(l)} = \text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right)
$$
$\mathbf{h}_n^{(l)}$ : $l$번째 레이어의 $D$차원 노드 $n$의 임베딩 벡터 (초기 값은 $\mathbf{X}_n$)

동작 : 각 노드 $n$에 대해 이웃 노드 $\mathcal{N}(n)$의 임베딩 벡터를 결합
요구사항 :
	1. 노드마다 이웃의 수가 다를 수 있으므로, 가변 길이 입력
	2. 순열 불변성
	3. 학습 가능성 (= 미분 가능한 매개변수가 존재)

#### Update
$$
\mathbf{h}_n^{(l+1)} = \text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right)
$$
$\mathbf{h}_n^{(l+1)}$ : $l+1$번째 레이어의 $D$차원 노드 $n$의 새로운 임베딩 벡터

동작 : 집계된 정보와 노드 자체 정보를 결합한 값으로 임베딩 벡터 업데이트
요구사항 :
	1. 학습 가능성 (= 미분 가능한 매개변수가 존재)
	2. 비선형성


- 각 레이어마다 독립적인 매개변수를 가질 수도 있고, 모든 레이어에서 매개변수를 공유할 수도 있음
- 위와 같은 구조를 **메시지 전달 신경망 (Message-Passing Neural Network, MPNN)** 이라고 부름

![[13.2.2.png]]
### 13.2.3 Aggregation operators
#### 1. 단순 합
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = \sum_{m \in \mathcal{N}(n)} \mathbf{h}_m^{(l)}
$$
특징:
	이웃 노드의 임베딩 벡터를 단순히 합산
	순열 불변성을 만족하며, 이웃 노드의 개수와 관계없이 정의 가능
	학습 가능한 매개변수가 없음
문제점:
	이웃 노드의 수가 많은 노드는 적은 노드보다 더 큰 영향을 받으므로, **숫자적인 불안정성(numerical instability)** 이 발생할 수 있음
	특히, 소셜 네트워크처럼 이웃 노드 수가 몇 배 이상 차이 나는 경우 문제를 일으킬 수 있음

#### 2. 평균
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = \frac{1}{|\mathcal{N}(n)|} \sum_{m \in \mathcal{N}(n)} \mathbf{h}_m^{(l)}
$$
특징:
	이웃 노드의 개수에 따라 합산 값을 정규화
	이웃 수의 차이에 따른 불균형을 줄임
문제점:
	그래프의 구조적 정보를 일부 버리게 됨 (이웃 노드의 개수의 차이가 없음)
	단순 합보다 표현력이 낮은 경우가 있음(Hamilton, 2020)
#### 3. 정규화된 합
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = 
\sum_{m \in \mathcal{N}(n)} \frac{\mathbf{h}_m^{(l)}}{\sqrt{|\mathcal{N}(n)| |\mathcal{N}(m)|}}
$$
특징:
	각 노드와 이웃 노드의 연결 수를 고려하여 정규화
	그래프의 구조를 반영하면서 이웃의 영향력을 조정
#### 4. 요소별 최대/최소값
특징:
	이웃 노드 임베딩의 각 요소별로 최대값(또는 최소값)을 선택
	순열 불변성을 만족하며, 이웃 수와 관계없이 정의 가능
	그래프 구조보다는 강한 특징만을 반영함

#### 수용 영역(receptive field)
특정 노드의 업데이트에 영향을 미치는 다른 노드의 범위를 의미

![[13.2.3.png]]
이전 층에서 이웃들의 값이 계속 반영됨
>GNN에서는 레이어가 쌓일수록 수용 영역이 점진적으로 확장됨

문제점 : 대규모이며 sparse한 그래프에서는 많은 레이어가 필요해서 계산 비용 증가
해결책 : 슈퍼 노드를 도입해 모든 노드를 직접 연결함으로써 정보 전파 가속화
#### 학습 가능한 Aggregation operators
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = 
\text{MLP}_{\theta} \left( \sum_{m \in \mathcal{N}(n)} \text{MLP}_{\phi}(\mathbf{h}_m^{(l)}) \right)
$$
집계 전후에 MLP를 추가해 구현함
특징 :
	매개변수 $\theta, \phi$는 모든 노드에 대해 공유되므로 불변성을 유지함
	합은 평균이나 요소별 최대 최소로 대체 가능
#### 엣지가 없는 경우
그래프에 엣지가 없으면 비구조적 데이터로 해석
MLP를 사용해 이를 학습하는 프레임워크 제공 가능 -> 딥 세트(Deep Sets)
### 13.2.4 Update operators
적절한 Aggregate operator를 선택하고 update operator의 형태를 정해야 함
#### 1. 기본적인 업데이트 연산
$$
\text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right) = 
f\left( \mathbf{W}_{\text{self}} \mathbf{h}_n^{(l)} + \mathbf{W}_{\text{neigh}} \mathbf{z}_n^{(l)} + \mathbf{b} \right)
$$
앞에서 이미 봤던 수식임
비선형성을 통해 모델의 표현력 강화
#### 2. 단순한 형태의 업데이트 연산
$$
\mathbf{h}_n^{(l+1)} = \text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right) = 
f\left( \mathbf{W}_{\text{neigh}} \sum_{m \in \mathcal{N}(n), m \neq n} \mathbf{h}_m^{(l)} + \mathbf{b} \right)
$$
$\mathbf{W}_{\text{self}} = \mathbf{W}_{\text{neigh}}$며 Aggregation을 단순 합으로 계산하면 위와 같음
계산 효율성을 높이고 구현을 간소화함
#### 노드 임베딩 초기화 방법
1. 노드의 관찰 데이터 $\mathbf{X}_n$로 설정
2. 임베딩 차원이 크면, 0 패딩 또는 선형 변환으로 매핑
#### GNN의 전체 표현
GNN은 여러 레이어로 구성되고 노드 임베딩을 반복적으로 변환함
$$
\begin{align}
\mathbf{H}^{(1)} &= \mathbf{F}\left( \mathbf{X}, \mathbf{A}, \mathbf{W}^{(1)} \right) \\ 
\mathbf{H}^{(2)} &= \mathbf{F}\left( \mathbf{H}^{(1)}, \mathbf{A}, \mathbf{W}^{(2)} \right) \\ 
&\vdots \\ 
\mathbf{H}^{(L)} &= \mathbf{F}\left( \mathbf{H}^{(L-1)}, \mathbf{A}, \mathbf{W}^{(L)} \right)
\end{align}
$$
$\mathbf{H}^{(l)}$ : $l$번째 레이어에서의 임베딩 행렬
$\mathbf{A}$ : 인접 행렬
$\mathbf{W}^{(l)}$ : $l$번째 레이어의 가중치와 편향

아래는 노드 순서 재배열에 대해 등변성을 가짐을 보여줌
$$
\mathbf{P}\mathbf{H}^{(l)} = \mathbf{F}\left( \mathbf{P}\mathbf{H}^{(l-1)}, \mathbf{P}\mathbf{A}\mathbf{P}^{\mathsf{T}}, \mathbf{W}^{(l)} \right)
$$
$\mathbf{P}$ : 순열 행렬

---

## 25.02.09
### 복습
그래프 신경망(GNN)은 여러 개의 계층으로 구성되어 노드 임베딩 벡터 집합 $\{h_n^{(l)}\}$을 $\{h_n^{(l+1)}\}$로 변환함
GNN의 마지막 컨볼루션 계층 이후엔 예측값을 얻으며 비용 함수를 정의해 학습함
학습된 네트워크를 사용해 새로운 데이터에 대한 예측을 수행함
### 13.2.5 Node classification
그래프에서 노드를 분류하는 작업
출력 계층을 소프트맥스(softmax) 함수를 계산하게끔 정의함
아래는 소프트맥스 함수로, 각 노드에 대해 $C$개의 클래스로 분류시킴
$$
y_{ni} = \frac{\exp\left(\mathbf{w}_i^\top \mathbf{h}_n^{(L)}\right)}{\sum_j \exp\left(\mathbf{w}_j^\top \mathbf{h}_n^{(L)}\right)}
$$
$\{w_i\}$ : 학습 가능한 가중치 벡터 집합.  $i = 1, \cdots, C$
$\{h_n^{(l)}\}$ : 마지막 계층($L$)에서의 노드 $n$의 임베딩 벡터

이후엔 전체 노드와 클래스에 대한 크로스엔트로피 손실 함수(cross-entropy loss function)를 정의함 (**책과 수식 다르니 참고!**)
$$
L = - \sum_{n \in V_{\text{train}}} \sum_{i=1}^C t_{ni} \log y_{ni}
$$
$\{t_{ni}\}$ : 노드 $n$에 대한 원-핫 인코딩(one-hot encoding)된 타겟 값 (정답인 인덱스만 1, 나머진 0)
가중치 벡터가 출력 노드 전체에서 공유되므로 $y_{ni}$는 노드 순서의 순열에 대해 등변성
따라서 손실함수 $L$은 노드 순서의 순열에 대해 불변성을 가짐
#### 노드의 유형
손실 함수에서 합은 학습에 사용되는 특정 노드 집합 $V_{train}$에 대해서만 수행됨
GNN에서는 노드를 다음의 3가지 유형으로 구분함
1. 학습 노드 $V_{train}$
   라벨이 주어진 노드로, GNN의 메시지 전달(message-passing) 연산에 포함됨
2. 전이적(transductive) 노드 $V_{trans}$
   라벨이 없는 노드로, 손실 함수에서 사용되지 않음
   하지만 학습 및 추론 과정에서 메시지 전달 연산에 참여할 수 있음
   따라서 추론 단계에서 해당 노드의 라벨을 예측할 수 있음
3. 귀납적(inductive) 노드 $V_{induct}$
   마찬가지로 손실 함수 계산에 사용되지 않음
   학습 단계에서 메시지 전달 연산에 참여하지 않음
   그러나 추론 단계에서는 메시지 전달에 포함되어 레이블이 최종적으로 예측됨
##### 예시 by GPT
- 학습 노드 ($V_{\text{train}}$​):
    - 기존 소셜 네트워크에서 관심사가 이미 알려진 사용자들.
    - 예: "스포츠"에 관심이 있다고 확인된 사용자들.
- 전이적 노드 ($V_{\text{trans}}$​):
    - 관심사는 모르지만 기존 소셜 네트워크 내에서 다른 사용자와 연결된 사용자.
    - 예: 친구 관계는 있지만 관심사는 알려지지 않은 사용자들.
- 귀납적 노드 ($V_{\text{induct}}$​):
    - 학습 이후 소셜 네트워크에 새로 가입한 사용자들.
    - 예: 학습 데이터에 없던 신규 사용자.

#### 전이적 학습 vs. 귀납적 학습
- 귀납적 학습 (Inductive Learning)
	- 전이적 노드($V_{\text{trans}}$​)가 존재하지 않는 경우, 즉 테스트 노드(및 해당 엣지)가 훈련 중에는 보이지 않는 경우, 이를 귀납적 학습이라 한다.
	- 이는 기본적으로 지도 학습(supervised learning)의 한 형태로 볼 수 있다.
- 전이적 학습 (Transductive Learning)
	- 만약 전이적 노드($V_{\text{trans}}$​)가 존재하는 경우, 이는 전이적 학습이라 불린다.
	- 전이적 학습은 반지도 학습(semi-supervised learning)의 한 형태로 해석할 수 있다.

따라서 GNN에서 노드 분류를 수행할 때 사용 가능한 노드의 종류와 훈련 방식에 따라 귀납적 학습과 전이적 학습을 구분할 수 있음
### 13.2.6 Edge classification
두 노드 간에 엣지가 존재해야하는지를 결정함
노드 임베딩 집합이 있을 때 두 임베딩 간의 dot product를 통해 엣지가 존재할 확률 $p(n, m)$을 계산할 수 있음
$$
p(n, m) = \sigma\left(h_n^\top h_m\right)
$$
$h_n$​, $h_m$​: 각각 노드 $n$과 $m$의 임베딩 벡터
$\sigma(x)=\frac{1}{1+exp(−x)}$​는 시그모이드 함수로, 결과값을 0과 1 사이의 확률로 변환

ex. 소셜 네트워크에서 두 사람이 공통 관심사를 공유하고 있는지 예측해 서로 연결되기를 원할 가능성을 판단
### 13.2.7 Graph classification
라벨이 있는 그래프 집합 $G_1​,G_2​,\cdots,G_N$​을 기반으로 새로운 그래프의 속성을 예측함
이를 위해 마지막 계층의 노드 임베딩 벡터들을 결합하는 방식이 노드의 순서에 의존하지 않아야 하며 출력 예측이 순서에 불변해야함
Aggregate 함수와 유사하지만 개별 노드 이웃 집합만을 포함하는게 아닌 모든 노드를 포함한다는 점에서 다름
가장 간단한 방법은 노드 임베딩 벡터들의 합
$$
y = f\left(\sum_{n \in V} h_n^{(L)}\right)
$$
$V$: 그래프의 노드 집합
$h_n^{(L)}$​: 마지막 계층에서 노드$n$의 임베딩 벡터
$f$: 학습 가능한 매개변수를 포함할 수 있는 함수(예: 선형 변환 또는 신경망)

이 외에도 평균, 원소별 최소/최대 같은 불변 집계 함수(invariant aggregation functions)를 사용할 수 있음
학습과 추론에서 서로 다른 그래프 집합이 사용되므로 그래프 수준의 예측은 귀납적 학습임
# 13.3 General Graph Networks
지금까지 다룬 GNN의 다양한 변형과 확장을 다룸
### 13.3.1 Graph attention networks (GAT)
어텐션 메커니즘을 GNN의 집계 함수(aggregation function)에 활용
즉, 특정 이웃 노드의 중요도를 반영해 가중치를 부여하는 방식으로 정보를 집계함
$$
z_n^{(l)} = \text{Aggregate}\left(h_m^{(l)} : m \in N(n)\right) = \sum_{m \in N(n)} A_{nm} h_m^{(l)}
$$
$N(n)$: 노드 $n$의 이웃 집합(neighborhood set)
$h_m^{(l)}$​: 계층 $l$에서의 노드 $m$의 임베딩 벡터
$A_{nm}$: 노드 $m$이 노드 $n$에게 주는 중요도(어텐션 계수)로, 이웃 노드 $m$이 메시지를 전달할 때 가중치를 결정함

#### 어텐션 계수의 조건
1. 비음수 조건 ($A_{nm} \geq 0$): 어텐션 가중치가 항상 0 이상이어야 함
2. 정규화 조건 ($\sum_{m \in N(n)} A_{nm} = 1$): 모든 이웃 노드의 어텐션 계수의 합이 1이 되도록 정규화
위 조건들을 만족시키기 위해 일반적으로 소프트맥스 함수를 사용해 어텐션 계수를 정규화함
#### 어텐션 계수 계산 방법 1: 쌍선형 형태 (Bilinear Form)
가장 기본적인 방법
$$
A_{nm} = \frac{\exp\left(\mathbf{h}_n^\top \mathbf{W} \mathbf{h}_m\right)}{\sum_{m' \in N(n)} \exp\left(\mathbf{h}_n^\top \mathbf{W} \mathbf{h}_{m'}\right)}
$$
$\mathbf{W}$: 학습 가능한 $D \times D$ 차원의 행렬로, 노드 임베딩 벡터 간의 관계를 학습함
분자: 노드 $n$과 노드 $m$ 간의 관계 강도를 계산한 값
분모: 정규화를 위한 항으로, 이웃 노드 전체에 대해 확률 분포를 형성함(소프트맥스 적용)
#### 어텐션 계수 계산 방법 2: 신경망 기반 어텐션 (Neural Network-based Attention)
더 일반적인 방법은 다층 퍼셉트론을 사용해 어텐션을 계산하는 것
$$
A_{nm} = \frac{\exp\{\text{MLP}(\mathbf{h}_n, \mathbf{h}_m)\}}{\sum_{m' \in \mathcal{N}(n)} \exp\{\text{MLP}(\mathbf{h}_n, \mathbf{h}_{m'})\}}
$$
순서 불변성: MLP는 하나의 연속적인 출력 값을 가지며, 입력 벡터의 순서가 바뀌어도 동일한 값을 출력해야함
노드 재배열에 대한 등변성: MLP가 모든 노드에 대해 동일한 함수로 공유되어 노드 재배열에 대해 등변성(equivariant)을 보장함
#### 다중 어텐션 헤드(Multi-Head Attention)
트랜스포머처럼 다중 어텐션 헤드를 도입해 성능 향상 가능
$H$개의 서로 다른 어텐션 헤드를 정의하며 각각 독립적인 매개변수를 학습함
집계 단계에서 concat 또는 선형변환을 수행함
노드 임베딩을 여러 관점에서 조합해 학습할 수 있으며 학습 성능이 향상됨
#### 트랜스포머와의 관계
완전 연결 그래프에서 GAT를 적용하면 일반적인 트랜스포머 인코더(Transformer Encoder)와 동일한 구조 (Self-Attention과 동일하게 동작)
따라서 GAT는 트랜스포머 모델을 그래프 구조로 확장한 형태로 볼 수 있음
### 13.3.2 Edge embeddings
일부 GNN에서는 엣지에도 데이터를 포함할 수 있음
엣지에 직접적인 관측값이 없더라도 학습 가능한 잠재 변수(hidden variables)를 가질 수 있음
이를 통해 GNN이 학습하는 내부(중간) 표현에 기여할 수 있음
따라서 노드 임베딩 $h_n^{(h)}$와 같이, 엣지 임베딩 $e_{nm}^{(l)}$을 도입해 다음과 같은 메시지 전달 방정식을 정의할 수 있음
#### 엣지 업데이트 (Edge Update)
$$
e_{nm}^{(l+1)} = \text{Update}_{\text{edge}}\left(e_{nm}^{(l)}, h_n^{(l)}, h_m^{(l)}\right)
$$
현재 계층 $l$에서 엣지 임베딩과 노드 임베딩을 함께 업데이트함
#### 노드 메시지 집계 (Node Aggregation)
$$
z_n^{(l+1)} = \text{Aggregate}_{\text{node}}\left(e_{nm}^{(l+1)} : m \in N(n)\right)
$$
업데이트된 엣지 임베딩 $e_{nm}^{(l+1)}$을 사용해, 이웃 노드 $m$과 연결된 모든 엣지 정보를 집계함
#### 노드 업데이트 (Node Update)
$$
h_n^{(l+1)} = \text{Update}_{\text{node}}\left(h_n^{(l)}, z_n^{(l+1)}\right)
$$
이전 계층의 노드 임베딩 $h_n^{(h)}$과 메시지 집계 결과 $z_n^{(l+1)}$을 활용해 새로운 노드 임베딩 생성

마지막 계층에서 학습된 엣지 임베딩은 엣지와 관련된 예측을 직접 수행하는데 사용할 수 있음
### 13.3.3 Graph embeddings
그래프 전체를 표현하는 임베딩 벡터 $g^{(l)}$도 유지 및 업데이트할 수 있음
이를 통해 더욱 일반화된 함수와 풍부한 학습 표현을 정의할 수 있음
#### 엣지 임베딩 업데이트 (Edge Update)
$$
e_{nm}^{(l+1)} = \text{Update}_{\text{edge}}\left(e_{nm}^{(l)}, h_n^{(l)}, h_m^{(l)}, g^{(l)}\right)
$$
#### 노드 메시지 집계 (Node Aggregation)
$$
z_n^{(l+1)} = \text{Aggregate}_{\text{node}}\left(e_{nm}^{(l+1)} : m \in N(n)\right)
$$
#### 노드 임베딩 업데이트 (Node Update)
$$
h_n^{(l+1)} = \text{Update}_{\text{node}}\left(h_n^{(l)}, z_n^{(l+1)}, g^{(l)}\right)
$$
#### 그래프 임베딩 업데이트 (Graph Update)
$$
g^{(l+1)} = \text{Update}_{\text{graph}}\left(g^{(l)}, \{h_n^{(l+1)} : n \in V\}, \{e_{nm}^{(l+1)} : (n, m) \in E\}\right)
$$
모든 노드/엣지의 최신 임베딩 정보를 활용해 학습함
![[13.3.1.png]]
![[13.3.2.png]]
### 13.3.4 Over-smoothing
GNN에서 발생할 수 있는 문제 중 하나로, 여러 번의 메시지 전달이 이루어진 후, 노드 임베딩 벡터들이 서로 매우 유사해지는 현상
층을 아무리 많이 쌓아도 노드 임베딩들이 거의 동일해지므로 모델의 표현력이 제한됨
#### 해결책 1: 잔차 연결
$$
h_n^{(l+1)} = \text{Update}_{\text{node}}\left(h_n^{(l)}, z_n^{(l+1)}, g^{(l)}\right) + h_n^{(l)}
$$
이전 노드 임베딩 정보가 완전히 사라지는 것을 방지
ResNet에서의 스킵 연결(skip connection)과 유사한 개념
#### 해결책 2: 모든 계층의 정보를 활용해 최종 출력층을 구성
$$
y_n = f\left(h_n^{(1)} \oplus h_n^{(2)} \oplus \cdots \oplus h_n^{(L)}\right)
$$
concat 연산을 활용해 하나의 벡터를 사용함으로써 특정 계층에서 발생하는 정보 손실을 방지
concat 대신 맥스 풀링을 사용할 수 있음 (각 차원에서 가장 중요한 정보만을 유지)
### 13.3.5 Regularization
GNN에도 일반적인 정규화 기법을 사용할 수 있음 (ex. L2 norm, 패널티 항)
#### 가중치 공유(Weight Sharing)
일반적인 GNN은 층마다 독립적인 파라미터를 학습함
계층 간의 가중치와 편향을 공유해 독립적인 파라미터 수를 줄여 모델을 정규화할 수 있음
#### 드롭아웃(Dropout)
1. 노드 드롭아웃: 훈련 중 일부 노드를 생략
2. 엣지 드롭아웃: 훈련 중 일부 엣지를 무작위로 제거
그래프의 연결 구조가 매 훈련마다 바뀌므로, 모델이 특정한 그래프 패턴에 과적합되지 않도록 도움
### 13.3.6 Geometric deep learning
GNN이 공간적 특성을 다루는 응용분야에서 사용될 경우, 추가적인 등변성 및 불변성 속성을 내재화할 수 있음
이렇게 기하학적 대칭성을 신경망 구조에 반영하는 연구 분야를 기하학적 딥러닝(Geometric Deep Learning)이라고 함

예를 들어 분자 구조 예측을 한다고 치면,
1. 회전 : 분자가 공간에서 회전해도 그 특성은 같아야함
2. 이동 : 좌표계를 이동시켜도 분자의 특성은 변하지 않음 
3. 대칭 반사 : 분자를 거울에 반사한 형태로 바꿔도 일부 물리, 화학적 특성은 동일하게 유지됨
#### 엣지 임베딩 업데이트 (Edge Update)
$$
e_{nm}^{(l+1)} = \text{Update}_{\text{edge}}\left(e_{nm}^{(l)}, h_n^{(l)}, h_m^{(l)}, \|r_n^{(l)} - r_m^{(l)}\|^2\right)
$$
$r_n^{(l)}$: 각 계층 $l$에서의 원자 $n$을 나타내는 좌표 임베딩 벡터
두 원자의 좌표 간의 제곱 거리가 추가됨
거리 정보는 회전, 이동, 반사에 대해 불변이므로, 모델이 이러한 변환에 영향을 받지 않도록 할 수 있음
#### 좌표 임베딩 업데이트 (Coordinate Update)
$$
r_n^{(l+1)} = r_n^{(l)} + C \sum_{(n, m) \in E} \left(r_n^{(l)} - r_m^{(l)}\right) \phi\left(e_{nm}^{(l+1)}\right)
$$
상대적인 좌표 차이를 사용해 좌표를 갱신함
원자의 위치 자체가 아닌 이웃 원자들과의 상대적인 위치 관계를 반영해 학습
이를 통해 회전 및 이동 변환에 대해 등변성을 가짐
$\phi\left(e_{nm}^{(l+1)}\right)$: 엣지 임베딩의 정보를 반영하는 신경망 함수
$C$: 정규화 계수로, 일반적으로 합산 항의 개수의 역수($\frac{1}{\mid E \mid}$)로 설정
#### 노드 메시지 집계 (Node Aggregation)
$$
z_n^{(l+1)} = \text{Aggregate}_{\text{node}}\left(\{e_{nm}^{(l+1)} : m \in N(n)\}\right)
$$
각 노드의 이웃 엣지 임베딩을 집계(aggregation)하여 메시지 벡터 생성
엣지 임베딩에 이미 거리 기반 정보가 포함되어 있어, 모델이 기하학적 관계를 반영할 수 있음
#### 노드 임베딩 업데이트 (Node Update)
$$
h_n^{(l+1)} = \text{Update}_{\text{node}}\left(h_n^{(l)}, z_n^{(l+1)}\right)
$$
기존과 동일하게 노드 임베딩을 업데이트함