이전 장들에서는 시퀸스나 이미지의 형태로 구조화된 데이터를 다뤘음
아래와 같은 **그래프 형태의 구조화된 데이터**도 존재
![[13.0.1.png]]
그래프 : **노드**라 불리는 객체들의 집합과 이를 연결하는 **엣지**로 구성됨
- (a) 분자 구조 : 노드 = 원자, 엣지 = 결합
- (b) 철도망 : 노드 = 도시, 엣지 = 평균 이동시간을 나타내는 연속 변수 (대칭적이므로 무방향)
- (c) WWW : 노드 = 페이지, 엣지 = 하이퍼링크 (방향성을 가짐)

이번 장에선 그래프 구조의 데이터에 딥러닝을 적용하는 방법을 알아봄
	이미지는 그래프의 특수한 형태로, CNN에서 영감을 받아 일반화된 접근법인 GNN을 구축
	핵심 : 그래프 노드 재정렬에 대해 **등변성(equivariance)** 또는 **불변성(invariance)** 를 보장> 노드 순서에 따라 모델의 출력이 영향을 받지 않고 동일해야함
>등변성 : 입력이 변하면 출력도 일관성 있게 변함
>불변성 : 입력이 변해도 출력이 변하지 않음 (등변성보다 하위개념)

# 13.1 Machine Learning on Graphs
크게 **3가지 목표**가 존재
	1. **노드 예측** : 
	   노드의 속성 예측 ex. 문서 분류
	2. **엣지 예측** : 
	   엣지의 존재 여부 예측 ex. 단백질 네트워크에서 일부만 알 때, 추가적인 엣지 탐색
	3. **그래프 전체 예측** : 
	   그래프 전체의 속성 예측 ex. 특정 분자가 물에 용해되는지 예측
	   그래프 회귀 또는 그래프 분류로 불림

그래프 예측 문제는 **2가지 방식** 존재
	1. **귀납적 학습** : 새로운 데이터에 대해 예측하도록 학습
	2. **전이적 학습** : 전체 구조와 일부 노드의 라벨이 주어지면 나머지 노드의 라벨을 예측하도록 학습 (반지도 학습의 한 형태)

그래프에서 직접적인 예측 대신, **임베딩을 학습해 후속 작업에 활용**할 수 있음
	**그래프 표현 학습 (Graph Representation Learning)**
	ex. 대규모 분자 구조 데이터셋을 사용해 딥러닝 모델을 학습해, 특정 과제에 맞게 미세 조정(fine-tuning)할 수 있는 기반 모델(foundation model)을 구축

### 13.1.1 Graph properties
앞으로 단순 그래프로 GNN의 핵심 개념을 살펴봄
단순 그래프 : 
	1. 노드간 엣지는 최대 하나
	2. 엣지는 무방향
	3. 자기 자신을 연결하는 self-edge는 없음

#### 그래프 기호
- 그래프는 $G = (V, E)$로, $V$는 노드의 집합, $E$는 엣지의 집합
- 노드는 $n = 1, \cdots, N$으로 인덱싱되며, 두 노드 사이 엣지는 $(n, m)$ 으로 표현
- 특정 노드 $n$과 연결된 노드들은 $n$의 이웃(neighbours)라고 하며, 이웃들의 집합을 $\mathcal{N}(n)$로 표현
#### 그래프에서 데이터 표현
노드와 엣지에 관측된 데이터를 포함할 수 있음
각 노드 $n$에 대해 $D$차원 열 백터 $\mathbf{x}_n$으로 해당 노드의 변수를 표현
모든 노드의 데이터를 전치시켜서 하나의 행렬 $\mathbf{X}$로 표현 ($N \times D$)
엣지에도 변수가 존재할 수 있지만 노드 변수에만 초점을 맞춤
### 13.1.2 Adjacency matrix
그래프의 엣지를 표현하는 간단하고 편리한 방법
#### 인접 행렬 정의
1. 노드의 순서 정의 ($n = 1,\cdots,N$)
2. $N \times N$ 크기의 인접행렬 $\mathbf{A}$를 생성

$A_{n,m} = 1$ : 노드 $n$에서 노드 $m$으로 엣지가 있음
$A_{n,m} = 0$ : 그렇지 않음

![[13.1.1.png]]
무방향 그래프에선 $A_{n,m}=A_{m,n}$이 항상 성립하므로 인접 행렬이 대칭임
(b)와 (c)는 노드 순서에 따라 행렬이 달라지는 문제점을 보여줌 (불변성을 가져야됨)

해결책 : 순열 불변성을 모델 설계에 반영 (inductive bias)

### 13.1.3 Permutation equivariance
순열은 노드의 순서를 바꾸는 작업으로, 순열 행렬 $P$로 표현
$$
\mathbf{P} = \begin{pmatrix} 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 & 0 \end{pmatrix}
$$
인접 행렬과 똑같은 크기이며 다음의 특징을 가짐
	각 행과 각 열에 1이 하나씩 존재하며 나머지는 모두 0
	$\mathbf{P}_{n,m}$이 1이면 노드 $n$이 노드 $m$으로 재배열됨을 의미

표준 단위 벡터를 $\mathbf{u}_n$으로 나타내면 순열 행렬 $\mathbf{P}$는 아래와 같음
$$
\mathbf{P} =
\begin{pmatrix}
\mathbf{u}_{\pi(1)}^{\mathsf{T}} \\
\mathbf{u}_{\pi(2)}^{\mathsf{T}} \\
\vdots \\
\mathbf{u}_{\pi(N)}^{\mathsf{T}}
\end{pmatrix}
$$
이때 $\pi(\cdot)$는 순열 함수로, 노드 $n$을 재배열 후의 노드 $m$으로 매핑함

- 재배열된 노드 데이터 행렬 : $\widetilde{\mathbf{X}} = \mathbf{P} \mathbf{X}$ (행의 순서가 바뀜)
- 노드 순서가 바뀐 인접 행렬 : $\widetilde{\mathbf{A}} = \mathbf{P} \mathbf{A} \mathbf{P}^{\mathsf{T}}$ (행과 열 모두 순열해야함)

#### GNN에서 불변성과 등변성
불변성(Invariance) : 
그래프의 **전역 속성**(예: 그래프 분류 결과)은 노드 순서와 무관해야 함
$$
y(\widetilde{\mathbf{X}}, \widetilde{\mathbf{A}}) = y(\mathbf{X}, \mathbf{A})
$$

등변성(Equivariance) : 
노드에 관한 **지역 속성**(예: 각 노드의 라벨)은 순열에 따라 동일한 방식으로 재배열되어야 함
$$
y(\widetilde{\mathbf{X}}, \widetilde{\mathbf{A}}) = \mathbf{P} y(\mathbf{X}, \mathbf{A})
$$

GNN 설계시, 위 불변성과 등변성을 inductive bias로 포함시켜야 함
# 13.2 Neural Message-Passing
각 신경망 층은 **노드 재배열에 대해 등변성**을 가져야 함 (여러 겹으로 쌓아도)
그래프 수준의 속성을 예측할 땐, **마지막 레이어가 불변성을 보장**하도록 설계함
>각 레이어는 매개변수에 대해 미분가능한 비선형 함수면서 출력이 등변성을 유지해야함

서로 다른 분자가 서로 다른 수의 원자를 가질 수 있는 것처럼 그래프의 크기가 다양할 수 있기에, **가변적인 그래프를 입력으로 처리**할 수 있어야 함 (트랜스포머처럼)
또한 대규모 그래프를 위해 스케일링이 되어어야 하므로, **매개변수 공유를 사용**함
매개변수 공유를 통해 불변성과 등변성도 모델에 내제 가능

### 13.2.1 Convolutional filters
이미지는 그래프 데이터의 일종
	노드 : 이미지의 각 픽셀
	엣지 : 인접한 픽셀 간의 관계 (대각선, 수평, 수직)
따라서 CNN에서 영감을 받고자 함

![[13.2.1.png]]
(a) 3x3 필터가 작용하는 방식
$$
z_i^{(l+1)} = f\left( \sum_j w_j z_j^{(l)} + b \right)
$$
좌항 : $l+1$번째 레이어에서 픽셀(노드) $i$의 값
우항 : 픽셀(노드) $i$ 주변 9개의 값에 대해 filter(가중치)를 주고 편향을 더한 값에 ReLU와 같은 비선형 함수를 취한 값
>픽셀(노드)의 순서를 재배열하면 등변성을 보장하지 못함 (필터의 가중치 때문)

(b) 필터를 그래프 관점에서 재구성
$$
z_i^{(l+1)} = f\left( w_{\text{neigh}} \sum_{j \in \mathcal{N}(i)} z_j^{(l)} + w_{\text{self}} z_i^{(l)} + b \right)
$$
이때 매개변수들(가중치랑 편향)은 노드에 관계없이 공유됨
>$w_{neigh}$는 이웃 노드에 공유되는 가중치로 순서가 바뀌어도 출력은 같음 (불변성)
모든 노드에 대해 이렇게 작동하므로 노드 순서가 바뀌어도 결과는 같은 방식으로 재배열 됨 (등변성)

자기 자신도 입력에 포함되는 이유? (by. GPT)
	CNN은 주변의 패턴이 중요한거지 개별 값은 큰 의미가 없음
	하지만 GNN에서는 노드 자신의 정보도 중요함
	ex. SNS에서 친구들의 특성뿐만 아니라 해당 사용자의 특성도 반영해야함
### 13.2.2 Graph convolution networks
CNN에서 영감을 받아 그래프 데이터를 처리하기 위한 DNN
목표 : 
	1. 노드 임베딩을 비선형적이고 유연하게 변환
	2. 학습 가능한 가중치와 편향 매개변수에 대해 미분 가능하도록 설계
	3. 그래프의 $l$번째 레이어에서 $l+1$번째 레이어로 정보를 매핑

크게 2가지 단계로 구성됨
	1. **Aggregation** : 불변성을 만족하며 이웃 노드에서 정보를 수집&결합
	2. **Update** : 이웃 정보와 노드 자체의 정보를 결합해 새로운 임베딩 벡터 계산

#### Aggregation
$$
\mathbf{z}_n^{(l)} = \text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right)
$$
$\mathbf{h}_n^{(l)}$ : $l$번째 레이어의 $D$차원 노드 $n$의 임베딩 벡터 (초기 값은 $\mathbf{X}_n$)

동작 : 각 노드 $n$에 대해 이웃 노드 $\mathcal{N}(n)$의 임베딩 벡터를 결합
요구사항 :
	1. 노드마다 이웃의 수가 다를 수 있으므로, 가변 길이 입력
	2. 순열 불변성
	3. 학습 가능성 (= 미분 가능한 매개변수가 존재)

#### Update
$$
\mathbf{h}_n^{(l+1)} = \text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right)
$$
$\mathbf{h}_n^{(l+1)}$ : $l+1$번째 레이어의 $D$차원 노드 $n$의 새로운 임베딩 벡터

동작 : 집계된 정보와 노드 자체 정보를 결합한 값으로 임베딩 벡터 업데이트
요구사항 :
	1. 학습 가능성 (= 미분 가능한 매개변수가 존재)
	2. 비선형성


- 각 레이어마다 독립적인 매개변수를 가질 수도 있고, 모든 레이어에서 매개변수를 공유할 수도 있음
- 위와 같은 구조를 **메시지 전달 신경망 (Message-Passing Neural Network, MPNN)** 이라고 부름

![[13.2.2.png]]
### 13.2.3 Aggregation operators
#### 1. 단순 합
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = \sum_{m \in \mathcal{N}(n)} \mathbf{h}_m^{(l)}
$$
특징:
	이웃 노드의 임베딩 벡터를 단순히 합산
	순열 불변성을 만족하며, 이웃 노드의 개수와 관계없이 정의 가능
	학습 가능한 매개변수가 없음
문제점:
	이웃 노드의 수가 많은 노드는 적은 노드보다 더 큰 영향을 받으므로, **숫자적인 불안정성(numerical instability)** 이 발생할 수 있음
	특히, 소셜 네트워크처럼 이웃 노드 수가 몇 배 이상 차이 나는 경우 문제를 일으킬 수 있음

#### 2. 평균
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = \frac{1}{|\mathcal{N}(n)|} \sum_{m \in \mathcal{N}(n)} \mathbf{h}_m^{(l)}
$$
특징:
	이웃 노드의 개수에 따라 합산 값을 정규화
	이웃 수의 차이에 따른 불균형을 줄임
문제점:
	그래프의 구조적 정보를 일부 버리게 됨 (이웃 노드의 개수의 차이가 없음)
	단순 합보다 표현력이 낮은 경우가 있음(Hamilton, 2020)
#### 3. 정규화된 합
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = 
\sum_{m \in \mathcal{N}(n)} \frac{\mathbf{h}_m^{(l)}}{\sqrt{|\mathcal{N}(n)| |\mathcal{N}(m)|}}
$$
특징:
	각 노드와 이웃 노드의 연결 수를 고려하여 정규화
	그래프의 구조를 반영하면서 이웃의 영향력을 조정
#### 4. 요소별 최대/최소값
특징:
	이웃 노드 임베딩의 각 요소별로 최대값(또는 최소값)을 선택
	순열 불변성을 만족하며, 이웃 수와 관계없이 정의 가능
	그래프 구조보다는 강한 특징만을 반영함

#### 수용 영역(receptive field)
특정 노드의 업데이트에 영향을 미치는 다른 노드의 범위를 의미

![[13.2.3.png]]
이전 층에서 이웃들의 값이 계속 반영됨
>GNN에서는 레이어가 쌓일수록 수용 영역이 점진적으로 확장됨

문제점 : 대규모이며 sparse한 그래프에서는 많은 레이어가 필요해서 계산 비용 증가
해결책 : 슈퍼 노드를 도입해 모든 노드를 직접 연결함으로써 정보 전파 가속화
#### 학습 가능한 Aggregation operators
$$
\text{Aggregate}\left( \{ \mathbf{h}_m^{(l)} : m \in \mathcal{N}(n) \} \right) = 
\text{MLP}_{\theta} \left( \sum_{m \in \mathcal{N}(n)} \text{MLP}_{\phi}(\mathbf{h}_m^{(l)}) \right)
$$
집계 전후에 MLP를 추가해 구현함
특징 :
	매개변수 $\theta, \phi$는 모든 노드에 대해 공유되므로 불변성을 유지함
	합은 평균이나 요소별 최대 최소로 대체 가능
#### 엣지가 없는 경우
그래프에 엣지가 없으면 비구조적 데이터로 해석
MLP를 사용해 이를 학습하는 프레임워크 제공 가능 -> 딥 세트(Deep Sets)
### 13.2.4 Update operators
적절한 Aggregate operator를 선택하고 update operator의 형태를 정해야 함
#### 1. 기본적인 업데이트 연산
$$
\text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right) = 
f\left( \mathbf{W}_{\text{self}} \mathbf{h}_n^{(l)} + \mathbf{W}_{\text{neigh}} \mathbf{z}_n^{(l)} + \mathbf{b} \right)
$$
앞에서 이미 봤던 수식임
비선형성을 통해 모델의 표현력 강화
#### 2. 단순한 형태의 업데이트 연산
$$
\mathbf{h}_n^{(l+1)} = \text{Update}\left( \mathbf{h}_n^{(l)}, \mathbf{z}_n^{(l)} \right) = 
f\left( \mathbf{W}_{\text{neigh}} \sum_{m \in \mathcal{N}(n), m \neq n} \mathbf{h}_m^{(l)} + \mathbf{b} \right)
$$
$\mathbf{W}_{\text{self}} = \mathbf{W}_{\text{neigh}}$며 Aggregation을 단순 합으로 계산하면 위와 같음
계산 효율성을 높이고 구현을 간소화함
#### 노드 임베딩 초기화 방법
1. 노드의 관찰 데이터 $\mathbf{X}_n$로 설정
2. 임베딩 차원이 크면, 0 패딩 또는 선형 변환으로 매핑
#### GNN의 전체 표현
GNN은 여러 레이어로 구성되고 노드 임베딩을 반복적으로 변환함
$$
\begin{align}
\mathbf{H}^{(1)} &= \mathbf{F}\left( \mathbf{X}, \mathbf{A}, \mathbf{W}^{(1)} \right) \\ 
\mathbf{H}^{(2)} &= \mathbf{F}\left( \mathbf{H}^{(1)}, \mathbf{A}, \mathbf{W}^{(2)} \right) \\ 
&\vdots \\ 
\mathbf{H}^{(L)} &= \mathbf{F}\left( \mathbf{H}^{(L-1)}, \mathbf{A}, \mathbf{W}^{(L)} \right)
\end{align}
$$
$\mathbf{H}^{(l)}$ : $l$번째 레이어에서의 임베딩 행렬
$\mathbf{A}$ : 인접 행렬
$\mathbf{W}^{(l)}$ : $l$번째 레이어의 가중치와 편향

아래는 노드 순서 재배열에 대해 등변성을 가짐을 보여줌
$$
\mathbf{P}\mathbf{H}^{(l)} = \mathbf{F}\left( \mathbf{P}\mathbf{H}^{(l-1)}, \mathbf{P}\mathbf{A}\mathbf{P}^{\mathsf{T}}, \mathbf{W}^{(l)} \right)
$$
$\mathbf{P}$ : 순열 행렬
### 13.2.5 Node classification
### 13.2.6 Edge classification
### 13.2.7 Graph classification
# 13.3 General Graph Networks
### 13.3.1 Graph attention networks
### 13.3.2 Edge embeddings
### 13.3.3 Graph embeddings
### 13.3.4 Over-smoothing
### 13.3.5 Regularization
### 13.3.6 Geometric deep learning