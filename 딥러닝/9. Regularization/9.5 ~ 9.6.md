# 9.5 Residual Connections
딥 뉴럴 네트워크에서, 네트워크의 층 수를 늘리면 일반화 성능이 향상.

Batch normalization, 가중치 및 bias의 초기화가 Gradient vanishing/exploding 문제를 해결하는데 도움이 된다.

하지만 Batch normalization을 사용하더라도, 네트워크의 층이 깊어지면 훈련이 어려워진다(Balduzzi et al., 2017). 이를 설명하는 개념 중 하나가 바로 **깨진 기울기 현상**.

#### 깨진 기울기(shattered gradients) 현상
깨진 기울기 현상은, 네트워크가 매우 깊어질 때 발생하는 문제로, 네트워크의 gradient가 불규 칙적이고 불연속적으로 변화하는 현상. 이 현상은 기울기 기반 최적화 알고리즘(gradient-bases optimization algorithms)의 효율성을 저하시킴.
ReLU 함수를 사용하면, 네트워크가 표현할 수 있는 선형 영역의 수가 기하급수적으로 증가하지만, 이로 인해 Loss function의 기울기가 불연속이 되는 결과가 발생한다.
> 깨진 기울기 현상을 해결하는 아이디어 중 하나가 바로 Residual Connection(He et al., 2015a).

입력값의 변화(x축)에 따라 gradient의 변화(y축)을 보여주는 그림
![[9.5.1.png]]
(a) 2개의 층, (b) 25개의 층, (c) 51개의 층 + 잔차 연결

### Residual Connection
아래와 같이 세 개의 층으로 구성된 신경망이 있다고 가정
$$\begin{aligned}
z_{1}&=F_{1}(x)\\
z_{2}&=F_{2}(z_{1})\\
y&=F_{3}(z_{2}) 
\end{aligned}$$
이때 $F_{l}(.)$ 함수들은 단순히 선형변환 + ReLU일 수도 있고, 선형변환, 활성화 함수, 정규화 층이 포함된 더 복잡한 구조일 수도 있음.

#### 잔차 연결
$$
\begin{aligned}
z_1 &= F_1(x) + x \\
z_2 &= F_2(z_1) + z_1 \\
y &= F_3(z_2) + z_2
\end{aligned}
$$
각 함수의 입력값을 출력값에 다시 더하는 것을 **잔차 연결**이라고 한다. 그리고 $F_{1}(x)+x$와 같은 함수와 잔차 연결의 조합을 **잔차 블록(Residual Block)** 이라고 한다.
![[9.5.2.png]]
>이러한 구조를 ResNet(Residual Network)라고 한다.

잔차 연결을 통해, 깊은 네트워크의 학습을 안정화 시킬 수 있다.

#### 항등 변환
> 항등 변환이란? $f(x)=x$ 처럼 입력된 값이 그대로 나오는 것을 항등 변환이라고 함.

잔차 블록에서, 함수의 파라미터가 매우 작을 경우, 함수 출력값이 거의 0에 가까워 지면서, 항등 변환을 형성. 이 덕분에 역전파 과정이 더 안정적으로 이루어짐. 또한 어떤 층은 학습에 더 필요 없다고 판단되면, 그 층은 항등 변환을 수행하며 학습을 방해하지 않게 됨.

# 9.6 Model Averaging
같은 문제를 해결하기 위해 여러 다른 모델을 훈련시켰을 경우, 하나의 최적 모델을 선택하려는 대신, 개별 모델이 만든 예측값을 평균냄으로써 일반화 성능을 향상 시킬 수 있음 : **앙상블**
이렇게 앙상블 예측을 **위원회 예측**이라고 하기도 하는 듯

예를 들어 확률을 출력하는 모델의 경우, 각 모델의 평균을 통해 예측 분포를 계산할 수 있음.
$$
p(y \mid x) = \frac{1}{L} \sum_{l=1}^{L} p_l(y \mid x)
$$
여러 모델을 평균화하면, 예측의 분산을 줄이고 더 나은 예측이 가능해짐.
하지만 우리는 일반적으로 하나의 데이터셋만을 가지고 있고, 앙상블을 위해서는 **부트스트랩(Bootstrap aggregation, 또는 배깅, Bagging)** 을 고려할 수 있음.
또는 원래의 데이터 세트를 이용하여, 다양한 아키텍쳐를 가진 여러 다른 모델을 훈련할 수도 있음. ex) 랜덤포레스트, DNN, SVM을 혼용. 

#### 앙상블 예측의 장점
훈련된 모델 $y_{1}(\mathbf{x}),...,y_{M}(\mathbf{x})$가 있다고 가정하고, 앙상블을 다음과 같이 정의.
$$
y_{\text{COM}}(\mathbf{x}) = \frac{1}{M} \sum_{m=1}^{M} y_m(\mathbf{x}).
$$

우리가 예측하려는 실제 함수 $h(\mathbf{x})$가 있다면, 각 모델의 출력 $y_{m}(\mathbf{x})$는 다음과 같이 주어짐.
$$y_{m}(\mathbf{x})=h(\mathbf{x})+\epsilon_{m}(\mathbf{x})$$
이때 $\epsilon_{m}(\mathbf{x})$는 오차

각 모델에 의해 발생하는 평균 오차는 다음과 같음.
$$
E_{\text{AV}} = \frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}} \left[ \epsilon_m(\mathbf{x})^2 \right]
$$

앙상블 모델의 오차는 다음과 같음.
$$
\begin{aligned}
E_{\text{COM}} &= \mathbb{E}_{\mathbf{x}} \left[ \left\{ \frac{1}{M} \sum_{m=1}^{M} y_m(\mathbf{x}) - h(\mathbf{x}) \right\}^2 \right], \\
&= \mathbb{E}_{\mathbf{x}} \left[ \left\{ \frac{1}{M} \sum_{m=1}^{M} \epsilon_m(\mathbf{x}) \right\}^2 \right].
\end{aligned}
$$

놀라운 사실! 오류가 평균 0이고, 서로 무상관이라는 가정하에,
$$
\mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})] = 0, \quad 
\mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x}) \epsilon_l(\mathbf{x})] = 0 \quad (m \neq l),
$$
이런 결과가 나옴.
$$E_\mathbf{COM}=\frac{1}{M}E_\mathbf{AV}$$
이는, 앙상블 모델이 개별 모델의 평균 오차를 $M$배나 줄일 수 있다는 것...
하지만 이는 개별 모델의 오차가 서로 상관이 없다는 아주 강력한 가정에 의존, 실제로는 오차가 일반적으로 높은 상관성을 가진다고 합니다. 하지만 그래도,
$$E_\mathbf{COM}\leq E_\mathbf{AV}$$
임을 증명할 수 있습니다.

#### 부스팅
모델 결합에 대한 또 다른 접근법으로, Boosting이 존재.
부스팅은 여러 기본 분류기를 결합하여 기존 기본 분류기 중 어떤 것보다도 성능이 더 뛰어난 형태의 앙상블을 만듦.

> 그러면 부스팅과 배깅의 차이점은 뭔가요?

부스팅에서는, 기본 분류기들이 순차적으로 훈련됨. 각 기본 분류기는 데이터 세트의 가중치를 사용하여 훈련되며, 각 데이터 포인트의 가중치는 이전 분류기의 성능에 따라 달라짐.

> 여기서는 AdaBoost에 대해서만 설명하나봐요

### 9.6.1 Dropout
드롭아웃은, 뉴럴 네트워크 학습 과정에서 **임의로 뉴런을 끄는(drop)** 기법. 학습 과정에서 랜덤하게 선택된 뉴런들만 참여하는 축소된 네트워크를 학습하는 효과를 나타냄. 드롭아웃을 활용하면, 여러 개별 모델을 훈련하지 않고도 지수적으로 많은 모델에 대한 앙상블을 하는 것과 같은 효과를 볼 수 있음. 드롭아웃은, 입력 노드와 히든 노드에만 적용됨.
![[9.6.1.png]]

> 지수적으로?
> 뉴런이 $N$개 있으면, 드롭아웃이 만들어 낼 수 있는 뉴런의 조합은 $2^{N}$개.

드롭아웃되는 노드의 출력을 0으로 만드는 것으로 드롭아웃을 구현할 수 있음. 레이어별로 독립적으로 마스크 벡터 $R_{i}\in\{0,1\}$ 정의하여 구현할 수 있음.
#### 마스크 벡터
$\mathbf{R}^{(l)}\in\{0,1\}^{n_{l}}$ : $l$번째 레이어의 마스크 벡터, $n_{l}$은 해당 레이어의 뉴런 수
마스크 벡터 $\mathbf{R}^{(l)}$의 각 원소는, 확률 $\rho$에 따라 1로 설정되거나 0으로 설정됨. (일반적으로 input 노드에서는 0.8, hidden 노드에서는 0.5로 설정)

각 레이어 $l$의 뉴런 활성화 값 $\mathbf{a}^{(l)}$에 대해, 
$$
\mathbf{a}_{\text{dropped}}^{(l)} = \mathbf{a}^{(l)} \odot \mathbf{R}^{(l)}
$$
이 계산을 통해서 드롭아웃된 뉴런의 출력은 0으로 되고, 나머지 뉴런은 그대로 활성화. 이때 $\odot$은 element-wise multiplicatioin(그냥 벡터 내적이라 생각하면 됨)

이렇게 마스크 벡터에 의해 일부 노드가 비활성화된 상태에서, 노드를 훈련해나감.

#### Monte Carlo Dropout
최종 예측에서도 이러한 드롭아웃을 사용.
10개~20개 정도의 마스크($R_{1},...,R_{K}$)를 만들고, 이 마스크를 적용하여 나온 예측 분포를 평균내어 최종 예측 분포를 만들어 냄.
$$
p(y \mid x) \approx \frac{1}{K} \sum_{k=1}^{K} p(y \mid x, R_k)
$$
이때, $R_{k}$는 모든 레이어의 마스크 벡터 $\{R_{k}^{(1)},...,R_{k}^{(L)}\}$를 포함함.
이 과정을 통해 앙상블 효과를 얻을 수 있음. (대 상 블... 또 당신입니까)

예측 단계에서, 드롭아웃을 사용하지 않고 모든 노드를 활성화 시킨 후 각 노드의 출력에 가중치 $\rho$를 곱하여 보정하는 아이디어도 있음.
