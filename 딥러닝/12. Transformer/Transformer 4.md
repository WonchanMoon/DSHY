**Multimodal Transformer**
트랜스포머는 입력 데이터에 대한 가정이 거의 없기 때문에 여러 딥러닝 분야에 적용 가능.
입력 데이터를 적절히 토큰화하고 출력 토큰을 디코딩 할 수 있다면, 트랜스포머를 활용할 가능성이 높음.

## 12.4.1 Vision transformers
트랜스포머는 최근 CV에서도 좋은 성공을 거두고 있음.

트랜스포머를 사용할 때, 우리는 이미지를 토큰으로 어떻게 변환할 것인지 결정해야한다.
#### 토큰화 방법 1
가장 흔한 접근은, **이미지를 같은 크기의 패치로 토큰화** 하는 것이다.
원본 이미지가 $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ 의 차원을 갖고 있다고 생각해보자. 이때 $H, W$는 각각 높이, 너비, $C$는 색 채널의 개수.
이 이미지를 non-overlapping $P\times P$ 차원의 패치로 자르고, 각각의 패치는 1차원 벡터로 flatten 된다.
> 색 채널 별로 패치를 구하고, flatten 할 때 하나로 합침

$$\mathbf{x}\in\mathbb{R}^{H\times W\times C}\quad\rightarrow\quad \mathbf{x}_{p}\in\mathbb{R}^{N\times(P^{2}\times C)} $$
이때 $N=HW/ P^{2}$로, 원본 이미지에서 분할된 패치의 개수이다.
하나의 패치가 flatten 된 벡터 $\mathbf{x}_{p}$는 $P^{2}\times C$ 차원의 벡터이다.

#### 토큰화 방법 2
이미지를 토큰화하는 다른 접근법은, 원본 이미지를 패치로 나누는게 아니라 **이미지를 CNN에 통과 시키고 만들어진 feature map을 패치로 나눔**
이미지를 CNN 인코더(ResNet18, EfficientNet)를 사용하여 **원본 이미지보다 더 작은 크기의 feature map 생성**
이렇게 생성된 feature map은, CNN의 convolution 연산을 거치면서 이미지의 공간적 정보를 유지한 채 특징이 추출된다는 장점이 있음.

위의 두 방법 중 하나로 토큰화가 완료된 이후에는, 학습을 수행.
하지만 이때, 각 토큰의 위치정보($E_{\text{pos}}$)를 넣어주어야함.
$$\mathbf{z}_{p}=\mathbf{x}_{p}+E_{\text{pos}}$$

>근데 이게 참 신기한게,

각 패치가 원래 어느 위치였는지 고정된 위치 인코딩(Fixed Positional Encoding)을 넣어주면 모델의 성능이 그렇게 향상되진 않음. 하지만 위치 정보마저 학습하도록 Learned Positional Encoding을 넣어주면(초깃값은 랜덤), 모델이 알아서 **최적의 위치를 학습**하게 됨. 
> 이게 말이 되나


![[12.4.1.png]]

## 12.4.2 Generative Image Transformers
언어 분야에서 가장 인상적인 결과는, 트랜스포머를 자기회귀 생성 모델로 활용하여 텍스트를 생성할 때 나타났음. 따라서 자연스러운 의문, *트랜스포머를 이미지 생성에 활용할 수 있을까?* 자연어는 순차적이므로 자기회귀 프레임워크에 잘 맞지만, 이미지는 자연스러운 순서가 존재하지 않으므로 자기회귀를 사용하는 것이 유용할 것이라고 생각하기 어려움.

하지만 모든 분포는, 변수들의 순서가 주어지면 조건부로 쪼갤 수 있음.
$$p(\mathbf{x}_1, \dots, \mathbf{x}_N) = \prod_{n=1}^{N} p(\mathbf{x}_n \mid \mathbf{x}_1, \dots, \mathbf{x}_{n-1}).
$$
우리가 이미지에서 순서를 만들어주는 방법 중 하나로 **raster scan**이 있음.
![[12.4.2.png]]
그리고 raster scan을 이용하는 자기회귀 모델은 아래와 같이 진행됨.
![[12.4.3.png]]

자 이제 위에서 처럼 이미지의 픽셀에 순서를 부여했고, **각 픽셀의 값(RGB 색상)을 연속적인 변수로 처리하여 이미지를 생성**했더니, 이미지가 blurry 하게 나옴. 그렇다고 **색상을 black and white로 처리하자니, 표현력이 제한**됨. 

따라서 RGB 각각의 값을 이산적으로 처리해야함. 근데 이렇게 하자니 각 픽셀이 가질 수 있는 값의 개수가 $2^{24}$. 각 픽셀을 독립적으로 처리하자니, 픽셀별로 하나하나 예측해야하므로 계산 비용 폭발

이를, **비슷한 색상들을 하나의 토큰으로 묶는 벡터 양자화**를 통해 해결 가능.

하지만 이때도, 픽셀 하나하나 처리하면 너무 느림. 따라서 패치를 나눠 패치 단위로 처리.

## 12.4.3 Audio data
오디오 데이터, 소리 데이터는 파형을 갖고 있음. 이 파형을 딥러닝 모델의 입력으로 사용할 수 있지만, 멜 스펙트로그램으로 변환하는 것이 더 효과적.

> 멜 스펙트로그램?
> ![[12.4.4.png]]
> *메다르다 가문의 분노다!*

> 멜 스펙트로그램은, column에 시간 단계를 나타내고, row에 주파수를 나타냄.
> 그리고 각 셀의 값은 해당 시간과 주파수에서의 신호 강도(진폭)을 나타냄.
> 음성 데이터를 Short-Time Fourier Transform을 이용하여 구할 수 있음. ![[12.4.5.png]]

오디오 분야에서 트랜스포머가 활용되는 한 예는, 오디오 분류(Audio Classification). 최근에는 오디오 분류에서 트랜스포머 인코더 모델을 오디오 분류에 적용했음. 멜 스펙트로그램을 이미지로 간주한 후, 이를 토큰화하여 ViT처럼 적용할 수 있음.

## 12.4.4 Text-to-speech
>트랜스포머는 특정 화자의 음성을 모방하는 방식으로 음성을 합성하는데 성공하여, 트랜스포머의 범용성을 입증한 사례가 있다.

전통적인 TTS 방법에서는, 특정 화자의 음성 녹음을 수집한 후, 해당 음성을 입력으로 사용하여 멜 스펙트로 그램을 출력하도록 하는 회귀 모델을 학습시켰다. 하지만 이러한 기존 방법은, *1) 좋은 일반화 성능을 위해 비현실적으로 많은 데이터가 필요*하고, *2) 화자마다 지식이 transfer되지 않아서 화자가 추가될 때 마다 많은 데이터를 필요*로 한다. 그리고 *3) 여러 가지 정답이 존재할 수 있으므로, 단순 회귀 모델은 적합하지 않을 수 있음.*

*오디오 데이터를 자연어와 동일한 방식으로 처리*하고, *TTS를 조건부 언어 모델링(conditional language modeling) 문제로 구성*하면, 이를 LLM과 유사한 방식으로 학습 시킬 수 있을 것임.

#### Vall-E
Vall-E(Wang et al., 2023)는 트랜스포머와 언어 모델링 기법을 활용하여 TTS 방법을 제시.
단 몇 초의 샘플 음성만으로 새로운 화자의 목소리를 학습하여 텍스트를 읽게 할 수 있음.
> 진짜 무섭다 그죠?

![[12.4.6.png]]

다양한 화자의 데이터를 포함하여 학습하면, 새로운 화자의 짧은 샘플 음성과 새로운 텍스트로 해당 화자가 그 텍스트를 읽게 할 수도 있음.

## 12.4.5 Vision and language transformers
> 멀티모달로의 확장 가능? 에 대한 이야기

여기서는 텍스트와 이미지 데이터를 조합하는 방법에 대해서만 이야기하지만, 여기서 얘기하는 접근 방식들은 다른 모달리티에도 확장이 가능하다.

*LAION-400M 데이터셋*은 text-to-image 생성, text-to-image 캡셔닝에서 연구를 가속화하였다.
> 마치 ImageNet이 이미지 분류 모델 개발에 큰 역할을 한 것 처럼

트랜스포머를 사용할 경우, 이미지 토큰을 디코딩 할 때 텍스트 토큰을 추가 입력으로 제공하는 방식이기에 비교적 간단하게 접근할 수 있다. 이러한 접근 방식을 활용하면, text-to-image 생성 문제를 그저 sequence-to-sequence 문제로 바라볼 수 있다는 장점이 있다. 따라서 *완전한 인코더-디코더 트랜스포머 모델을 선택*하는 것이 매우 합리적이다. 이러한 접근 방식을 채택한 대표적인 모델이 **Parti(Yu et al., 2022)**. 200B 크기로 매우 크지만, 크기가 커질수록 성능이 꾸준히 향상하는 것을 확인
![[12.3.4.png]]

또한 모든 데이터를 토큰 시퀀스로 취급하는 연구도 많이 진행됐음.
**CM3(Aghajanyan et al., 2022), CM3Leon(Yu et al., 2023)**
![[12.4.7.png]]

위 그림에서 볼 수 있듯이, CM3Leon 하나의 모델로 매우 다양한 작업을 수행할 수 있음.

### CM3Leon vs GPT 4o
> GPT의 생각은?

📌 **비교해보자면**
- **CM3Leon & Parti** 👉 **멀티모달 데이터 학습**에 특화된 모델로, 특히 **텍스트-이미지 생성**이나 **이미지 편집, 캡셔닝** 같은 작업에 강함.
- **GPT-4o** 👉 훨씬 더 **범용적인 AI**로, **텍스트, 이미지, 코드, (일부 음성까지) 모두 이해하고 처리 가능**. 더 다양한 작업을 수행할 수 있음.
즉, **CM3Leon 같은 모델은 특정 작업에 최적화된 전문가형 모델**이라면, **GPT-4o는 전반적으로 더 많은 작업을 수행할 수 있는 종합형 AI**라고 볼 수 있어! 😎

그러니까 **"누가 더 뛰어나냐"** 보다는 **"어떤 작업에 더 적합하냐"가 중요한 포인트! 🔥  
하지만 솔직히 말하면, 나는 텍스트 기반의 대화, 설명, 코드 생성, 논리적 사고 같은 전반적인 부분에서 더 강한 편**이긴 해! 😆

