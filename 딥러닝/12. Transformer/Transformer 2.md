# 12.2. Natural Language

## 12.2.1 Word Embedding
**Embedding Process**는 $D\times K$ 크기의 행렬 $\mathbf{E}$로 정의할 수 있다. 여기서 $D$는 임베딩 공간의 차원이고, $K$는 dictionary의 차원. 원핫 인코딩된 입력 벡터 $\mathbf{x}_{n}$에 대해, 임베딩 벡터 $\mathbf{v}_{n}$은 다음 식으로 계산이 가능함.
$$\mathbf{v}_{n}=\mathbf{E}\mathbf{x_{n}
}$$

행렬 $\mathbf{E}$는 텍스트로 이루어진 큰 데이터 셋(코퍼스)으로 학습할 수 있음. 이를 위한 다양한 방식이 존재하고, 우리 교재에서는 **word2vec**을 볼거임.

### Word2Vec
word2vec은, 두 개의 층으로 이루어진 신경망으로 볼 수 있음.

학습 데이터셋은, 텍스트에서 **$M$개의 인접한 단어**로 이루어진 **윈도우**를 고려하여 샘플을 형성. 일반적으로 $M=5$. 이 접근 방식에는 두 가지 접근 방식이 있음.
**Continuous Bag of Words(CBOW)** 와 **Skip-gram**
![[12.2.1.png]]

#### Continuous Bag of Words (CBOW)
주어진 문맥 단어들을 input으로 받아, 중심 단어를 예측하는 방법.

>지피티의 예시
예를 들어, "The quick brown fox jumps over the lazy dog"라는 문장에서,
중심 단어가 "fox"라면, 
문맥 단어는 {"quick", "brown", "jumps", "over"}일 수 있습니다.
#### Skip-gram
주어진 중심 단어를 input으로 받아, 주변 문맥 단어를 예측하는 방법.


이 학습 절차는, 일종의 self-supervised learning이라고 볼 수 있음. 
데이터는 라벨이 없는 대규모 코퍼스이며, 무작위로 작은 단어 시퀀스의 윈도우가 추출됨. 네트워크가 예측하려는 단어를 마스킹처리하여, 라벨이 텍스트 자체에서 생성됨. 모델이 학습된 후, 임베딩 행렬 $\mathbf{E}$를 다음과 같이 결정.
- CBOW 에선, **transpose of the second-layer weight matrix** for CBOW
- skip-gram 에선, **first-layer weight matrix** for skip-gram

이렇게 단어 임베딩을 마치고 나면, 의미적으로 비슷한 단어는 임베딩 공간에서 가까운 위치로 매핑됨. 임베딩된 벡터를 통해 벡터 연산이 가능해짐. 
> ex: '파리는 프랑스에 해당하고, 로마는 이탈리아에 해당한다'를 임베딩 벡터 연산으로 표현하면, $$\mathbf{v}(\text{Paris}) - \mathbf{v}(\text{France}) + \mathbf{v}(\text{Italy}) \simeq \mathbf{v}(\text{Rome})$$
> 처럼 나타낼 수 있음

임베딩은 원래 자연어 처리를 위한 도구로 개발됨. 하지만 요즘은 deep neural network를 위한 전처리 단계로 더 많이 사용. 이러한 관점에서, 임베딩은, DNN 학습의 첫번째 레이어로 생각할 수 있음. 임베딩은, **이미 사전학습된 임베딩 행렬을 사용**하거나, **시스템의 전체 end-to-end 학습과정의 일부로 학습**되거나 할 수 있음. 만약에 내가 후자를 택한다면, 임베딩 레이어는 무작위로 initialize하거나, 표준 임베딩 행렬을 사용할 수 있음.

> 표준 임베딩 행렬?
> : 이미 학습된 임베딩 벡터를 포함하고 있는 사전 정의된 행렬
> : Google의 **Word2Vec**, Stanford의 **GloVe**, Facebook의 **FastText** 등
> : Transformer 기반 임베딩 **BERT**, **GPT**도 있음.

## 12.2.2 Tokenization
우리가 고정된 단어 사전을 사용한다면, 문제가 많음. 내 사전에 없는 단어는 처리를 못하거나, 오타가 난 단어를 처리할 수 없음. 이를 해결하기 위한 접근법으로는, 단어 수준이 아니라 문자 수준에서 작업하는 것이 가능함. 
> 문자 수준 작업에서는, hello 를 h, e, l, l, o로 하나하나 쪼개서 인식함.

문자 수준 작업에서는, 오타 교정, 컴퓨터 코드, 수학 공식 들도 잘 다룰 수 있다는 장점이 있음. 그러나 이러한 접근법은, 단어 구조를 폐기해버리고, 주어진 텍스트를 처리하기 위해 훨씬 많은 단계가 필요하고 계산 비용이 엄청 증가한다는 단점이 있음.

단어 수준과 문자 수준 작업의 이점을 결합할 수 있는 방법은, 바로 **token으로 변환하는 전처리 단계를 수행**하는 것임. 여기서 **토큰, token**은 일반적으로 작은 문자 집합으로 구성됨. 하지만 전체 단어가 포함될 수도 있고, 더 긴 단어의 조각도 포함될 수 있음. 

이런 **tokenization**은, 언어가 아닌 다른 종류의 시퀀스(컴퓨터 코드, 수식)을 처리하거나, 다른 형태(이미지)를 처리할 수 있도록 해줌. 또한, cook, cooks, cooked는 모두 cook을 공유하고, cook 자체를 하나의 토큰으로 처리해버릴 수 있음.

tokenization에는 다양한 접근 방식이 있음. 그 중 하나로 **Byte Pair Encoding, BPE**가 있음. 이 과정은 개별 문자에서 시작하여 더 긴 문자열로 병합해 나감. 처음에는 개별 문자들의 리스트로 초기화 되고, 이후 텍스트 본문에서 가장 자주 나타나는 **인접한 토큰 쌍**을 찾아 새 토큰으로 대체. 하지만 이때 단어가 병합되지 않도록, 두 번째 토큰이 공백 문자로 시작되는 경우에는 새로운 토큰이 생성되지 않음.
![[12.2.2.png]]

## 12.2.3 Bag of words
이제 순서가 있는 벡터들의 시퀀스(예를 들어 자연어에서 단어, 토큰들)의 결합 분포를 모델링하는 과정을 다룸. 가장 간단한 방법은, 단어들이 동일한 분포로부터 독립적으로 추출된다고 가정하는 것.
$$p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = \prod_{n=1}^{N} p(\mathbf{x}_n)
$$
이 모델의 파라미터(각 단어의 확률)의 MLE는, 학습 데이터에서 해당 단어가 나타난 비율.

이러한 접근법이 Bag of words이고, 이 방법은 단어의 순서를 완전히 무시하게 되는 단점이 있음.

우리는 Bag of words 접근법을 사용하여, 간단한 텍스트 분류기를 구성할 수 있음. 예를 들어 레스토랑 리뷰를 긍정과 부정으로 분류하는 sentiment analysis(감정 분석). 교재에서는 나이브 베이즈 분류기를 만들었음(생략). 하지만 테스트 데이터에, 학습 데이터에 없던 단어가 등장할 경우 확률이 0으로 되어버리는데, 이때 smoothing을 통해 이 문제를 해결할 수 있음. 예를 들어 **라플라스 스무딩**으로, 모든 확률값에 일정한 값을 더하는 방법이 있음.

## 12.2.4 Autoregressive model
> Bag of Words의 문제점.
> 너무 당연하지만, 단어의 순서를 무시하게 됨.

아하 ! Autoregressive한 아이디어로 접근해보자.
![[12.2.3.png]]
$$p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = \prod_{n=1}^{N} p(\mathbf{x}_n \mid \mathbf{x}_1, \ldots, \mathbf{x}_{n-1})
$$
우리는 위 식의 우변의 각 항(예를 들어 $p(\mathbf{x}_{2}|\mathbf{x}_{1})$을, 표로 표현할 수 있다.
$$
\begin{array}{|c|c|c|}
\hline
x_1 & p(x_2 = 0 \mid x_1) & p(x_2 = 1 \mid x_1) \\
\hline
0 & 0.7 & 0.3 \\
1 & 0.4 & 0.6 \\
\hline
\end{array}
$$

여기서 이 다음 경우, $p(\mathbf{x}_{3}|\mathbf{x}_{2}, \mathbf{x}_{1})$를 나타낸다면 표는 아래와 같다.
$$
\begin{array}{|c|c|c|c|}
\hline
x_1 & x_2 & p(x_3 = 0 \mid x_1, x_2) & p(x_3 = 1 \mid x_1, x_2) \\
\hline
0 & 0 & 0.5 & 0.5 \\
0 & 1 & 0.6 & 0.4 \\
1 & 0 & 0.3 & 0.7 \\
1 & 1 & 0.2 & 0.8 \\
\hline
\end{array}
$$
이 처럼, 변수의 개수가 늘어감에 따라 표의 크기는 지수적으로 증가한다.
-> 매우 비 효율적

아하 ! 가장 최근의 것 $L$개에만 의존하게 하자.
예시: $L=2$
$$p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = p(\mathbf{x}_1)p(\mathbf{x}_2 \mid \mathbf{x}_1) \prod_{n=3}^{N} p(\mathbf{x}_n \mid \mathbf{x}_{n-1}, \mathbf{x}_{n-2})
$$
> $L=1$ : bi-gram
> $L=2$ : tri-gram
> $L=n$ : n-gram

고품질의 텍스트 모델을 만들려면, 언어의 장기적인 의존성을 고려해야함. 하지만 위의 모델에서 장기적인 의존성을 얻기 위해 $L$을 키우는 것은, 계산 비용이 지수적으로 증가하기에 불가능.
장기적인 의존성을 허용하면서도, n-gram 모델의 매개변수 수가 지수적으로 증가하는 것을 피할 수 있는 한 방법은 히든 마르코프 모델(HMM)을 사용하는 것.


## 12.2.5 RNN
**Recurrent Neural Network**는, HMM에서 영감을 받아 설계되었음.
RNN은 각 시퀀스 단계에서, Hidden State를 활용하여 이전 정보를 기억함.
입력된 단어와 이전 숨겨진 상태를 받아 현재 상태와 출력을 계산.
**각 단계에서 가중치를 공유**하며, 긴 시퀀스를 처리하면서도 매개변수의 수를 효율적으로 유지할 수 있음.

![[12.2.4.png]]
> 그림 설명

인코더 : 입력 시퀀스를 처리하여 하나의 벡터 $\mathbf{z}^{*}$로 압축.
디코더 : $\mathbf{z}^{*}$를 기반으로 출력 시퀀스를 생성, 이때 이전에 생성된 단어를 입력으로 사용하는 자기회귀 구조를 따름.

RNN 역시, **Gradient Descent로 학습**. 
하지만 시퀀스가 길어지면 **gradient vanishing, exploding**이 발생. 또한 **장기 의존성 문제**도 있음.

이러한 문제를 해결하는 가장 대표적인 예시가 바로 **LSTM**(Long Short-Term Memory), **GRU**(Gated Recurrent Unit). 하지만 이들 역시 RNN에 비해 성능은 향상되었지만, 여전히 문제점들이 있음.
1. 장기 의존성 모델링에 한계가 있음. 
2. 또한 LSTM은 RNN에 비해 학습 속도도 느림. 
3. 그리고 RNN은 순환구조 때문에, 하나의 학습 샘플 내에서 병렬 계산을 지원하지 못함.

위 문제들을 트랜스포머를 이용함으로써 해결

