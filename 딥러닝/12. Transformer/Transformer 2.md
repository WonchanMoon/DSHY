# 12.2. Natural Language

## 12.2.1 Word Embedding
**Embedding Process**는 $D\times K$ 크기의 행렬 $\mathbf{E}$로 정의할 수 있다. 여기서 $D$는 임베딩 공간의 차원이고, $K$는 dictionary의 차원. 원핫 인코딩된 입력 벡터 $\mathbf{x}_{n}$에 대해, 임베딩 벡터 $\mathbf{v}_{n}$은 다음 식으로 계산이 가능함.
$$\mathbf{v}_{n}=\mathbf{E}\mathbf{x_{n}
}$$

행렬 $\mathbf{E}$는 텍스트로 이루어진 큰 데이터 셋(코퍼스)으로 학습할 수 있음. 이를 위한 다양한 방식이 존재하고, 우리 교재에서는 **word2vec**을 볼거임.

### Word2Vec
word2vec은, 두 개의 층으로 이루어진 신경망으로 볼 수 있음.

학습 데이터셋은, 텍스트에서 **$M$개의 인접한 단어**로 이루어진 **윈도우**를 고려하여 샘플을 형성. 일반적으로 $M=5$. 이 접근 방식에는 두 가지 접근 방식이 있음.
**Continuous Bag of Words(CBOW)** 와 **Skip-gram**
![[12.2.1.png]]

#### Continuous Bag of Words (CBOW)
주어진 문맥 단어들을 input으로 받아, 중심 단어를 예측하는 방법.

>지피티의 예시
예를 들어, "The quick brown fox jumps over the lazy dog"라는 문장에서,
중심 단어가 "fox"라면, 
문맥 단어는 {"quick", "brown", "jumps", "over"}일 수 있습니다.
#### Skip-gram
주어진 중심 단어를 input으로 받아, 주변 문맥 단어를 예측하는 방법.


이 학습 절차는, 일종의 self-supervised learning이라고 볼 수 있음. 
데이터는 라벨이 없는 대규모 코퍼스이며, 무작위로 작은 단어 시퀀스의 윈도우가 추출됨. 네트워크가 예측하려는 단어를 마스킹처리하여, 라벨이 텍스트 자체에서 생성됨. 모델이 학습된 후, 임베딩 행렬 $\mathbf{E}$를 다음과 같이 결정.
- CBOW 에선, **transpose of the second-layer weight matrix** for CBOW
- skip-gram 에선, **first-layer weight matrix** for skip-gram

이렇게 단어 임베딩을 마치고 나면, 의미적으로 비슷한 단어는 임베딩 공간에서 가까운 위치로 매핑됨. 임베딩된 벡터를 통해 벡터 연산이 가능해짐. 
> ex: '파리는 프랑스에 해당하고, 로마는 이탈리아에 해당한다'를 임베딩 벡터 연산으로 표현하면, $$\mathbf{v}(\text{Paris}) - \mathbf{v}(\text{France}) + \mathbf{v}(\text{Italy}) \simeq \mathbf{v}(\text{Rome})$$
> 처럼 나타낼 수 있음

임베딩은 원래 자연어 처리를 위한 도구로 개발됨. 하지만 요즘은 deep neural network를 위한 전처리 단계로 더 많이 사용. 이러한 관점에서, 임베딩은, DNN 학습의 첫번째 레이어로 생각할 수 있음. 임베딩은, **이미 사전학습된 임베딩 행렬을 사용**하거나, **시스템의 전체 end-to-end 학습과정의 일부로 학습**되거나 할 수 있음. 만약에 내가 후자를 택한다면, 임베딩 레이어는 무작위로 initialize하거나, 표준 임베딩 행렬을 사용할 수 있음.

> 표준 임베딩 행렬?
> : 이미 학습된 임베딩 벡터를 포함하고 있는 사전 정의된 행렬
> : Google의 **Word2Vec**, Stanford의 **GloVe**, Facebook의 **FastText** 등
> : Transformer 기반 임베딩 **BERT**, **GPT**도 있음.

## 12.2.2 Tokenization
우리가 고정된 단어 사전을 사용한다면, 문제가 많음. 내 사전에 없는 단어는 처리를 못하거나, 오타가 난 단어를 처리할 수 없음. 이를 해결하기 위한 접근법으로는, 단어 수준이 아니라 문자 수준에서 작업하는 것이 가능함. 
> 문자 수준 작업에서는, hello 를 h, e, l, l, o로 하나하나 쪼개서 인식함.

문자 수준 작업에서는, 오타 교정, 컴퓨터 코드, 수학 공식 들도 잘 다룰 수 있다는 장점이 있음. 그러나 이러한 접근법은, 단어 구조를 폐기해버리고, 주어진 텍스트를 처리하기 위해 훨씬 많은 단계가 필요하고 계산 비용이 엄청 증가한다는 단점이 있음.

단어 수준과 문자 수준 작업의 이점을 결합할 수 있는 방법은, 바로 **token으로 변환하는 전처리 단계를 수행**하는 것임. 여기서 **토큰, token**은 일반적으로 작은 문자 집합으로 구성됨. 하지만 전체 단어가 포함될 수도 있고, 더 긴 단어의 조각도 포함될 수 있음. 

이런 **tokenization**은, 언어가 아닌 다른 종류의 시퀀스(컴퓨터 코드, 수식)을 처리하거나, 다른 형태(이미지)를 처리할 수 있도록 해줌. 또한, cook, cooks, cooked는 모두 cook을 공유하고, cook 자체를 하나의 토큰으로 처리해버릴 수 있음.

tokenization에는 다양한 접근 방식이 있음. 그 중 하나로 **Byte Pair Encoding, BPE**가 있음. 이 과정은 개별 문자에서 시작하여 더 긴 문자열로 병합해 나감. 처음에는 개별 문자들의 리스트로 초기화 되고, 이후 텍스트 본문에서 가장 자주 나타나는 **인접한 토큰 쌍**을 찾아 새 토큰으로 대체. 하지만 이때 단어가 병합되지 않도록, 두 번째 토큰이 공백 문자로 시작되는 경우에는 새로운 토큰이 생성되지 않음.
![[12.2.2.png]]

## 12.2.3 Bag of words
이제 순서가 있는 벡터들의 시퀀스(예를 들어 자연어에서 단어, 토큰들)의 결합 분포를 모델링하는 과정을 다룸. 가장 간단한 방법은, 단어들이 동일한 분포로부터 독립적으로 추출된다고 가정하는 것.
$$p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = \prod_{n=1}^{N} p(\mathbf{x}_n)
$$
이 모델의 파라미터(각 단어의 확률)의 MLE는, 학습 데이터에서 해당 단어가 나타난 비율.

이러한 접근법이 Bag of words이고, 이 방법은 단어의 순서를 완전히 무시하게 되는 단점이 있음.

우리는 Bag of words 접근법을 사용하여, 간단한 텍스트 분류기를 구성할 수 있음. 예를 들어 레스토랑 리뷰를 긍정과 부정으로 분류하는 sentiment analysis(감정 분석). 교재에서는 나이브 베이즈 분류기를 만들었음(생략). 하지만 테스트 데이터에, 학습 데이터에 없던 단어가 등장할 경우 확률이 0으로 되어버리는데, 이때 smoothing을 통해 이 문제를 해결할 수 있음. 예를 들어 **라플라스 스무딩**으로, 모든 확률값에 일정한 값을 더하는 방법이 있음.

## 12.2.4 Autoregressive model

## 12.2.5 RNN