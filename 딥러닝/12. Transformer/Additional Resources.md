# Beam Search
입력 문장이 `"I am"`이라고 가정하고, 모델이 다음 단어를 예측하는 과정을 보겠습니다.

> $B=2$ 가정
### Step 1: 초기 가설
- 모델은 `"I am"` 다음에 올 토큰의 확률 분포를 계산합니다:
	- `happy (0.5)`, `sad (0.3)`, `tired (0.2)`
- 상위 2개의 토큰을 선택합니다: **(상위 $B$ 개의 토큰)**
	- 가설 1: `"I am happy" (0.5)`
	- 가설 2: `"I am sad" (0.3)`
        
### Step 2: 가설 확장
- 각 가설에 대해 다음 토큰의 확률을 계산합니다:
	- `"I am happy"` → `today (0.6)`, `now (0.4)`
	- `"I am sad"` → `today (0.5)`, `now (0.5)`
- 확장된 가설과 확률: **($B^{2}$ 개)** 
	- `"I am happy today" (0.5 × 0.6 = 0.3)`
	- `"I am happy now" (0.5 × 0.4 = 0.2)`
	- `"I am sad today" (0.3 × 0.5 = 0.15)`
	- `"I am sad now" (0.3 × 0.5 = 0.15)`

### Step 3: Pruning
- 상위 2개의 가설을 선택: **(상위 $B$ 개의 토큰)**
	- `"I am happy today" (0.3)`
	- `"I am happy now" (0.2)`

### 반복 
- 위 과정을 반복하며 시퀀스를 확장합니다.


# BERT-mask

좋은 질문이야! 새로운 데이터셋에서도 **마스크(⟨mask⟩)** 를 만들어 사용하는 방법도 생각해볼 수 있지만, 그게 적합하지 않은 이유들이 있어. 아래에서 자세히 설명할게:

---
### 1. 미세 조정의 목적은 특정 태스크에 적합한 모델을 만드는 것
- **사전 학습(pre-training)** 은 대규모 데이터셋에서 일반적인 언어 패턴(문법, 의미 관계 등)을 학습하기 위한 단계야.
- 반면, **미세 조정(fine-tuning)** 은 특정 태스크(예: 감정 분석, 질문 응답, 문서 분류)에 맞게 모델을 최적화하는 단계야.
- 만약 미세 조정 단계에서도 ⟨mask⟩를 만든다면, 모델은 여전히 "마스킹된 단어를 예측하는 문제"를 학습하게 돼. 그러나 실제 태스크(예: 감정 분류)는 단어를 예측하는 것이 아니라 입력된 텍스트의 특정 특성을 이해하고 활용하는 거지. 따라서, ⟨mask⟩를 사용하는 것은 미세 조정의 목표와 맞지 않아.

---
### 2. 미세 조정 데이터셋의 크기 문제
- 일반적으로, 미세 조정에 사용되는 데이터셋은 사전 학습 데이터셋에 비해 훨씬 작은 경우가 많아.
- 만약 작은 데이터셋에서 ⟨mask⟩를 사용해 학습한다면, 이미 부족한 데이터셋에서 모델이 배워야 할 중요한 정보를 마스킹해버릴 위험이 있어. 이는 학습 효율을 떨어뜨릴 가능성이 높아.

---
### 3. 사전 학습과 미세 조정의 역할 분리
- 사전 학습 단계는 모델이 언어의 일반적인 구조를 학습하는 데 초점이 맞춰져 있고, 여기에서 ⟨mask⟩를 사용하는 게 효과적이야.
- 미세 조정 단계에서는 모델이 실제 태스크의 입력 데이터와 최대한 유사한 데이터를 처리할 수 있어야 해. 새로운 데이터셋에서 ⟨mask⟩를 사용하면, 모델이 "태스크 자체"보다는 "⟨mask⟩ 예측"에 초점을 맞출 가능성이 있어.
---
### 4. 실제 데이터와의 불일치 문제
- ⟨mask⟩는 학습 과정에서만 사용되는 "인공적인 요소"야. 실제로 모델이 적용될 환경(예: 감정 분석 애플리케이션)에서는 ⟨mask⟩ 같은 토큰이 없기 때문에, 미세 조정 단계에서 이를 사용하는 건 현실적인 데이터와의 괴리를 키우는 결과를 낳을 수 있어.
---
### 결론
미세 조정 데이터셋에서도 마스크를 사용할 수는 있겠지만, **미세 조정의 목표(특정 태스크에 적합한 모델 만들기)** 와 충돌하고, 데이터 효율성이나 실제 데이터와의 일치성을 해칠 가능성이 높아. 그래서 새로운 데이터셋에서는 마스크를 만들지 않고, 완전한 형태의 데이터를 기반으로 모델을 학습시키는 거야.

이 점이 BERT 같은 모델에서 **사전 학습**과 **미세 조정**을 구분하는 핵심적인 이유 중 하나야! 😊