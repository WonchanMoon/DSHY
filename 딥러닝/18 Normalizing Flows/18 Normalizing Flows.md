잠재 공간에서 데이터 공간으로의 신경망은 보통 비가역적(일대일대응x)이기 때문에 가능도(likelihood) 함수는 일반적으로 다루기 어렵거나, 정의되지 않을 수 있음
GAN에서는 이를 해결하기 위해 판별자 신경망을 도입해 적대적 학습을 진행함

해당 장에서는 **비선형 잠재 변수 모델**을 학습하기 위한 다른 방법을 다룸
이 방법은 가능도를 근사하지 않고 직접 계산할 수 있게 신경망 모델을 제한함

- 비선형 잠재 변수 모델: 잠재 변수 $z$를 샘플링하고, 복잡한 비선형 함수를 통해 데이터 $x$로 변환하는 과정을 통해 데이터 분포를 생성하는 모델
### Overview
잠재 변수 $z$에 대해 기본 분포(base distribution)라고도 불리는 분포 $p_z(z)$와, 심층 신경망에 의해 주어지는 비선형 함수 $x = f(z, w)$를 정의하여 잠재 공간을 데이터 공간으로 변환한다고 가정

만약 $p_z(z)$가 가우시안과 같이 단순한 분포라고 가정하면, 각 잠재 샘플 $z^\star \sim p_z(z)$를 신경망에 통과시켜 대응하는 데이터 샘플 $x^\star = f(z^\star, w)$를 생성하므로, 이 모델에서의 샘플링은 매우 간단하고 효율적임

이 모델의 가능도 함수를 계산하기 위해서는 데이터 공간의 분포가 필요한데, 이는 신경망 함수의 역함수에 의존함
이를 $z = g(x, w)$로 나타내며, $z = g(f(z, w), w)$를 만족함
이는 모든 $w$의 값에 대해 함수 $f(z, w)$와 $g(x, w)$가 서로 역함수, 즉 전단사 함수(bijective)여야 하며, 그 결과 각 $x$의 값이 고유한 $z$의 값에 대응하고 그 반대도 성립함을 의미함 (잠재 공간의 차원과 데이터 공간의 차원이 동일)

따라서 변수 변환 공식을 사용하면 데이터 밀도를 다음과 같이 표현할 수 있음
$$
p_x​(x∣w)=p_z​(g(x,w)) ∣​detJ(x)∣​
$$
$$
J_{ij}(x) = \frac{\partial g_i (x, w)}{\partial x_j}
$$
독립적인 데이터 포인트로 구성된 학습 집합 $\mathcal{D} = \{x_1, \dots, x_N\}$을 고려하면, (18.1)식에 따라 로그 가능도 함수는 다음과 같이 주어짐
$$
\ln p(\mathcal{D} \mid \mathbf{w}) = \sum_{n=1}^{N} \ln p_{\mathbf{x}}(\mathbf{x}_n \mid \mathbf{w}) 

= \sum_{n=1}^{N} \left\{ \ln p_{\mathbf{z}}(g(\mathbf{x}_n, \mathbf{w})) + \ln \lvert \det \mathbf{J}(\mathbf{x}_n) \rvert \right\}
$$
해당 가능도 함수를 사용해 신경망을 학습시킬 수 있음

다양한 분포를 모델링하기 위해 네트워크의 각 층을 가역적으로 만들어 전체 함수가 가역적이게 만듦 (자코비안 행렬식도 chain rule을 사용하면 됨)

이런식으로 유연한 분포를 모델링하는 접근법은, 확률 분포를 일련의 매핑을 통해 변환하는 것이 유체의 흐름과 유사하다는 점에서 *정규화 흐름(normalizing flow)* 이라고 불림

해당 장에선 실제로 사용되는 2가지 정규화 흐름인 *커플링 흐름(coupling flows)* 과 *자기회귀 흐름(autoregressive flows)* 의 핵심 개념을 논의하고, 가역 매핑을 정의하기 위해 신경 미분 방정식(neural differential equations)을 사용하는 방법인 *연속적 흐름(continuous flows)* 에 대해서도 살펴봄
## 18.1 Coupling Flows
우리의 목표는 단일 가역 함수 레이어를 설계하는 것으로, 이를 여러 개 결합하여 매우 유연한 가역 함수 클래스(invertible functions)를 정의할 수 있도록 하는 것

- 만약 선형 변환$x=az+b$이라면?
	$z=\frac{1}{a}(x-b)$로 쉽게 역산됨
	하지만 연속된 선형 변환은 결국 단일 선형변환과 동일함 (합성에 있어서 closed)
	또한 가우시안 분포에 대한 선형 변환은 가우시안 분포로, 결과 분포가 항상 가우시안 분포가 됨

위 문제를 해결하기 위한 한 방법으로 *real NVP(real-valued non-volume-preserving)* 이라고 불리는 정규화 흐름 모델이 존재함
![[18.1.1.png]]
아이디어는 잠재 변수 벡터 $z$를 두 부분으로 분할하는 것으로, 출력 벡터 $x$도 같은 차원으로 분할됨
- 출력 벡터의 첫 부분은 잠재 변수 벡터의 첫 부분을 그대로 복사 ($x_A=z_A$)
- 출력 벡터의 두 번째 부분은 아래와 같은 선형 변환을 거침
$$
x_B = \exp \big( s(z_A, w) \big) \odot z_B + b(z_A, w)
$$
	$s(z_A​,w)$와 $b(z_A, w)$는 신경망의 실수값 출력이며, 지수 함수($\exp$)는 곱셈 항이 음수가 아닌 값을 갖도록 보장함
	$\odot$은 두 벡터의 원소별 곱셈을 의미
	지수 함수 또한 원소별로 적용됨
	$s$와 $b$에 사용된 $w$는 동일한 벡터로 표현됐지만 다르게 구현될 수 있음

신경망 함수를 사용했기 때문에 매우 유연한 변환이면서 **전체 변환은 가역적**임
만약 $x = (x_A, x_B)$의 값이 주어지면
	$z_A​=x_A$
	$z_B = \exp \big( -s(z_A, w) \big) \odot \big( x_B - b(z_A, w) \big)$
	이때, $s$와 $b$ 신경망은 가역적일 필요가 없는게 인상깊음

자코비안 행렬은 다음과 같음
$$
J = \begin{bmatrix} 
I_d & 0 \\ 
\frac{\partial z_B}{\partial x_A} & \operatorname{diag}(\exp(-s)) 
\end{bmatrix}
$$
lower triangular matrix로 행렬식은 대각 원소들의 곱임
따라서 행렬식은 **$\exp \big( -s(z_A, w) \big)$의 원소들의 곱**임을 알 수 있음

$z_A$의 값이 변환에 의해 변경되지 않는 점을 해결하기 위해 아래와 같이 역할을 교환하는 레이어를 추가할 수 있음
![[18.1.2.png]]

real NVP 모델은 커플링 플로우(coupling flows)라고 불리는 정규화 흐름(normalizing flows)의 광범위한 클래스에 속함
해당 클래스는 선형 변환이 아래와 같은 보다 일반적인 형태로 대체됨
$$
x_B = h(z_B, g(z_A, w))
$$
여기서 $h$는 $g$에 대해 효율적으로 가역적인 함수로, 커플링 함수(coupling function)라고 불림 ($g$는 일반적인 신경망)
아래는 two moons로 알려진 간단한 데이터셋에서의 real NVP 정규화 흐름의 예시임 
![[18.1.3.png]]
## 18.2 Autoregressive Flows
변수 집합에 대한 결합 분포가 각 변수에 대한 조건부 분포들의 곱으로 항상 쓸 수 있다는 점에서 아이디어를 얻음
![[18.2.1.png]]
$x$벡터 내의 변수들에 대해 순서를 선택한다면 다음과 같이 쓸 수 있음
$$
p(x_1, \dots, x_D) = \prod_{i=1}^{D} p(x_i \mid x_{1:i-1})
$$
해당 분해(factorizaiton)는 마스크드 자기회귀 흐름(masked autoregressive flow, MAF)이라고 불리는 정규화 흐름의 한 클래스를 구성하는 데 사용될 수 있으며, 이는 다음과 같이 주어짐
$$
x_i = h(z_i, g_i(x_{1:i-1}, w_i))
$$
$h$는 $z_i$에 대해 쉽게 가역적인 커플링 함수며, $g_i$는 DNN으로 표현되는 컨디셔너(conditioner)임
masked인 이유는 네트워크 가중치의 일부를 0으로 강제해 자기회귀 제약을 구현했기 때문

이 경우, 가능도 함수는 아래와 같은 역계산으로 주어짐
$$
z_i = h^{-1}(x_i, g_i(x_{1:i-1}, w_i))
$$
$z$들을 계산하는건 병렬적으로 가능하기 때문에 효율적임

해당 모델은 순차적으로 샘플링하기 때문에 $x_1​,\cdots,x_{i−1}​$의 값들이 계산되어야만 $x_i$​를 계산할 수 있으므로 느림
따라서 역 자기회귀 흐름(inverse autoregressive flows, IAF)을 정의할 수 있음 (위 그림의 (b))
$$
x_i = h(z_i, \tilde{g}_i(z_{1:i-1}, w_i))
$$
하지만 역함수의 경우엔 $z$들을 순차적으로 계산해야하므로 느림

커플링 흐름과 자기회귀 흐름은 밀접하게 관련됨
자기회귀 흐름은 상당한 유연성을 가지지만 데이터 차원에 대해 선형적으로 증가하는 계산 비용이 존재
커플링 흐름은 차원을 두 그룹으로 나누어 일반성을 희생함으로써 효율성을 얻는 자기회귀 흐름의 특수한 경우로 볼 수 있음
## 18.3 Continuous Flows
미분 방정식으로 정의된 심층 신경망을 사용할 수 있음 (무한한 수의 레이어를 가지는 DNN)
먼저 신경 미분 방정식(neural ODE)의 개념을 소개한 후, 이것이 정규화 흐름 모델의 수식화에 어떻게 적용될 수 있는지 살펴봄
### 18.3.1 Neural differential equations
신경망은 보통 여러 레이어로 구성될 때 유용함 -> 레이어 수가 무한히 많아진 모델은 어떨까?

우선 잔차 네트워크(residual network)를 생각해봄
$$
z^{(t+1)} = z^{(t)} + f(z^{(t)}, w)
$$
$t$는 네트워크 내의 레이어를 나타냄
모든 레이어에서 동일한 함수 $f$와 공유 파라미터 벡터 $w$가 사용됨
레이어 수를 증가시키면서 각 레이어에서 도입되는 변화가 점점 작아진다고 생각해보면 아래와 같은 미분 방정식으로 벡터의 변화를 표현할 수 있음
$$
\frac{dz(t)}{dt} = f(z(t), w)
$$
극한으로 가면 $z$는 연속 변수 $t$의 함수인 $z(t)$로 볼 수 있음
$t$는 종종 시간으로 불림 
해당 수식은 신경 보통 미분 방정식(neural ordinary differential equation), 또는 neural ODE라고 불림 (보통은 단일 변수 $t$가 존재한다는 의미)

만약 네트워크의 입력을 $z(0)$으로 나타내면 출력 $z(T)$는 아래와 같은 미분 방정식의 적분으로 얻어짐
$$
z(T) = \int_{0}^{T} f(z(t), w) \, dt
$$
미분 방정식을 풀기 위해 오일러의 순방향 적분법(Euler’s forward integration method)을 사용하거나 더 강력한 수치적분 알고리즘들을 사용할 수 있음

아래 그림은 레이어 기반 신경망과 신경 ODE의 비교를 나타냄
![[18.2.2.png]]
### 18.3.2 Neural ODE backpropagation
입력 벡터 $z(0)$의 값과 관련된 출력 타겟 벡터, 그리고 출력 벡터 $z(T)$에 의존하는 손실 함수 $L(\cdot)$가 주어진 데이터 셋이 있다고 가정

- 한 가지 접근법은 순방향 전달(forward pass) 동안 ODE solver가 수행한 모든 연산을 자동 미분(automatic differentiation)으로 미분하는 것
	간단하지만 메모리 측면에서 비용이 크고 수치 오차를 제어하는 측면에서도 최적이 아님

- 따라서 ODE solver를 블랙박스로 취급하고, 명시적 역전파(explicit backpropagation)의 연속적 아날로그로 볼 수 있는 어드조인트 민감도(adjoint sensitivity) 방법을 사용

신경 ODE에 역전파를 적용하기 위해 어드조인트(adjoint)라는 개념을 다음과 같이 정의
$$
a(t) = \frac{dL}{dz(t)}
$$
이때 $a(T)$는 출력 벡터에 대한 손실의 일반적인 미분임

어드조인트는 다음과 같이 주어지는 자체 미분 방정식을 만족함
$$
\frac{d a(t)}{dt} = -a(t)^T \nabla_z f(z(t), w)
$$
이는 미적분의 연쇄 법칙의 연속적 버전이라고 할 수 있음
이 미분 방정식은 $a(T)$에서 시작하여 역방향으로 적분함으로써 풀 수 있으며, 역시 블랙박스 ODE 솔버를 사용하여 수행할 수 있음

우리의 신경 ODE는 동일한 파라미터 벡터 $w$가 네트워크 전반에 걸쳐 공유되므로, 손실의 미분은 $t$에 대한 적분이 되어 다음과 같이 나타남
$$
\nabla_w L = -\int_{0}^{T} a(t)^T \nabla_w f(z(t), w) \, dt
$$

위 어드조인트 방법을 사용해 학습된 신경 ODE는 전통적인 레이어 기반 네트워크와 달리 순방향 전달의 중간 결과를 저장할 필요가 없으므로 메모리 비용이 일정함
또한 관측치가 임의의 시간에 발생하는 연속 시간 데이터를 자연스럽게 처리함
### 18.3.3 Neural ODE flows
신경 ODE를 통해 정규화 흐름 모델을 구성하는 대안적 접근법을 정의할 수 있음
밀도의 변환이 다음과 같은 미분 방정식을 적분함으로써 계산됨 (논문이 존재)
$$
\frac{d \ln p(z(t))}{dt} = -\operatorname{Tr} \frac{\partial f}{\partial z(t)}
$$
이를 연속 정규화 흐름이라고 하며 아래의 사진과 같이 나타남
![[18.3.1.png]]
해당 수식은 행렬식 대신 trace를 포함하므로 더 효율적으로 계산됨

- 연속 정규화 흐름의 학습 효율성은 flow matching 기법으로 크개 개선될 수 있음
	이 기법은 정규화 흐름을 확산 모델(diffusion models)에 더 가깝게 만들며, 적분기(integrator)를 통한 역전파의 필요성을 제거하는 동시에 메모리 요구량을 크게 줄이고 추론 속도를 빠르게 하며 학습의 안정성을 높임
