---
날짜: 2025-04-05
완료: false
tags:
---

# Evidence Lover Bound

잠재변수 $z$ 가 있는 확률 모델 $p(x,z|w)$을 다루는데 우리는 관측변수의 likelihood인 $p(x|w)$를 최대화 하여야한다. 하지만 계산이 힘든 관계로 식을 변형하여 접근
$$\ln p(x \mid w) = \mathcal{L}(q, w) + \mathrm{KL}(q(z) \parallel p(z \mid x, w))$$
- $q(z)$: 우리가 임의로 선택한 분포 -> $p(x,z|w)$와 비슷한 분포를 고르면 ELBO가 타이트해짐
- $\mathcal{L}(q, w)$가 ELBO라고 불리며 식은 아래와 같음
$$\mathcal{L}(q, w) = \int q(z) \ln \left( \frac{p(x, z \mid w)}{q(z)} \right) \, dz$$

$\mathcal{L}(q, w)$를 최대화 하기 위해 우리는 **EM 알고리즘**을 활용한다.
- E 단계
	- $KL$ 발산을 최소화하는 $q(z) = p(z \mid x, w^{(\mathrm{old})})$ 에 대해 $\mathcal{L}(q, w)$ 최대화
- M 단계
	- $w$ 에 대해 $\mathcal{L}(q, w)$ 최대화
	- $w$ 업데이트


## Expectation maximization
이런 EM 알고리즘을 확률적 PCA에서 어떻게 활용되는지 볼건데, 결국 데이터 $X$의 생성 과정을 설명하는 잠재 변수 $Z$, 투영 행렬 $W$, 잡음 분산 $\sigma^2$을 추정하는 것

- E 단계
	- 잠재 변수 $z_n$의 사후 분포 편균과 분산 계산
	- 현재 추정된 주성분(축)을 기준으로 데이터를 저차원 공간에 사영
	- 즉, 각 데이터가 해당 축에서 어디쯤 있는지를 계산
$$\mathbb{E}[z_n] = \mathbf{M}^{-1} \mathbf{W}^T (x_n - \bar{x})$$$$\mathbb{E}[z_n z_n^T] = \sigma^2 \mathbf{M}^{-1} + \mathbb{E}[z_n] \mathbb{E}[z_n]^T$$
여기서 $\mathbf{M} = \mathbf{W}^T \mathbf{W} + \sigma^2 \mathbf{I}$

- M 단계
	- E-step에서 구한 기대값을 이용해 $W$와 $\sigma^2$을 업데이트
$$\mathbf{W}_{\text{new}} = \left[ \sum (x_n - \bar{x}) \mathbb{E}[z_n]^T \right] \left[ \sum \mathbb{E}[z_n z_n^T] \right]^{-1}$$
$$\sigma^2_{\text{new}} = \frac{1}{ND} \sum \left\{ \| x_n - \bar{x} \|^2 - 2 \mathbb{E}[z_n]^T \mathbf{W}_{\text{new}}^T (x_n - \bar{x}) + \mathrm{Tr} \left( \mathbb{E}[z_n z_n^T] \mathbf{W}_{\text{new}}^T \mathbf{W}_{\text{new}} \right) \right\}$$

> 이렇게 계산하면 뭐가 좋은가?
	-고차원 데이터에 적합: 공분산 행렬을 직접 계산하지 않아도 됨
	-효율적: 고유값 분해(PCA의 핵심 연산)보다 더 빠름
	-확장 가능성: 누락된 데이터(missing data)도 자연스럽게 처리 가능

![[Pasted image 20250405231603.png]]

- 같은 식에 방법으로 **factor analysis**에서도 똑같이 적용 가능!



# Nonlinear Latent Variable Models
지금까지는 **선형 변환 기반**의 잠재 변수(latent variable) 모델을 다뤘으니, 제는 **딥러닝의 복잡한 비선형 변환**을 사용해서 더 유연한 모델을 한번 살펴보자

### how?
**잠재 변수 z**는 간단한 분포 (예: 정규분포)에서 샘플링하여, 신경망 함수를 활용하여 x를 얻음
$$ p_z(z) = \mathcal{N}(0, I) \rightarrow x = g(z, w)$$
모델을 학습하려면 $x$의 확률 밀도 $p(x)$ 를 계산할 수 있어야 함.
$$p_x(x) = p_z(z(x)) \cdot \left| \det J(x) \right|$$
여기서 문제는....
$x$에 대해 $p(x)$ 를 계산하려면 z를 알아야 하는데, z를 얻기 위해서는 $x=g(z,w)$ 를 **역함수**로 풀어야 함
-> 이것은 일반적인 경우 불가능한 문제

## Nonlinear manifolds
잠재 공간(z)의 차원(M) < 데이터 공간(x)의 차원(D)인 경우, x는 M차원의 매니폴드(manifold)에 국한됨. 이는 데이터가 전체 공간을 채우지 않고, 저차원 구조에 위치함을 의미함. 자연 이미지처럼 복잡하지만 구조적인 데이터를 잘 모델링할 수 있음. 비선형 잠재 변수 모델은 데이터가 어떤 매니폴드에 놓여 있다고 가정함으로써 일반화 성능이 좋아짐.

### how?
조건부 확률분포 $p(x∣z,w)$를 신경망 출력으로 정의:
$$p(x \mid z, w) = \mathcal{N}(x \mid g(z, w), \sigma^2 I)$$
- $g(z,w)$: 신경망 함수
- $\sigma^2I$: 공분산 행렬

주변분포(marginal distribution) $p(x)$는 다음과 같이 적분 형태로 표현됨
$$p(x) = \int p(z) p(x \mid z) \, dz$$

## likelihood function
$$p(x \mid w) = \int p(x \mid z, w) p(z) \, dz$$
둘 다 Gaussian이지만, $g(z,w)$가 비선형이므로 해석적으로 **적분이 불가능**함
-> 샘플링 기반 근사로 해결 (monte corlo)

$$p(x \mid w) \simeq \frac{1}{K} \sum_{i=1}^{K} p(\mathbf{x} \mid \mathbf{z}_i, \mathbf{w})$$
- $z$를 여러 번 샘플링하여 평균을 취함으로써 likelihood를 근사.
- $K$가 클수록 정확하지만 계산량이 큼.

**하지만,** 단순 Gaussian noise 모델은 이미지 간 의미적 유사성을 반영하지 못함.  
특히 $\sigma^2$가 작으면 작은 차이에도 likelihood가 급감.

## Discrete data
당연히 범주형 변수도 모델링 가능
$$p(x \mid z, w) = \prod_{i=1}^{D} g_i(z, w)^{x_i} \left(1 - g_i(z, w)\right)^{1 - x_i}$$
### 실제 적용에서의 문제점과 해결법
이미지와 같이 연속 변수가 정수로 표현되는 경우(예: RGB 값 0~255), 모델이 특정 값들에만 확률을 몰아주게 되어 오버피팅할 수 있다.  
-> 이로 인해 likelihood가 0이 되는 문제도 발생.
이 문제는 **dequantization (탈양자화)** 라는 기법으로 해결할 수 있다. 이는 각 정수 값에 대해 작은 확률 노이즈를 추가해 연속 공간으로 확장하는 방식이다.  
-> 이 방법은 병적인 해(poor solution)로의 수렴을 방지하며, 더 유연한 모델 학습이 가능해진다.

