---
날짜: 2025-03-29
완료: false
tags:
---
## Probabilistic Latent Variables
### Probabilistic PCA
확률적 PCA는 PCA를 확률 모델로 재해석한 것으로, 잠재 변수를 기반으로 데이터를 생성하는 **가우시안 생성 모델**입니다. 이는 PCA를 최대 우도 추정(Maximum Likelihood Estimation) 문제로 바꾼 방식

| 항목           | 기존 PCA (Conventional PCA)     | 확률적 PCA (Probabilistic PCA)           |
| ------------ | ----------------------------- | ------------------------------------- |
| **모델 형태**    | 결정론적 선형 변환                    | 확률적 생성 모델 (가우시안 기반)                   |
| **잠재 변수**    | 명시적으로 정의되지 않음                 | 확률적 잠재 변수(latent variable)로 명시        |
| **학습 방법**    | 고유벡터 분해 (Eigen Decomposition) | EM 알고리즘 사용                            |
| **결측값 처리**   | 불가능 또는 어려움                    | EM 알고리즘으로 결측값 처리 가능                   |
| **혼합 모델 확장** | 어려움                           | Mixture 모델로 확장 가능                     |
| **성능 비교 기준** | 재구성 오류 기반                     | 로그우도(likelihood) 기반으로 다른 확률 모델과 비교 가능 |
| **데이터 생성**   | 생성 불가능                        | 모델을 통해 새로운 샘플 생성 가능                   |
| **차원 선택**    | 사용자가 명시적으로 지정                 | 베이지안 확장에서 자동 결정 가능                    |
| **분류 문제 적용** | 직접적 사용 어려움                    | 클래스 조건 밀도 모델링 가능 (분류 활용 가능)           |
| **계산 효율성**   | 고차원에서 계산량 증가                  | 일부 주성분만 필요 시 효율적 (공분산 행렬 생략 가능)       |


## 16.2.1 generative model
### 확률적 PCA 구성요소 

**잠재 변수 z**: 보이지 않는 원인
- 관측 데이터 x를 만들어낸다고 가정하는 **숨겨진 원인**
    z는 평균이 0이고 공분산이 단위 행렬인 **표준 정규분포**를 따른다고 가정
    $p(z)=N(0,I)p(z) = \mathcal{N}(0, I)$

**관측 데이터 x**: 실제 우리가 보는 데이터
- z라는 잠재 변수가 주어졌다 가정
- $x = Wz + \mu + \epsilon$
    - $W$: z를 x로 바꿔주는 **선형 변환 행렬**
    - $\mu$: x의 **평균값**
    - $\epsilon$: **노이즈(잡음)**. 정규분포를 따르며, 데이터에 자연스러운 오차를 표현함.

---


- $z$를 샘플링해서, $x$를 생성할 수 있는 구조 => **생성 모델 (generative model)**
- 즉, z가 먼저 결정되고, z를 이용해서 x가 만들어지는 과정을 확률적으로 정의.
    

---

>[💡 비유로 이해하기  ]
>
>📷 예를 들어 사진을 찍는다고 생각해보세요.
    - 현실의 사물은 **잠재 변수 z**라고 볼 수 있어요.
    - 이 사물이 카메라 렌즈를 통해 사진으로 찍히면, 그게 **관측 데이터 x**예요.
    - 렌즈의 성능이나 조명 등으로 인해 사진에 약간의 **노이즈**가 들어갈 수 있어요.
    - 이때 사진 x는 사물 z를 선형적으로 변형해서 만들어졌고, 여기에 약간의 오차(노이즈)가 더해진 거죠.


## 16.2.2 Likelihood function
#### 데이터 $x$에 대해 파라미터 $\mathbf{W}, \mu, \sigma^2$를 최대우도로 추정

관측값 x의 확률은 잠재변수 z를 적분해서 구함
$$p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z}) \, dz$$
이건 가우시안 + 선형 조합이기 때문에, 결과도 가우시안 분포
$$p(\mathbf{x}) = \mathcal{N}(\mathbf{x} \mid \mu, \mathbf{C})$$

공분산행렬은 잠재공간의 선형 변환 + 노이즈의 형태
$$\mathbf{C} = \mathbf{W} \mathbf{W}^\top + \sigma^2 \mathbf{I}$$
![[Pasted image 20250330075454.png]]

## 16.2.3 Maximum likelihood

### 로그 우도 함수
- 관측 데이터에 대한 로그 우도는 다음과 같이 주어짐:  $$\ln p(\mathbf{X} \mid \mu, \mathbf{W}, \sigma^2) 
= \sum_{n=1}^{N} \ln p(\mathbf{x}_n \mid \mathbf{W}, \mu, \sigma^2)
= -\frac{ND}{2} \ln(2\pi) - \frac{N}{2} \ln \lvert \mathbf{C} \rvert 
\,-\, \frac{1}{2} \sum_{n=1}^{N} (\mathbf{x}_n - \mu)^\top \mathbf{C}^{-1} (\mathbf{x}_n - \mu)$$
- 여기서 $C=\mathbf{W}\mathbf{W}^T + \sigma^2 \mathbf{I}$는 공분산 행렬.

---

### 최대우도 추정
- 데이터 평균 $\mu$: 단순한 $\bar{x}$ (전체 평균)으로 추정
- 잠재공간 변환 $W_{ML}$ : 고유값 분해 기반으로 닫힌형 해 존재
$$\mathbf{W}_{ML} = \mathbf{U}_M (\mathbf{L}_M - \sigma^2 \mathbf{I})^{1/2} \mathbf{R}$$
- $U_{M}$: 공분산 행렬의 상위 $M$개의 고유 백터
- $L_M$ 상위 고유값들의 대각 행렬
- $R$: 임의의 직교 행렬
- $\sigma^2_{ML}$:무시된 고유값들의 평균으로 계산
$$\sigma^2_{ML} = \frac{1}{D - M} \sum_{i = M+1}^{D} \lambda_i$$

---

#### 확률적 PCA와 고전적 PCA의 관계
- 확률적 PCA는 **PCA를 확률 모델로 일반화**한 것.
- $\sigma^2 \rightarrow 0$일 때, 확률적 PCA는 고전적인 PCA와 동일해짐.
- PCA의 투영은 확률적 PCA의 **잠재 변수 평균**으로 해석됨:
$$\mathbb{E}[\mathbf{z}|\mathbf{x}] = \mathbf{M}^{-1} \mathbf{W}^T (\mathbf{x} - \bar{\mathbf{x}})$$
- 투영된 값의 복원:
$$\mathbf{W} \mathbb{E}[\mathbf{z}|\mathbf{x}] + \boldsymbol{\mu}$$
---

### 결론
- 확률적 PCA는 **잡음이 있는 데이터의 분산 구조**를 좀 더 잘 모델링함.
- 주성분 방향에서는 고유값을 재현하고, 직교 방향에서는 균일한 잡음 분산 $\sigma^2$를 사용함.
- 공분산 구조의 제약 덕분에, 필요한 파라미터 수(자유도)가 일반 가우시안보다 적음:
    $자유도 수 = DM + 1 - \frac{M(M - 1)}{2}$


## 16.2.4 Factor analysis

- <mark style="background: #ABF7F7A6;">잠재변수(z)</mark>를 이용해 관측된 데이터(x)를 설명하는 통계 모델.
- 확률적 PCA와 비슷하지만, **공분산 구조가 다름**:
    - 확률적 PCA는 등방성($\sigma^2$) 공분산.
    - 요인분석은 **대각 행렬(Ψ)** 공분산 사용 → 각 변수의 **고유한 분산**을 따로 모델링.

#### 조건부 분포
$$p(\mathbf{x} \mid \mathbf{z}) = \mathcal{N}(\mathbf{W} \mathbf{z} + \mu, \, \mathbf{\Psi})$$
- $W$ : 요인 부하량(factor loadings)
- $\psi$ : 변수별 독립적인 노이즈(대각 행렬)


#### 주변 분포 (z를 적분한 후 x만 남긴 분포)
$$p(\mathbf{x}) = \mathcal{N}(\mu, \, \mathbf{W} \mathbf{W}^\top + \mathbf{\Psi})$$
- 관측 데이터의 공분산을 $WW^T$ 와 $\psi$로 나누어 설명

### 🎓 예시: 학생의 시험 성적 분석

#### 🧾 상황

- 어떤 학생이 다음 3과목 시험을 봤다고 하자:
    
    - **수학**
        
    - **물리**
        
    - **국어**
        

#### ❓ 목적

"학생들의 성적이 어떤 **잠재 능력**(보이지 않는 특성)으로부터 영향을 받았을까?"를 알고 싶어.

---

#### ✅ PCA 방식으로 접근하면?

- PCA는 성적 간의 **전체적인 상관 구조**를 보고 가장 분산이 큰 방향을 찾는다.
    
- 이걸로 우리는 예를 들어 다음과 같이 해석할 수 있어:
    
    - PC1 (주성분 1): 수학과 물리를 잘 본 학생일수록 점수가 높음 → "이과 능력"
        
    - PC2 (주성분 2): 국어 점수가 높고 나머지는 낮음 → "문과 성향"
        

> 📌 하지만 PCA는 **모든 과목에 같은 정도의 노이즈(오차)**가 있다고 가정함.

---

#### ✅ 요인분석 방식으로 접근하면?

- 요인분석은 "학생 성적 = 잠재 능력 × 영향도 + 과목별 잡음"으로 본다.
    
- 예를 들어 잠재 능력이 2개 있다고 하자:
    
    - **잠재 요인 1 (이과 능력)**: 수학과 물리에 영향
        
    - **잠재 요인 2 (언어 능력)**: 국어에 영향
        

그럼 수학 점수는 이렇게 모델링됨:

```
수학 = (이과 능력 × 0.8) + (언어 능력 × 0.1) + 과목별 노이즈
```

- 여기서 **과목별 노이즈**는 Ψ에 의해 조절됨.  
    예를 들어 국어는 평가 방식이 들쭉날쭉해서 노이즈가 클 수 있음 → Ψ값이 큼
    

> 📌 요인분석은 **과목마다 서로 다른 정도의 오차**를 고려할 수 있음.

---

#### 🔍 차이 정리

|특징|PCA|요인분석|
|---|---|---|
|노이즈 모델링|모두 동일한 노이즈|과목(변수)마다 다름|
|잠재 요인 해석|수학적 분산 기준|해석 가능성 있음 (요인 부하량 해석)|
|목적|차원 축소|잠재 요인 추정 및 해석|

---





## 16.2.5 Independent component analysis(ICA)
### ICA
- 관측된 데이터에서 서로 독립적인 신호(성분)를 찾아내는 것.
- <mark style="background: #ABF7F7A6;">전제</mark>: 관측된 데이터는 여러 신호(잠재 변수)의 선형 혼합이다.
-  신호(잠재 변수)들은 서로 독립적이고 <mark style="background: #ABF7F7A6;">비가우시안</mark> 분포를 따른다

- PCA(주성분 분석)는 잠재 변수를 가우시안 분포로 가정함 → 회전에 대해 불변 → 다른 신호 구분 X
- 반면, **ICA는 비가우시안 분포**를 사용 → 서로 다른 신호 구분 가능!!!

### 어디서 쓰는고
- 두 사람이 동시에 말할 때, 두 마이크로 녹음된 소리는 **두 사람 목소리의 선형 혼합**이다.
- ICA를 사용하면 각 사람의 원래 목소리(신호)를 분리해낼 수 있다.
- 이와 같은 문제를 blind source separation(눈가린 음원 분리)라고 한다.
- 단순한 신호 복원이 아니라, **잠재 변수 모델링**과 **정보 극대화** 같은 방식으로 해석 가능.
- 잡음이 섞인 경우에도 적용 가능하며, 잠재 변수가 가우시안 혼합일 때도 사용할 수 있음.
- 다양한 실제 데이터에 잘 맞고, 뇌파 분석, 음성 처리, 이미지 분해 등에 사용됨.


## Kalman filter
- 지금까지 배운 건, 데이터들이 **서로 독립적**이라고 가정하는 모델, 하지만 현실은 그렇지 않음. 
- 순서가 중요한 경우가 많다.
- Kalman Filter는 **이전 정보(과거)** 를 보고 **다음 상태(미래)** 를 예측
- 그리고 실제로 관측한 값과 비교해서 보정.
![[Pasted image 20250330084001.png|500]]