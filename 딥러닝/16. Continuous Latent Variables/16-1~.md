---
날짜: 2025-03-15
완료: false
tags:
---
## Start with...
데이터의 특성을 먼저 알고 들어가야하는데, 알아야할 특성들을 살펴보자
- **데이터의 차원 축소 특성**
    - 많은 데이터는 원래 데이터 공간보다 훨씬 낮은 차원의 구조(매니폴드) 근처에 위치함.
    - 예를 들어보자.
        - 손글씨 숫자 데이터(MNIST)를 100×100 픽셀 이미지(10,000차원 공간)로 변환하지만, 실제 변동은 숫자의 위치(수평, 수직 이동)와 회전(3차원)으로 제한됨. 즉, 데이터가 10,000차원 공간에 퍼져 있는 것처럼 보이지만, 실제로는 3차원 매니폴드 내에 존재함.
        - 다 쉽게 이해해보면 종이로 비행기를 접으면 3차원 구조지만 2차원 패턴은 유지가 됨
    - 결론적으로 데이터도 고차원이지만, 저차원의 구조를 따라긴다.
- **매니폴드는 비선형적**
    - 이미지의 회전과 이동이 단순 선형 변환이 아닌, 특정 픽셀 값이 비선형적으로 변함.
- **실제 데이터의 추가적인 변동 요소**
    - 필체 스타일, 크기 차이 등으로 인해 매니폴드의 자유도가 증가할 수 있음.
    - 하지만 전체 데이터 차원에 비하면 이러한 자유도의 수는 여전히 적음.
    - 결론적으로 이미지 변형으로, 매니폴드의 자유도는 변화 가능하지만, 그래 봤자 실질 차원에 비하면 작다.
- **데이터의 노이즈 해석**
    - 데이터 포인트가 항상 매끄러운 저차원 매니폴드 위에 정확히 위치하지 않음.
    - 매니폴드에서 벗어나는 부분을 ‘노이즈’로 해석할 수 있음.
    - 빵을 완벽한 모양으로 만들어도, 구우면 살짝 울퉁불퉁해짐
        - 그정도는 노이즈로 볼수 있어~의 느낌
    - 이를 통해 **생성 모델(generative model)** 접근법을 도입할 수 있음.
        - 당장은 배우지 않음


### 연속 잠재 변수 모델과 PCA
- 데이터의 저차원 구조를 설명하는 가장 기본적인 방법은 **PCA(주성분 분석)**.
- PCA는 선형 가우시안 잠재 변수 모델의 Maximum Likelihood의 해로 해석 가능.
	- 신기하네요
- 확률적 모델(PPCA, 요인 분석)로 확장하면 EM 알고리즘을 활용하여 보다 강력한 모델링 가능.
- 단순 PCA를 넘어서 **정규화 흐름(normalizing flow), 변분 오토인코더(VAE), 가우시안 프로세스** 등을 활용하여 **비선형 구조**도 모델링 가능
	- 예를 들면, RGB의 그림을 흑백으로 변환하면 차원을 줄일 수 있음(PCA의 역활)
	- 근데 이제, 만화풍, 수체화 풍으로 이미지를 바꾸면 비선형적인 변화가 필요함. (VAE 같은 모델을 활용)

## Principal Component Analysis
### PCA
- PCA는 **차원 축소**, **손실이 있는 데이터 압축**, **특징 추출**, **데이터 시각화** 등의 목적으로 사용됨.
- **Kosambi–Karhunen–Loève 변환**이라고도 불림.
- 데이터 집합을 저차원 선형 공간(principal subspace)으로 직교 투영하는 기법.
- 두 가지 정의로 설명 가능:
    1. **분산 최대화 관점**: 투영된 데이터의 **분산을 최대화**하는 선형 투영.
    2. **오차 최소화 관점**: 원본 데이터와 투영된 데이터 간의 평균 제곱 거리(mean squared distance를 **최소화**하는 선형 투영.

### 최대 분산 공식화 (Maximum Variance Formulation)
- 해당 기법의 목표를 집고 넘어가자.
	- $D$-차원 데이터 집합 $\{x_n\}$을 $M$-차원 ($M < D$) 공간으로 투영.
	- 투영된 데이터의 **분산을 최대화**하는 방향을 찾는 것이 핵심.
- 1차원 공간으로의 투영($M=1$)
	- 투영 방향을 $D$-차원 단위 벡터 $\mathbf{u}_1$로 설정 ($\mathbf{u}_1^T \mathbf{u}_1 = 1$).
	- 각 데이터 $x_n$은 스칼라 값 $\mathbf{u}_1^T x_n$으로 투영됨.
	- 투영된 데이터의 평균은:
$$\bar{x} = \frac{1}{N} \sum_{n=1}^{N} x_n$$
- 투영된 데이터의 분산은:
$$\frac{1}{N} \sum_{n=1}^{N} \left( \mathbf{u}_1^T x_n - \mathbf{u}_1^T \bar{x} \right)^2 = \mathbf{u}_1^T S \mathbf{u}_1$$
- 여기서, 공분산 행렬 $S$는 다음과 같이 정의됨:
$$S = \frac{1}{N} \sum_{n=1}^{N} (x_n - \bar{x}) (x_n - \bar{x})^T$$

#### 최적화
- 목표: 분산($\mathbf{u}_1^T S \mathbf{u}_1$)을 최대화.
- 제약 조건: $\mathbf{u}_1$이 단위 벡터 ($\mathbf{u}_1^T \mathbf{u}_1 = 1$).
	- 제약이 없으면 무한으로 분산을 키울수 있기 때문
- 라그랑주 승수 $\lambda_1$을 도입하여 최적화 문제 변환:
$$
\mathbf{u}_1^T S \mathbf{u}_1 + \lambda_1 (1 - \mathbf{u}_1^T \mathbf{u}_1)
$$
- 최적 조건을 구하기 위해 미분 후 0으로 설정:
$$
S \mathbf{u}_1 = \lambda_1 \mathbf{u}_1
$$
- 즉, $\mathbf{u}_1$은 공분산 행렬 $S$의 고유벡터.
	- 선형대수때 보던거랑 비슷하쥬?
- 식의 양변에 $\mathbf{u}_1^T$를 곱하면:

$$
\mathbf{u}_1^T S \mathbf{u}_1 = \lambda_1
$$

- 따라서, 최대 분산을 갖는 방향 $\mathbf{u}_1$은 $S$의 가장 큰 고유값 $\lambda_1$에 대응하는 고유벡터.
- 이 고유벡터를 **첫 번째 주성분**(first principal component)이라고 함.

#### 추가 주성분
- 첫 번째 주성분을 찾은 후, 그 방향과 **직교하는 방향** 중에서 두 번째로 분산이 큰 방향을 찾음.  
- 이 과정을 반복하면 두 번째, 세 번째, … 주성분을 찾을 수 있음.
	- **PC1** = 가장 큰 고유값에 대응하는 고유벡터  
	- **PC2** = 두 번째로 큰 고유값에 대응하는 고유벡터  
	- **PC3** = 세 번째로 큰 고유값에 대응하는 고유벡터  
- 이렇게 $M$개의 주성분을 선택하면, 원래 $D$-차원의 데이터를 $M$-차원으로 축소 가능!


### Minimum-error formulation
- 그다음으로 PCA의 오차 최소화 관점에서 다시 정리해보자
-  이를 위해, $D$차원 직교 **기저(orthonormal basis)** 벡터 $\{\mathbf{u}_i\}$를 정의하자.  
- 이 벡터들은 다음 조건을 만족한다.

$$
\mathbf{u}_i^T \mathbf{u}_j = \delta_{ij}
$$
- 즉, 서로 직교하고 단위 길이를 갖는다.  
- 이제 데이터 점 $x_n$은 이 기저를 이용해 다음과 같이 표현할 수 있다.
$$
x_n = \sum_{i=1}^{D} \alpha_{ni} \mathbf{u}_i
$$

- 여기서 계수 $\alpha_{ni}$는 각 데이터에 따라 달라짐.  
- 이 식은 기저 벡터를 기준으로 한 좌표 변환을 의미.  
- 직교성을 활용하여 $\alpha_{ni} = \mathbf{u}_i^T x_n$으로 두면, 아래와 같이 표현 가능

$$
x_n = \sum_{i=1}^{D} (\mathbf{u}_i^T x_n) \mathbf{u}_i
$$


### 차원 축소를 통한 근사
- 우리는 데이터 $x_n$을 낮은 차원의 부분 공간에 투영하여 표현하고 싶다.  
- 즉, $M$차원 ($M < D$)의 부분 공간을 선택해 근사 표현을 한다.
$$
\tilde{x}_n = \sum_{i=1}^{M} z_{ni} \mathbf{u}_i + \sum_{i=M+1}^{D} b_i \mathbf{u}_i
$$
	- $\{z_{ni}\}$ : 각 데이터 점에 따라 달라지는 계수  
	- $\{b_i\}$ : 모든 데이터에서 동일한 상수  

- 우리는 차원 축소로 인해 발생하는 오류를 최소화해야 함.  
- 오차를 측정하는 방법으로 **제곱 오차**를 사용하여 다음을 최소화한다.
$$
J = \frac{1}{N} \sum_{n=1}^{N} \| x_n - \tilde{x}_n \|^2
$$
- 이제 정의된 오차 $J$를 최적화해보자
	- $z_{ni}$ 최적화  
		- $\tilde{x}_n$을 위 식에 대입한 후, 미분하여 $z_{ni}$에 대해 0으로 설정하면
		- $$   z_{ni} = x_n^T \mathbf{u}_j$$
		-  즉, $z_{ni}$는 데이터 벡터와 기저 벡터의 내적으로 결정된다.
	- $b_i$ 최적화  
		- 같은 방식으로 $b_i$를 접근
$$   b_j = \bar{x}^T \mathbf{u}_j $$
	- 오차 벡터  
		- 위 두 값을 대입하면,   $$
   x_n - \tilde{x}_n = \sum_{i=M+1}^{D} \left\{ (x_n - \bar{x})^T \mathbf{u}_i \right\} \mathbf{u}_i
   $$
		   - 즉, 오차 벡터는 주성분 공간(**Principal Subspace**)에 직교하는 방향에 놓이게 된다.  
		   - 따라서, 최소 오차는 **직교 투영**(orthogonal projection)을 통해 발생한다.
	- 오차 측도 $J$ 변형  
		- 이를 통해, 최종적으로 오차 $J$를 기저 벡터 $\{\mathbf{u}_i\}$를 이용한 표현으로 정리하면,
		- 여기서 $S$는 데이터의 공분산 행렬이다.
$$
J = \sum_{i=M+1}^{D} \mathbf{u}_i^T S \mathbf{u}_i
$$
	- 최적의 기저 벡터 선택
		- 우리는 이제 $J$를 최소화하는 최적의 기저 벡터를 찾아야 한다.  
		- 이를 위해, $\mathbf{u}_i$가 공분산 행렬 $S$의 고유벡터가 되도록 설정하면,
$$
S \mathbf{u}_i = \lambda_i \mathbf{u}_i
$$
		- 즉, $\mathbf{u}_i$는 $S$의 고유벡터이고, 대응하는 고유값 $\lambda_i$를 가진다.  
		- 이제 오차 측도 $J$는,
$$
J = \sum_{i=M+1}^{D} \lambda_i
$$
		- 즉, 고유값이 작은 방향에서 차원을 축소해야 오차가 최소화된다.  
		- 따라서, $M$개의 가장 큰 고유값에 해당하는 고유벡터들을 선택하면 최적의 주성분 공간을 찾을 수 있음

- 정리해보면,
	- PCA는 데이터의 공분산 행렬의 **고유값 분해**를 이용하여,  
	  가장 큰 고유값을 가지는 $M$개의 고유벡터를 선택하는 과정이다.
	- 이렇게 하면, 데이터의 변동을 가장 잘 보존하면서 차원을 축소할 수 있다.
	- 오차는 나머지 $D - M$개의 고유값들의 합으로 나타난다.	
👉 즉, PCA는 "데이터의 변동이 큰 방향을 우선적으로 유지하는 차원 축소 기법"이라고 볼 수 있다.


### PCA를 어따 써먹는데?

- PCA(Principal Component Analysis)는 데이터 차원을 줄이면서 정보 손실을 최소화하는 기법이다.
- 이걸 어따 써 먹을수 있는고 하니 *데이터 압축*에 쓸수 있더라
- **데이터 압축**
	- 공분산 행렬의 고유벡터 $\mathbf{u}_i$는 원래 데이터와 같은 차원의 벡터이며, 이를 **이미지 형태로 표현 가능**하다.  
	- 첫 4개의 주성분과 대응하는 **고유값**($\lambda$):
$$
\lambda_1 = 3.4 \times 10^5, \quad
\lambda_2 = 2.8 \times 10^5, \quad
\lambda_3 = 2.4 \times 10^5, \quad
\lambda_4 = 1.6 \times 10^5
	$$
	- 고유값이 클수록 데이터의 분산을 더 많이 설명하며, 적은 수의 주성분만으로도 원본 데이터를 효과적으로 표현 할수 있음
- **데이터 화이트닝**
	- 주성분 분석(PCA)의 응용으로, 차원 축소가 아닌 **데이터 전처리**에 초점을 둠
	- 데이터의 평균을 0, 공분산을 단위 행렬$I$로 변환하여 변수 간 상관성을 제거
	- 표준화(Standardization) 과정  
		- 데이터 $x_n$을 평균 0, 단위 분산을 가지도록 변환:
$$
\rho_{ij} = \frac{1}{N} \sum_{n=1}^{N} \left( \frac{x_{ni} - \bar{x}_i}{\sigma_i} \right) \cdot \left( \frac{x_{nj} - \bar{x}_j}{\sigma_j} \right)
$$
		- $\sigma_i$ : 변수 $x_i$의 표준편차  
		- $\rho_{ij}$ : **표준화된 데이터의 상관행렬**(correlation matrix)  
		- $\rho_{ij} = 1$ : 완전 상관, $\rho_{ij} = 0$ : 무상관
	- PCA를 이용한 화이트닝
		- 공분산 행렬의 고유값 분해
	$$
   \Sigma U = U L
   $$
			- $L$ : 대각 행렬(diagonal matrix), 대각 원소는 고유값($\lambda_i$)  
			- $U$ : 직교 행렬(orthogonal matrix), 열 벡터는 고유벡터(eigenvectors)  
		- 화이트닝 변환(Whitening Transform)   $$
   y_n = L^{-1/2} U^T (x_n - \bar{x})
   $$
			- 변환 후 데이터 $y_n$의 평균은 0, 공분산은 단위 행렬이 됨.  
		- 화이트닝된 데이터의 공분산 확인    $$
   \frac{1}{N} \sum_{n=1}^{N} y_n y_n^T = I
$$ 
			- 화이트닝 후 변수들이 완전히 **비상관(uncorrelated)** 상태가 됨.

### High-dimensional data
- 고차원 데이터에서 PCA 적용 문제해보자.
- 데이터 차원이 샘플 개수보다 큰 경우 $D > N$ 이면, PCA 수행 시 최소 $D - N + 1$개의 고유값이 0이 된다. 
- 즉, 데이터는 최대 $N - 1$ 차원의 부분 공간을 형성하며, $M > N - 1$인 데이터 개수보다 많은 주성분을 선택하는 것은 의미가 없다.

- 해당 특징을 데이터의 가지는 기존 PCA는 계산의 비효율성이 있음
	- 공분산 행렬 $S = \frac{1}{N} X^T X$ (크기 $D \times D$)의 고유값 분해를 수행해야 함.  
	- 연산 복잡도는 $O(D^3)$로, $D$가 매우 크면 비효율적.
- 차원 축소를 활용한 효율적 계산  
	- $X$를 $N \times D$ 크기의 중심화된 데이터 행렬로 정의.  
	- PCA의 기본 고유값 방정식:  
  $$
  \frac{1}{N} X^T X \mathbf{u}_i = \lambda_i \mathbf{u}_i
  $$
	- 양변에 $X$를 곱하면,  
  $$
  \frac{1}{N} X X^T (X \mathbf{u}_i) = \lambda_i (X \mathbf{u}_i)
  $$
	- 여기서 $\mathbf{v}_i = X \mathbf{u}_i$로 정의하면,  
  $$
  \frac{1}{N} X X^T \mathbf{v}_i = \lambda_i \mathbf{v}_i
  $$
	- 즉, $N \times N$ 크기의 행렬 $X X^T$에서 고유값 문제를 풀어야 한다. 
	- 연산 복잡도가 $O(N^3)$로 줄어들어 훨씬 효율적.
- 원래 데이터 공간의 고유벡터 복원  
	- 양변에 $X^T$를 곱하면,  
  $$
  \left( \frac{1}{N} X^T X \right) (X^T \mathbf{v}_i) = \lambda_i (X^T \mathbf{v}_i)
  $$

	- 즉, $X^T \mathbf{v}_i$는 원래 공분산 행렬 $S$의 고유벡터이며, 고유값은 동일.  
	- 하지만, $X^T \mathbf{v}_i$는 정규화되지 않았으므로, 정규화 과정 필요:  
  $$
  \mathbf{u}_i = \frac{1}{(N \lambda_i)^{1/2}} X^T \mathbf{v}_i
  $$
	- 이를 통해 원래 $D$-차원 공간에서의 주성분을 복원 가능.

| 방식                   | 계산 복잡도        |
| -------------------- | ------------- |
| 기존 방식 (직접 고유값 분해)    | $O(D^3)$      |
| 효율적인 방식 (변형된 고유값 문제) | $O(N^3 + DN)$ |
