# 5.3 Generative Classifiers
Likelihood와 prior를 통해 posterior를 계산하는 **생산적** 접근 방식 (With Bayes' Thm)
- 클래스가 2개뿐일 때, 1개 클래스에 대한 posterior
>$$\begin{aligned}p(\mathcal{C}_1|\mathbf{x})&=\quad\frac{p(\mathbf{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}|\mathcal{C}_1)p(\mathcal{C}_1)+p(\mathbf{x}|\mathcal{C}_2)p(\mathcal{C}_2)}\\&=\quad\frac1{1+\exp(-a)}=\sigma(a)\end{aligned}$$
>$where \ a=\ln\frac{p(\mathbf{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}|\mathcal{C}_2)p(\mathcal{C}_2)}$

$\sigma(a)$를 **logistic sigmoid function**이라고 함 (sigmoid란 S자 모양이라는 뜻)
![sigmoid](images/5.3%20sigmoid.png)
모든 실수를 유한한 범위에 매핑하기 때문에 squashing function이라고도 불림
- logistic sigmoid function의 성질
1. 대칭성
>$$\sigma(-a)=1-\sigma(a)$$
1. 역함수는 logit function
>$$a = ln(\frac{\sigma}{1-\sigma})$$
>두 클래스에 대한 확률의 비율에 log를 취한 형태 (log odds)

logistic sigmoid의 출현이 다소 인위적으로 보일 수 있지만 $a$가 함수의 형태를 가진다면 의미가 있음. 만약 $x$에 대해 linear function이면 posterior가 **일반화 선형 모형**에 의해 좌우됨.
- 두개 이상의 클래스일 때, 1개 클래스에 대한 posterior
>$$\begin{aligned}p(\mathcal{C}_k|\mathbf{x})&=\quad\frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_jp(\mathbf{x}|\mathcal{C}_j)p(\mathcal{C}_j)}\\&=\quad\frac{\exp(a_k)}{\sum_j\exp(a_j)}\end{aligned}$$
>$where \ a_k=\ln\left(p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)\right)$
>normalized exponential로 알려져 있으며 logistic sigmoid의 다중 클래스 버전
>softmax function이라고도 불리며 max function의 스무딩 버전


이제 클래스 조건 밀도에 대한 특정 형태를 선택하고, 먼저 연속 입력 변수 $\mathbf{x}$를 살펴본 다음 간략하게 이산 입력에 대해 살펴볼 예정
### 5.3.1 Continuous inputs
클래스 조건 밀도가 가우시안으로 가정 (모든 클래스에 대해서 같은 공분산 행렬인 것도 가정)
> $$p(\mathbf{x}|\mathcal{C}_k)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\boldsymbol{\Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^{\mathrm{T}}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)\right\}$$
- 클래스가 2개뿐일 때, 1개 클래스에 대한 posterior
앞선 logistic sigmoid 적용
>$$p(\mathcal{C}_1|\mathbf{x})=\sigma(\mathbf{w}^\mathrm{T}\mathbf{x}+w_0)$$
>$$\begin{aligned}&\mathbf{w}&&=\quad\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)\\&w_0&&=\quad-\frac12\boldsymbol{\mu}_1^\mathrm{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac12\boldsymbol{\mu}_2^\mathrm{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\ln\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\end{aligned}$$

같은 공분산이라는 가정이 있었기에 $\mathbf{x}$의 제곱꼴이 빠짐 (linear function -> 일반화 선형 모형)
다음의 결정 경계를 가지는 형태
![decision boundary](images/5.3.1%20decision%20boundary.png)
- 두개 이상의 클래스일 때, 1개 클래스에 대한 posterior
>$$a_k(\mathbf{x})=\mathbf{w}_k^\mathrm{T}\mathbf{x}+w_{k0}$$
>$$\begin{aligned}\mathbf{w}_k&=\quad\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k\\w_{k0}&=\quad-\frac{1}{2}\boldsymbol{\mu}_k^\mathrm{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k+\ln p(\mathcal{C}_k)\end{aligned}$$

이번에도 $a_k(\mathbf{x})$가 linear function으로 일반화 회귀 모형
만약 두 클래스의 posterior가 같다면 오분류율이 작은 클래스를 따름
만약 공분산 행렬이 클래스마다 다르다면 quadratic function의 형태로 다음과 같이 곡선의 결정 경계를 가짐
![quadratic decision boundary](images/5.3.1%20quadratic%20decision%20boundary.png)
### 5.3.2 Maximum likelihood solution
클래스 조건 밀도 $p(\mathbf{x}|\mathcal{C}_k)$를 매개변수 함수 형태로 특정했기 때문에, MLE를 통해 파라미터를 정할 수 있음
- 클래스 2개, 같은 공분산을 가진 가우시안
  prior : $p(\mathcal{C}_1)=\pi,\ p(\mathcal{C}_2)=1-\pi$

클래스 1의 $n$번째 데이터 $\mathbf{x}_n$ ($t_n=1$)
>$$p(\mathbf{x}_n,\mathcal{C}_1)=p(\mathcal{C}_1)p(\mathbf{x}_n|\mathcal{C}_1)=\pi\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})$$

클래스 2의 $n$번째 데이터 $\mathbf{x}_n$ ($t_n=0$)
>$$p(\mathbf{x}_n,\mathcal{C}_2)=p(\mathcal{C}_2)p(\mathbf{x}_n|\mathcal{C}_2)=(1-\pi)\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})$$

따라서 likelihood function은 다음과 같음
>$$p(\mathbf{t},\mathbf{X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=\prod_{n=1}^N\left[\pi\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\right]^{t_n}\left[(1-\pi)\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\right]^{1-t_n}$$
>$where \ \mathbf{t} = (t_1, ... ,t_N)^{T}$

MLE $\pi$
>$$\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2}$$
>결국 MLE prior는 단순히 data의 비율임을 알 수 있음

MLE $\mu_1, \mu_2$
>$$\boldsymbol{\mu}_1=\frac1{N_1}\sum_{n=1}^Nt_n\mathbf{x}_n$$
>$$\boldsymbol{\mu}_2=\frac1{N_2}\sum_{n=1}^N(1-t_n)\mathbf{x}_n$$
>모든 입력의 평균과 같음

MLE $\boldsymbol{\Sigma}$
>$\begin{aligned}&\mathbf{S}&&=\quad\frac{N_1}N\mathbf{S}_1+\frac{N_2}N\mathbf{S}_2\\&\mathbf{S}_1&&=\quad\frac1{N_1}\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\mathrm{T}\\&\mathbf{S}_2&&=\quad\frac1{N_2}\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\mathrm{T}\end{aligned}$
>각 공분산의 가중 평균을 취한 $\mathbf{S}$와 같음

다중 클래스에 대해서도 쉽게 확장 가능
가우시안의 MLE가 이상치에 강건하지 않기 때문에, 클래스를 가우시안 분포에 피팅하는 건 이상치에 민감함
### 5.3.3 Discrete features
입력 변수가 $D$개이고 각 변수가 binary(0, 1)인 가정
입력 조합은 $2^D$개이지만 확률의 합은 1이어야 하므로 실제 확률 값들은 $2^D-1$개
변수의 개수에 따라 지수적으로 증가하므로 제한된 표현을 찾아야 함
>naive Bayes 가정 : 클래스가 주어질 때, 특성들이 독립적이다.

따라서 다음과 같이 class가 주어질 때의 분포가 구해짐
>$$p(\mathbf{x}|\mathcal{C}_k)=\prod_{i=1}^D\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}$$
>$$a_k(\mathbf{x})=\sum_{i=1}^D\left\{x_i\ln\mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\right\}+\ln p(\mathcal{C}_k)$$
>두번째 공식은 softmax 함수 만들면서 생기는 함수
>여기서도 $\mathbf{x}$에 대한 함수가 linear함을 알 수 있음
>만약 $K=2$라면 logistic sigmoid를 고려하면 됨
>값이 두가지(0, 1)이상이더라도 비슷한 결과가 나옴
### 5.3.4 Exponential family
앞서 봤듯이 가우시안 분포나 이산 입력에 대해서 posterior가 일반화 선형 모형으로 주어짐
이때 활성화 함수로 logistic sigmoid($K=2$), softmax($K\ge2$)를 가짐

다음은 exponential family의 분포에 따른 일반화된 클래스 조건 밀도 함수임
>$$p(\mathbf{x}|\boldsymbol{\lambda}_{k},s)=\frac{1}{s}h\left(\frac{1}{s}\mathbf{x}\right)g(\boldsymbol{\lambda}_{k})\exp\left\{\frac{1}{s}\boldsymbol{\lambda}_{k}^{\mathrm{T}}\mathbf{x}\right\}$$
>$s$는 스케일링 파라미터로 모든 클래스에서 공유됨

2개 클래스의 경우에 logistic sigmoid에 따른 $a(\mathbf{x})$를 구하면 다음과 같음
>$$a(\mathbf{x})=(\boldsymbol{\lambda}_1-\boldsymbol{\lambda}_2)^\mathrm{T}\mathbf{x}+\ln g(\boldsymbol{\lambda}_1)-\ln g(\boldsymbol{\lambda}_2)+\ln p(\mathcal{C}_1)-\ln p(\mathcal{C}_2)$$

K개 클래스면 softmax에 따른 $a(\mathbf{x})$를 구하면 다음과 같음
>$$a_k(\mathbf{x})=\boldsymbol{\lambda}_k^\mathrm{T}\mathbf{x}+\ln g(\boldsymbol{\lambda}_k)+\ln p(\mathcal{C}_k)$$

둘 다 linear function임을 알 수 있음
# 5.4 Discriminative Classifiers
앞선 생성형 접근 말고, 일반화 선형 모형의 파라미터를 MLE로 직접 예측하는 방법도 존재
이 경우엔 $p(\mathcal{C}_k|\mathbf{x})$에 바로 MLE를 적용함 (discriminative probabilistic modelling : 판별 확률 모델링)
장점 : 학습할 파라미터가 적음 (앞선 클래스 조건 밀도의 형태가 실제와 많이 다를 경우 성능 향상)
### 5.4.1 Activation functions
선형회귀의 결과값은 $(-\infty,\infty)$이지만 분류에선 posterior가 $(0,1)$에 존재하게 하기 위해 비선형 함수인 활성화 함수를 사용
>$$y(\mathbf{x},\mathbf{w})=f\begin{pmatrix}\mathbf{w}^\mathrm{T}\mathbf{x}+w_0\end{pmatrix}$$
>통계학에선 이것의 역함수를 link function이라고도 함
>활성화 함수가 비선형이어도 입력과 출력이 상수이므로, 결정 경계는 $\mathbf{x}$에 대한 linear function
>이러한 성질이 선형회귀 모형보다 더욱 복잡한 분석적, 계산적 특성으로 이끔
>그럼에도 불구하고 책의 남은 장들의 더 유연한 비선형 모델에 비해선 간단함
### 5.4.2 Fixed basis functions
지금까지 입력을 그대로 사용했지만 입력에 고정된 비선형 변환을 적용한 후에도 모델에 적용할 수 있음 (basis functions $\phi(\mathbf{x})$)
비선형 변환을 적용한 후의 결정 경계는 새로운 특성 공간 $\phi$에선 선형이지만 원 공간에서는 비선형일 수 있음
![basis functions](images/5.4.2%20basis%20functions.png)
basis function 중 하나는 보통 상수값으로 설정해 bias 역할 ($\phi_0(\mathbf{x}) = 1$)

실전에서는 원 공간에서 클래스 조건 밀도가 중첩되는 경우가 많으며 posterior에서도 나타남 (ex. 어느 클래스로 분류될 확률이 100%가 아닌 경우)
이럴 땐 posterior를 정확히 모델링 하는게 최적이며 결정 이론을 적용해 분류를 진행함

비선형 변환을 사용해도 클래스 간 중첩을 완전히 없앨 수 없지만, 적절한 비선형 함수를 선택하면 posterior를 모델링하는게 쉬워짐

고정된 basis function model은 중요한 한계점을 가지고 있으며 이는 나중 장들에 의해 설명 (데이터에 맞추어진 basis function)

### 5.4.3 Logistic regression
0에서 1사이의 연속적인 확률값을 예측하고, 이 값을 클래스로 바꿔서 **이진 분류**를 수행하는 모델
5.3에서 logistic sigmoid로 쓰인 posterior의 형태를 통계학에서 logistic regression이라고 부름
>$$p(\mathcal{C}_1|\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma\left(\mathbf{w}^\mathrm{T}\boldsymbol{\phi}\right)$$
>다음의 성질도 만족함 : $p(\mathcal{C}_2|\phi)=1-p(\mathcal{C}_1|\boldsymbol{\phi})$ 
>$M$차원 특성 공간에 대해 $M$개의 조절 가능한 파라미터를 가짐
>가우시안 분포를 활용한 생성적 접근 방식에선 차원에 대해 이차적으로 증가했음

- MLE
MLE를 하기 위해 시그모이드 함수의 미분값을 알아야 함
>$$\frac{\mathrm{d}\sigma}{\mathrm{d}a}=\sigma(1-\sigma)$$

likelihood 함수는 다음과 같음
>$$p(\mathbf{t}|\mathbf{w})=\prod_{n=1}^Ny_n^{t_n}\left\{1-y_n\right\}^{1-t_n}$$
>where $\mathbf{t} = (t_1,\ldots,t_N)^\mathrm{T}$ and  $y_n = p(\mathcal{C}_1|\phi_n)$

likelihood에 -log를 취해 오차함수를 정의하는데 이는 **cross-entropy error function**과 같음
>$$E(\mathbf{w})=-\ln p(\mathbf{t}|\mathbf{w})=-\sum_{n=1}^N\left\{t_n\ln y_n+(1-t_n)\ln(1-y_n)\right\}$$
>where $y_n=\sigma(a_n)$ and $a_n=\mathbf{w}^\mathrm{T}\boldsymbol{\phi}_n$
>gradient를 취하면 시그모이드 함수의 미분값이 상쇄되어 다음의 모습 (오차와 basis function의 곱들의 합)
>$$\nabla E(\mathbf{w})=\sum_{n=1}^N(y_n-t_n)\boldsymbol{\phi}_n$$
>즉, 각 데이터 포인트에서 오차가 기울기 계산에 직접적으로 기여함
>이 형태가 선형회귀 모형에서 MSE에 대한 gradient와 정확히 일치하며, 본질적으로 유사한 구조를 띄는 것을 알 수 있음

gradient(기울기)가 0이 되는 지점의 파라미터가 MLE지만, 비선형성 때문에 간단한 공식으로 표현이 안됨
따라서 SGD(Stochastic Gradient Descent)를 사용할 수 있음
>개별 데이터 포인트마다의 기울기로 가중치를 업데이트 하는 방법

하지만 비선형성이 크지 않고 오차 함수가 convex하므로 IRLS(iterative reweighted least squares)를 사용할 수 있음
>반복적으로 가중치를 재조정하여 최소제곱법을 적용하는 방법
>DNN 같이 복잡한 모델에선 효율적이지 않으므로 SGD가 더 적합

- MLE의 문제점
선형적으로 완벽히 분리가 되는 데이터에 대해서 MLE에서 **가중치가 무한대로 증가**하기 때문에 로지스틱 함수가 계단 함수처럼 되어 일반화 성능이 떨어짐 (**과적합**)
과적합이 되면 모든 데이터에 대해 동일한 posterior를 제공하므로 해가 여러개 존재할 수 있음
정규화 항을 추가함으로써 일반화 성능을 키울 수 있음
### 5.4.4 Multi-class logistic regression
다중 클래스로 확장한 형태로, 이번엔 softmax 사용
>$$p(\mathcal{C}_k|\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_j\exp(a_j)}$$
>where $a_k=\mathbf{w}_k^T\phi$

미분값은 다음과 같음
>$$\frac{\partial y_k}{\partial a_j}=y_k(I_{kj}-y_j)$$
>$I$는 항등 행렬

- likelihood 함수
>$$p(\mathbf{T}|\mathbf{w}_1,\ldots,\mathbf{w}_K)=\prod_{n=1}^N\prod_{k=1}^Kp(\mathcal{C}_k|\boldsymbol{\phi}_n)^{t_{nk}}=\prod_{n=1}^N\prod_{k=1}^Ky_{nk}^{t_{nk}}$$
>where $y_{nk}=y_k(\boldsymbol{\phi}_n)$, and $\mathbf{T}$ is  an $N\times K$ matrix


-log 취하면 **cross-entropy error function**
>$$E(\mathbf{w}_1,\ldots,\mathbf{w}_K)=-\ln p(\mathbf{T}|\mathbf{w}_1,\ldots,\mathbf{w}_K)=-\sum_{n=1}^N\sum_{k=1}^Kt_{nk}\ln y_{nk}$$
>$$\nabla_{\mathbf{w}_j}E(\mathbf{w}_1,\ldots,\mathbf{w}_K)=\sum_{n=1}^N\left(y_{nj}-t_{nj}\right)\boldsymbol{\phi}_n$$
>선형 모델의 MSE와 로지스틱 회귀의 cross-entropy error와 유사한 형태로, 똑같이 SGD로 최적화 가능

선형 분류 모델은 다음 그림처럼 단일 레이어 NN으로 표현될 수 있음
![multiclass logistic](images/5.4.4%20multiclass%20logistic.png)
앞과 똑같이 오차를 통해 가중치의 기울기가 결정됨
>$$\frac{\partial E(\mathbf{w}_1,\ldots,\mathbf{w}_K)}{\partial w_{ij}}=\sum_{n=1}^N\left(y_{nk}-t_{nk}\right)\phi_i(\mathbf{x}_n)$$
>
### 5.4.5 Probit regression
지수족(exponential family)에 대한 클래스 조건 분포는 로지스틱 함수, 소프트맥스 함수가 선형함수에 작용한 형태
모든 클래스 조건 밀도가 이와 같이 간단히 생기지 않을 것이므로 새로운 형태의 discriminative probabilistic model을 제안
- two-class case
우선 GLM(일반화 선형 모형)에서 사용했던 $p(t=1|a)=f(a)$ 사용
$a_n=\mathbf{w}^\mathrm{T}\boldsymbol{\phi}_n$에 대해 다음과 같이 노이즈가 있는 임계값 모델을 고려
>$$\begin{cases}t_n=1,&\mathrm{if~}a_n\geqslant\theta,\\t_n=0,&\text{otherwise.}\end{cases}$$
>where $\theta$ is threshold drawn from $p(\theta)$

활성화 함수를 적분하면 다음과 같음
![activate function integral](images/5.4.5%20activate%20function%20integral.png)
>$$f(a)=\int_{-\infty}^ap(\theta)\mathrm{~d}\theta $$
>밀도 함수가 가우시안이면 **probit function** (자료 첫 그림 참고)
>$$\Phi(a)=\int_{-\infty}^a\mathcal{N}(\theta|0,1)\mathrm{~d}\theta$$
>평균과 분산이 바뀌어도 모델이 바뀌진 않음

- error function
특정 구간에서의 가우시안 함수 적분
실제 계산에서 오류 함수를 사용하는 건 매우 일반적
>$$\mathrm{erf}(a)=\frac2{\sqrt{\pi}}\int_0^a\exp(-\theta^2/2)\mathrm{~d}\theta $$

probit function과 관련이 있음
>$$\Phi(a)=\frac12\left\{1+\frac1{\sqrt{2}}\mathrm{erf}(a)\right\}$$
>즉, probit function은 오류 함수의 변형

- probit regression
probit activation function을 기반으로 한 GLM
MLE로 파라미터 결정 가능하며 앞선 내용들로 쉽게 가능
로지스틱과는 달리 probit은 극값에 대해 제곱의 지수꼴로 수렴되므로 이상치에 훨씬 민감함
### 5.4.6 Canonical link functions
앞선 결과들을 통해 선형 회귀, 로지스틱 회귀, 다중 로지스틱 회귀가 모두 비슷한 형태의 결과를 가짐을 알 수 있었음
지수족(exponential family)에 기반한 조건부 분포를 가정하고 canonical link function을 활성화 함수로 선택하면 위와 같은 일반화된 결과를 가짐을 보여줌
- 지수족
>$$p(t|\eta,s)=\frac1sh\left(\frac ts\right)g(\eta)\exp\left\{\frac{\eta t}s\right\}$$
>$$y\equiv\mathbb{E}[t|\eta]=-s\frac d{d\eta}\ln g(\eta)$$
>결국 $\eta=\psi(y)$의 관계

- GLM
>$$y=f(\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi})$$
>$f$는 활성화함수이며 역함수는 link function(연결 함수)로 통계학에서 쓰는 용어

log likelihood function
>$$\ln p(\mathbf{t}|\eta,s)=\sum_{n=1}^N\ln p(t_n|\eta,s)=\sum_{n=1}^N\left\{\ln g(\eta_n)+\frac{\eta_nt_n}s\right\}+\mathrm{const}$$
>여기서 모든 관측치에 대해 같은 스케일 파라미터 $s$를 가정함 (ex. 가우시안 분포의 분산)
>$$\begin{aligned}\nabla_{\mathbf{w}}\ln p(\mathbf{t}|\eta,s)&=\quad\sum_{n=1}^N\left\{\frac{\mathrm{d}}{\mathrm{d}\eta_n}\ln g(\eta_n)+\frac{t_n}s\right\}\frac{\mathrm{d}\eta_n}{\mathrm{d}y_n}\frac{\mathrm{d}y_n}{\mathrm{d}a_n}\nabla_\mathbf{w}a_n\\&=\quad\sum_{n=1}^N\frac1s\left\{t_n-y_n\right\}\psi^{\prime}(y_n)f^{\prime}(a_n)\boldsymbol{\phi}_n\end{aligned}$$
>where $a_n=\mathbf{w}^\mathrm{T}\boldsymbol{\phi}_n$

이때, $f^{-1}(y)=\psi(y)$로 설정하면 $f(\psi(y))=y$가 성립하므로 $f^{\prime}(\psi)\psi^{\prime}(y)=1$
$a={\psi}$가 되므로 $f^{\prime}(a)\psi^{\prime}(y)=1$가 성립
따라서 다음과 같이 오류 함수의 기울기가 간단해짐
>$$\nabla\ln E(\mathbf{w})=\frac1s\sum_{n=1}^N\{y_n-t_n\}\boldsymbol{\phi}_n$$
>앞서 봤던 형태와 굉장히 유사함
>오류 함수의 선택과 output 활성화 함수 선택이 자연스럽게 이어지는 모습
>단일 레이어 NN의 맥락에서 유도했지만 이는 앞으로 다룰 DNN에서도 적용됨