# 6.4 Error functions
머신러닝에서 자주 사용되는 loss function들에 대한 소개를 해주는 것 같아요~
## Regression
### single target variable
$$p(t|X,w,\sigma^{2})=\prod^{N}_{n=1}p(t_{n}|y(x_{n},w), \sigma^{2})$$
머신러닝에서 우리는 일반적으로, loss function을 최소화 하는 것에 관심이 있음. 따라서 negative log likelihood function을 고려.
$${1\over {2\sigma^{2}}}\sum\limits^{N}_{n=1}\{y(x_{n},w)-t_{n}\}^{2}+{N\over 2}\ln \sigma^{2}+{N\over 2} \ln (2\pi)$$
그리고 이 경우, $w$에 대해서 negative log likelihood function을 최소화 하는 것은, sum-of-squared error를 최소화 하는 것과 같음.
$$\text{sum-of-squared-error}=E(w)={1\over 2}\sum\limits^{N}_{n=1}\{y(x_{n},w)-t_{n}\}^{2}$$
이걸 최소화 하도록 하는 $w^{*}$를 구해도, 이게 likelihood function의 global maximum은 아닐 수도 있음. 왜냐하면, $y(x_{n},w)$가 매우 복잡해서, $E(w)$가 non-convex 일수도 있기 때문.

이제 우리가 $w^{*}$를 구하고나면, $\sigma^{2}$는 다음과 같이 구해질 수 있음.
$$\sigma^{2*}={1\over N}\sum\limits^N_{n=1}\{y(x_{n},w^{*})-t_{n}\}^{2}$$

### Multiple target variable
#### Multiple target variable이 무엇인가?
회귀 문제에서 어떤 경우 여러 개의 타겟 변수를 동시에 예측해야 하는 경우가 있음.
ex) 다변량 회귀(나이, 성별, 식습관 등으로 키와 몸무게를 동시에 예측)
ex) 다중 출력 시간 시계열(시간 시계열 데이터를 기반으로 여러 미래 시점의 값을 예측)

target value들에 대한 conditional distribution이 아래와 같다고 가정하자.
$$p(\mathbf{t}|\mathbf{x},\mathbf{w})=\mathcal{N}\left(\mathbf{t}|\mathbf{y}(\mathbf{x},\mathbf{w}),\sigma^2\mathbf{I}\right)$$
이 경우, $K$ 개의 target variable, $t_{1},...,t_{K}$이 있다고 가정하고, 각 target variable들이 $(\mathbf{x},\mathbf{w})$가 주어졌을 때 서로 독립이며, 모두 같은 분산 $\sigma^{2}$를 공유한다고 가정하자. 
위 식에서 공분산 행렬은 $K\times K$ 행렬이다.

마찬가지로, 위 경우 $\mathbf{w}$에 대해서 likelihood function을 최대화 하는 것은, sum-of-squared-error를 최소화 하는 것과 같다.
$$E(\mathbf{w})=\frac12\sum_{n=1}^N\|\mathbf{y}(\mathbf{x}_n,\mathbf{w})-\mathbf{t}_n\|^2$$
위 식을 최소화 하는 $\mathbf{w}^{*}$를 구하고 나면, 이어서 $\sigma^{2}$도 구할 수 있다.
$$\sigma^{2*}={1\over NK}\sum\limits^{N}_{n=1}||\mathbf{y}(\mathbf{x}_n,\mathbf{w}^{*})-\mathbf{t}_n\|^2$$

## Classification
### Binary Classification
이진 분류에서 target의 conditional distribution은 다음과 같다.
$$p(t|\mathbf{x},\mathbf{w})=y(\mathbf{x},\mathbf{w})^{t}\{1-y(\mathbf{x},\mathbf{w}))\}^{(1-t)}$$
이진 분류 문제에서는, cross-entropy loss 를 사용합니다.
$$E(\mathbf{w})=-\sum\limits^{N}_{n=1}\{t_{n}\ln y_{n}+(1-t_{n})\ln (1-y_{n})\}$$
이때 $y_{n}=y(\mathbf{x}_{n},\mathbf{w})$를 의미합니다.

우리가 만약, 이진 값을 가지는 $K$개의 타겟 변수를 예측하고자 한다면,
이 경우 target의 conditional distribution은 다음과 같다.
$$p(\mathbf{t}|\mathbf{x},\mathbf{w})=\prod^{K}_{k=1}y_{k}(\mathbf{x},\mathbf{w})^{t_k}[1-y_{k}(\mathbf{x},\mathbf{w})]^{1-t_{k}}$$
이때 $y_{k}(\mathbf{x},\mathbf{w})$는 입력 $\mathbf{x}$와 모델 매개변수 $\mathbf{w}$를 통해 계산된 $k$번째 target 변수의 확률
$t_{k}$는 실제 레이블 값(0 또는 1)임.

마찬가지로 이 경우의 loss function은
$$E(\mathbf{w})=-\sum\limits^{N}_{n=1}\sum\limits^{K}_{k=1}\{t_{nk}\ln y_{nk}+(1-t_{nk})\ln (1-y_{nk})\}$$
이고, 이때 $y_{nk}=y_{k}(\mathbf{x}_{n},\mathbf{w})$ 임. 즉, $n$ 번째 데이터의 $k$ 번째 출력 확률

### Multiclass Classification
다중 클래스 분류 문제는, 출력 클래스가 서로 상호 배타적인 $K$개의 클래스를 가지는 경우.
이때 target 변수는 one-hot encoding 되어 있어야함.
특정 클래스 $k$에만 1, 나머지는 0으로 코딩.

이 경우 모델의 출력은, 각 클래스에 속할 확률을 계산

다중 클래스 분류 문제에서도 Cross-Entropy Loss를 최소화하는 문제를 품,
$$
E(\mathbf{w}) = - \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_k(x_n, \mathbf{w})
$$

이때 $t_{nk}$:  $n$번째 샘플의 $k$번째 클래스 타겟 값 (0 또는 1), $y_k(x_n, \mathbf{w})$: 모델이 $k$번째 클래스에 대해 출력한 확률임.

cross-entropy loss를 최소화 하면서 모델 적합을 완료하고, 이후 특정 $x$에 대해 각 클래스에 속할 확률은 softmax함수로 계산
$$
y_k(x, \mathbf{w}) = \frac{\exp(a_k(x, \mathbf{w}))}{\sum_{j=1}^K \exp(a_j(x, \mathbf{w}))}
$$
이때 $a_k(x, \mathbf{w})$: 가중치 $\mathbf{w}$ 와 $x$의 선형 결합 값임.

# 6.5 Mixture Density Networks
머신러닝에서 다루는 문제는 종종 $p(t|x)$를 모델링 하는 것인 경우가 있음. 그리고 $p(t|x)$를 정규분포로 가정하곤 하는데, 현실에서는 하나의 출력($t$)이 여러 입력($x$)로 부터 올 수 있기에, 단순히 $p(t|x)$를 정규분포로 가정하면 잘못된 예측으로 이어질 수 있음.
따라서 이를 해결하기 위해, 여러 개의 정규분포가 혼합된 모델로 $p(t|x)$를 표현할 수 있음. 이를 혼합 밀도 네크워크(Mixture Density Network, MDN)라고 함.

### MDN
$$p(t|x)=\sum_{k=1}^K\pi_k(x)\mathcal{N}(t|\mu_k(x),\sigma_k^2(x))$$
이때 $\pi_k(x)$ 는 각 정규분포 $\mathcal{N}(t|\mu_k(x),\sigma_k^2(x))$ 의 가중치로 이해할 수 있음.  그리고 $\pi_{k}(x), \mu_{k}(x), \sigma_{k}^{2}(x)$는 모두 데이터 $x$에 의존하는 값들임. 입력 데이터에 따라 동적으로 변함.
> 유연함

또한, 이때 $\pi_{k}(x)$는 각 분포의 가중치기에, 0에서 1 사이의 값을 가져야하고, 합이 1이어야함.
$$\sum\limits^{K}_{k=1}\pi_{k}(x)=1,\quad 0\leq\pi_{k}(x)\leq1$$
그리고 이 $\pi_{k}(x)$ 들은 다음과 같이 softmax 함수를 통해서 계산될 수 있음.
$$\pi_k(x)=\frac{\exp(a_k^\pi)}{\sum_{j=1}^K\exp(a_j^\pi)}$$
이때 $a_{k}^{\pi}=W^{\pi}_{k}z+b^{\pi}_{k}$ 라는데, ... 아마 신경망에서의 각 layer에서의 input, 이전 layer의 output을 말하는 듯?

그리고 평균과 분산은 각각 다음과 같이 계산함.
$$\begin{aligned}
\sigma_{k}(x)&=\exp (a^{\sigma}_{k})\\
\mu_{kj}(x)&=a^{\mu}_{kj}
\end{aligned}$$
여기서도 마찬가지로, $a^{\sigma}_{k},a^{\mu}_{kj}$는 각각 신경망에서의 각 layer에서의 input, 이전 layer의 output임.

그리고 이렇게 MDN을 구성했을 때, MDN의 negative log likelihood 함수는 아래와 같음.
$$E(\mathbf{w})=-\sum_{n=1}^{N}\ln\left\{\sum_{k=1}^{K}\pi_{k}(\mathbf{x}_{n},\mathbf{w})\mathcal{N}\left(\mathbf{t}_{n}|\boldsymbol{\mu}_{k}(\mathbf{x}_{n},\mathbf{w}),\sigma_{k}^{2}(\mathbf{x}_{n},\mathbf{w})\right)\right\}$$
#### 각 매개변수에 대한 gradient
$$\begin{aligned}
\frac{\partial E_n}{\partial a_k^\pi}&=\pi_k-\gamma_{nk}\\
\frac{\partial E_n}{\partial a_{kl}^\mu}&=\gamma_{nk}\left(\frac{\mu_k-t_n}{\sigma_k^2}\right)\\
\frac{\partial E_n}{\partial a_k^\sigma}&=\gamma_{nk}\left[\frac{\|t_n-\mu_k\|^2}{\sigma_k^2}-1\right]
\end{aligned}$$
이 값들은 Gradient Descent로 손실함수를 최소화하는데 사용.

이때 $\gamma_{nk}$는 아래와 같음. $$\gamma_{nk}=\frac{\pi_k\mathcal{N}_{nk}}{\sum_{l=1}^{K}\pi_l\mathcal{N}_{nl}}, \quad \mathcal{N}_{nk}=\mathcal{N}(t_n|\mu_k(x_n),\sigma_k^2(x_n))$$
이렇게 이해가 전혀 가지 않은 과정으로 MDN을 학습하고나면, 이제 다음과 같은 값들을 계산할 수 있음.
