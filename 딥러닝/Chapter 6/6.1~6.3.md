# Limitations of Fixed Basis Functions
### 고정된 함수의 한계
일반적으로 사용 하는 선형 기저함수 모델
$$y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^Mw_j\phi_j(\mathbf{x})+w_0\right)$$
- 기저함수는 훈련 데이터와 독립적으로 고정
- 이는 모델의 유연성을 제한한다

## The curse of dimensionality
### 차원의 저주
- 차원이 증가할 수록 데이터 분포의 밀도가 낮아저, 학습 알고리즘이 적절한 패턴을 알기 어려워진다
- 거리측정의 의미가 줄어든다.
- 차원이 늘어나면 지수적으로 필요한 데이터가 증가함
- 과적합의 위험이 증가함
- 결국 실용성과 효율성이 감소한다
## High dimensional spaces
### 고차원 공간
- 고차원 공간
- 하지만 고차원에도 장점이 있다는데...
$$\frac{V_D(1)-V_D(1-\epsilon)}{V_D(1)}=1-(1-\epsilon)^D$$
- 전체 부피 $V_{D(1)}$ 중에서 중심에서부터 반지름 $1-\epsilon$ 의 영역이 차지하는 부피의 비율.
- 이는 구의 부피에서 중심으로부터 까지의 영역을 $1-\epsilon$ 제외한 부분이 전체에서 차지하는 비율을 나타냄.
- 결국 높은 차원에서는 껍질에 부피가 집중

- 저차원에도 분명한 한계가 존제함
	- 저차원의 직관이 고차원 분포를 항상 설명하지는 않음
	- ![[images/Pasted image 20241117073353.png]]

## Data manifolds
### 데이터 매니폴드
- 전통 머신러닝 방법의 한계
	- 입력변수가 많아 질수록 필요한 기저함수의 수가 기하급수적으로증가
	- 이는 기저 함수가 데이터나 문제 특성에 따라 동적으로 조정
- 이미지 데이터
	- 이미지 데이터는 3차원 manifold로 볼수 있음 (가로, 세로, 색체널)
	- 해상도를 높여도 본질은 변하지 않음
- manifold
	- 실질 차원 정도로 생각 할 수있음
		- 일반적으로 데이터 공간보다 훨씬 낮기 때문에 효율성이 대폭 개선
		- 신경망은 이러한 매니폴드 차원에 적용한 기저 함수로 학습 가능
		- 예측에 중요한 방향만 학습하여 효율성을 높일수 있다.
			- 예를 들면 방향만을 알고 싶으면 1차원으로도 생각할수 있다.
## Data dependent basis functions
### 데이터 의존 기저함수
- 도메인 지식을 활용하여 기저를 직접만드는 것이 과거의 머신러닝의 트렌드
- 하지만 효과적이지 못해 데이터를 기반으로 기저함수를 학습하는 데이터 중심 접근법으로 변경

- 데이터 중심의 기저함수
	- manifold에 집중된 기저함수가 중요
	- SVM을 활용
	- 데이터 크기보다는 작지만 여전히 데이터 크기에 따라 기저함수 수가 증가함
	- 여전히 많은 한계
	- 결국 DNN으로 대체

# Multilayer Networks
### 다층 네트워크
- 학습 가능한 기저함수를 선택하고, 훈련하는 형식으로 조정
- 모델의 Loss function을 최소화 하는 방향으로 학습하며, 보통 SGD같은 방식을 활용
- 기저함수는 선형 결합형태를 따르며, 비선형 변환을 활용하여 유연하게 대처
![[images/Pasted image 20241117075608.png]]

- Parameter matrices
- Universal approximation
	- 신경망이 충분히 큰 은닉층과 적절한 활성화 함수를 사용하면 모두 근사가 가능하다.
- Hidden unit activation functions
- ![[images/Pasted image 20241117080016.png]]
- 일반적으로는 ReLU 활용, 경우에 따라 tanh와 leakly ReLU를 활용할 때도 있음.
- 나머지는 별로 안쓰는 듯함
- 대칭성 있는 활성화 함수를 활용하면 각 은닉 유닛에 대해 가중치와 바이어스의 부호를 반전시킬 수 있음. 결과적으로 변화하지 않는 문제를 가짐

# Deep Networks
### 딥 네트워크
- 다층으로 구성된 신경망
- 다층 네트워크가 2층 네트워크 보다 상대적으로 적은 유닛으로 함수를 표현 가능하다

## Hierarchical Representations
### 계층적 표현
- DNN은 출력이 입력 공간과 계층적 관계를 통해 연결, inductive bias를 포함
- 저층에서 저수준의 특징을 감지하여 고층으로 갈수록 높은 수준의 특징을 감지
- 직선 -> 수염 -> 고양이 이런순인 셈
## Distributed Representations
### 분산표현
- 특징이라는 것은 높은 값을 말하고, 낮은 값을 특징이 없는 것으로봄
- 특징 구조를 잘 활용하면 분류 클래스가 지수적으로 늘어나는 것을 막을 수 잇음

## Representation Learning
### 표현 학습
- 마지막 층은 두 클래스를 구별하는 역할, 단순 선형 분류기의 역할
- 표현 학습은 데이터를 비션형적으로 변환하여 후속 작업을 쉽게 해결 할 수 있도록 함.
- 이를 비지도학습(오토 인코더)에 활용하여 레이블링을 할 수 있음(사전 훈련 활용)

## Transfer Learning
### 전이 학습
- 이미 학습된 모델을 활용하여 다른 학습에 활용
- 이렇게 다양한 데이터 케이스로 학습한 모델은 한층더 성숙해짐
- 일반적으로 앞단의 층들은 고정한 체로 학습을 진행하는 방법
-  데이터셋이 부족한 케이스에 비교적 높은 성능을 자랑
![[images/Pasted image 20241117082621.png]]

## Contrastive learning
### 반대 학습
- 데이터들의 유사성을 기반으로 한 학습
- infoNCE Loss
$$E(\mathbf{w})=-\ln\frac{\exp\{\mathbf{f_w}(\mathbf{x})^\mathrm{T}\mathbf{f_w}(\mathbf{x}^+)\}}{\exp\{\mathbf{f_w}(\mathbf{x})^\mathrm{T}\mathbf{f_w}(\mathbf{x}^+)\}+\sum_{n=1}^N\exp\{\mathbf{f_w}(\mathbf{x})^\mathrm{T}\mathbf{f_w}(\mathbf{x}_n^-)\}}$$
- 다음 로스를 줄이는 방향으로 학습
![[images/Pasted image 20241117083027.png]]