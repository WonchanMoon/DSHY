---
날짜: 2025-02-16
완료: false
tags:
---

## 15.3 Expectation-Maximization Algorithm
- EM 알고리즘은 **잠재 변수(Latent Variable)** 가 포함된 확률 모델에서 **최대 가능도(MLE, Maximum Likelihood Estimation) 또는 최대 사후 확률(MAP, Maximum a Posteriori)** 을 추정하는 방법이다. 보통은 MLE로 진행
- 왜 해야하는가?
	- **불완전 데이터(Incomplete Data)**: 현실에서는 잠재 변수 $Z$를 모른다. 이때 **주어진 $X$만으로 $\theta$를 찾기 어렵다**.
	- MLE 함수의 해를 찾기 힘들다.
		- 미분이 힘들다.
### EM algorithm 과정
- **E-Step (Expectation Step, 기대 단계)**
	- 현재의 파라미터$\theta^{old}$ 을 사용하여 잠재 변수$Z$의 사후 확률 분포를 계산
		- $p(\mathbf{Z} \mid \mathbf{X}, \theta^{\text{old}})$
	- 이 사후 분포를 이용해 **완전 데이터 log-likelihood의 기대값** $Q(\theta, \theta^{\text{old}})$을 계산
		- $Q(\theta, \theta^{\text{old}}) = \sum_{\mathbf{Z}} p(\mathbf{Z} \mid \mathbf{X}, \theta^{\text{old}}) \ln p(\mathbf{X}, \mathbf{Z} \mid \theta)$
- **M-Step (Maximization Step, 최대화 단계)**
	- 기대값 $Q(\theta, \theta^{\text{old}})$을 최대화 하는 새로운 파라미터 $\theta^{new}$ 를 찾음
		- $\theta^{\text{new}} = \arg\max_{\theta} Q(\theta, \theta^{\text{old}})$
		- 기대값을 최대화 하는 방향으로 업데이트
- **반복수행**
	- 반복할수록 불완전 데이터에 대한 가능도 $p(X|\theta)$가 증가한다.
![[Pasted image 20250216081442.png]]

## GMM
- 저번장에서 GMM 에서 어떻게 EM- algorithm이 적용될 수 있었는지를 확인
- [[15.1~15.2]]
- GMM은 데이터가 독립적(i.i.d.)이라고 가정하지만, **시간적인 순서가 있는 데이터(예: 음성, 주식 가격)** 를 모델링할 때는 **은닉 마르코프 모델(HMM)** 을 사용
- HMM은 가우시안 컴포넌트 간의 **전이 확률(transition probability)** 을 고려하여 확장한 모델.
![[Pasted image 20250216082109.png]]


## Mixtures of Bernoulli Distributions
- 이산 이진 변수를 대상으로 하는 혼합 모델
- **잠재 클래스 분석(latent class analysis)** 모델
$$p(x \mid \mu) = \prod_{i=1}^{D} \mu_i^{x_i} (1 - \mu_i)^{(1 - x_i)}$$
- 여기서 $\mathbf{x} = (x_1, \dots, x_D)^{T}$, $\boldsymbol{\mu} = (\mu_1, \dots, \mu_D)^{T}$  
- 개별 변수는 독립적이며, 평균과 공분산은 쉽게 구할 수 있음
	- $\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu}$
	- $\mathrm{cov}[\mathbf{x}] = \mathrm{diag}(\mu_i (1 - \mu_i))$
- 혼합 모델
	- $p(x \mid \pi, \mu) = \sum_{k=1}^{K} \pi_k p(x \mid \mu_k)$ 
	- $p(\mathbf{z} \mid \boldsymbol{\pi}) = \prod_{k=1}^{K} \pi_k^{z_k}$
	- 완성된 log-likelihood 함수
$$\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi}) = \sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk} \left\{ \ln \pi_k + \sum_{i=1}^{D} \left[ x_{ni} \ln \mu_{ki} + (1 - x_{ni}) \ln (1 - \mu_{ki}) \right] \right\}$$
### EM 알고리즘
- E-Step: 책임도(Responsibility) 계산
	- 각 데이터 포인트가 특정 구성요소 kkk에 속할 확률(책임도):
		- $\gamma(z_{nk}) = \frac{\pi_k p(x_n \mid \mu_k)}{\sum_{j} \pi_j p(x_n \mid \mu_j)}$
	- $N_k$ 구성요소 $k$에 속하는 데이터의 효과적인 개수
		- $N_k = \sum_{n=1}^{N} \gamma(z_{nk})$
	- $X_k$ 책임도를 반영한 데이터 가중 평균
		- $\mathbf{X}_k = \sum_{n=1}^{N} \gamma(z_{nk}) x_n$
- M-Step: 파라미터 업데이트
	- 각 군집의 평균 업데이트
		- $\mu_k = \frac{\mathbf{X}_k}{N_k}$
	- 혼합 계수 업데이트
		- $\pi_k = \frac{N_k}{N}$

### 베르누이 모델의 특징
- 가능도 함수가 무한으로 발산하는 특이점이 없음
- 베르누이 분포의 가능도는 0~1 범위 내에서 정의 되므로, 극단적인 값이 나오지 않음

--- 
다음시간
## Evidence Lower Bound
- 우리는 log likelihood 함수를 직접 최적화 하는 것이 어려움으로 이를 우회하는 방법을 찾음
- 잠재 변수 Z을 도입하여 완전 데이터 우도 함수 $p(\mathbf{X}, \mathbf{Z} \mid \theta)$ 를 최적화하는 접근법을 사용
- 우리는 다음식을 최적화 하는 게 목표
$$p(\mathbf{X} \mid \theta) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \theta)$$
- 하지만 이항을 직접 최적화하는 것은 어려워 잠재변수 $Z$의 분포 $q(Z)$를 도입하여 변헝
- 다음 로그 가능도를 아래와 같이 분리
$$\ln p(\mathbf{X} \mid \theta) = \mathcal{L}(q, \theta) + KL(q \parallel p)$$
- $\mathcal{L}(q, \theta)$ : **변분 하한(variational lower bound)**
- $KL(q \parallel p)$ : **쿨백-라이블러 발산(Kullback-Leibler divergence)** (두 분포 사이의 차이를 측정)
- 여기서 $\mathcal{L}(q, \theta)$은 다음과 같이 정의 
$$\mathcal{L}(q, \theta) = \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left( \frac{p(\mathbf{X}, \mathbf{Z} \mid \theta)}{q(\mathbf{Z})} \right)$$
- 이는 q(Z)에 대한 기대값(expectation)을 나타내며, 최적화할 수 있는 대상
- $KL(q \parallel p)$ 은 발산의 성질이 있는데
$$KL(q \parallel p) = - \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left( \frac{p(\mathbf{Z} \mid \mathbf{X}, \theta)}{q(\mathbf{Z})} \right) \geq 0.$$
- 이는 **항상 0 이상이며**, 오직 $q(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X}, \theta)$때 0이 됌
- 따라서, 변분 하한(ELBO)은 로그 우도의 하한을 제공함:
$$\mathcal{L}(q, \theta) \leq \ln p(\mathbf{X} \mid \theta).$$![[Pasted image 20250216100608.png]]


