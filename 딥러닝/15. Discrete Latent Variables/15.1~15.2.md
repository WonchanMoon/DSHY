---
날짜: 2025-02-08
완료: false
tags:
  - 클러스터링
  - latent
---
## Start

- 복잡한 데이터의 분포는 여러 개의 단순한 분포 가우시안 분포를 합쳐서 만들 수 있음
- 변수에는 2가지 가 있는데, 관측변수와 잠재 변수로 나눌 수 있음
	- 관측 변수 : 우리가 실제로 측정하거나 볼 수 있는 데이터의 일부
	- 잠재 변수 : 직접적으로 관측되지는 않지만, 데이터 생성 과정에 중요한 역할을 하는 변수
		- ex) 사진에서 물체가 찍힌 각도나 조명 조건 등이 될 수 있음
> 잠재 변수를 도입하면 데이터의 복잡한 분포를 더 간단하고 이해하기 쉬운 방식으로 설명 할 수 있음


## K-means 클러스터링
- 데이터를 $K$개의 군집으로 나누는 방법
- 중심좌표 $\mu_{k}$  $,k= 1,2,...,K$ 와의 겨리를 계산하여 어떤 군집에 포함될지 정하는 방식
- 다음을 E-step과 M-step으로 분리하여 이해할 수 있음
	- **E-step (Expectation step)**
		- $\mu_{k}$를 고정하고 $r_{nk}$를 최적화 한다. 즉, 각 데이터 포인즈 $x_n$을 가장 가까운 클러스터 중심에 할당
		- $r_{nk} = \begin{cases} 1 & \text{if } k = \arg\min_{j} \| x_n - \mu_j \|^2 \\ 0 & \text{otherwise} \end{cases}$
	- **M-step (Maximization step)**
		- $r_{nk}$를 고정한 상태에서 $\mu_{k}$를 최적화 한다
		-  $\mu_k = \frac{\sum_{n=1}^{N} r_{nk} x_n}{\sum_{n=1}^{N} r_{nk}}$

- Error Function $J$는 다음과 같이 정의
$$J = \sum_{n=1}^{N} \sum_{k=1}^{K} r_{nk} \left\| x_n - \mu_k \right\|^2$$
- $J$가 더이상 감소 하지 않을 때 수렴한다.
![[Pasted image 20250208203806.png]]
- 한계
	- K-means의 중요한 특성은 1개의 데이터가 1개의 클래스에 존재한다는 것인데, 어떠한 데이터 에 대해서는 그 경계가 불분명할 수 있다. (두 중심점 사이에 존재하는 경우)
	- 이러한 경우, **확률적 접근법을** 채택하면 ‘소프트(soft)’ 할당이 가능하여 더 적절한 결과를 얻을 수 있다.

## Image Segmentation & Image Compression
- K-means를 활용한 응용사레의 대표격이 **이미지 분할(image segmentation)과 이미지 압축(image compression)**
- 이미지 분할
	- 픽셀의 RGB 3차원의 값들을 k-means로 군집화 한후 중심 값으로 다시 그릴 수 있다
	- 다만 그럴 경우 공간적 근접성은 고려하지 않는게 되는데 이는 적절하지 않다.
	- 그래서 더 복잡한 구조의 알고리즘이 필요하다고함
![[Pasted image 20250208204256.png]]
- 이미지 압축
	- 이미지 압축은 손실 없는 압축과 손실 압축으로 나뉜다.
	- 손실 없는 압축
		- 원복 데이터를 정확히 복원 가능
	- 손실 압축
		- 약간의 정보 손실을 감소 하더라도 더 높은 압축을 가능하게 함
		- 해당경우 K-means를 활용하여 각 픽셀을 군집화 하고 각 클러스터의 ID만 기억 함으로써 압축 할 수 있고 코드북 백터로 불린다.
		- Example
			- 이미지가 N개의 픽셀로 이루어져있다고 가정하면 1개의 픽샐당 3개의 채널, 채널당 8 bit(0~256)의 정보가 필요함 -> 24N개의 용량
			- 각 픽셀을 K개의 클러스터에 할단된 ID를 전송해야함 -> $log_{2}K$ 
			- K개의 클러스터 중심도 전송해야함 -> 24K
			- 결과적으로 $24N \rightarrow 24K + Nlog_{2}K$ 

## Mixture of Gaussians
- 이전에 우리는 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 단순한 가우시안 성분의 선형 결합(linear superposition)으로 설명했다고함
- 이산 잠재 변수(discrete latent variables)를 통해 가우시안 혼합 모델을 수식화할 수 있음

- 가우시안 혼합 모델은 다음과 같이 나타 낼 수 있음
$$p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)$$
- 여기서 $\pi_{k}$는 혼합 계수((mixing coefficient))

### 잠재변수의 도입
- $K$차원의 이진 잠재 변수 $z$를 도입해보자
- $z_{k}\in\{0,1\}$ 이며 정확히 하나의 요소만 1이고 나머지는 0
	- 이는 어느 가우시안 성분에 속하는지를 보여 줄 수 있다
- 공분포 $p(x,z)$는 주변 분포 $p(z)$와 조건부 분포 $p(x|z)$ 로 표현 할 수 있음
- $p(z_k = 1) = \pi_k$을 만족하며 $\pi_k$ 는 $0 \leq \pi_k \leq 1$ , $\sum_{k=1}^{K} \pi_k = 1$ 을 만족한다
- 이는  $\pi_k$ 가 확률로서 유효해야함을 의미함

- 잠재 변수 분포
	- $p(z) = \prod_{k=1}^{K} \pi_k^{z_k}$
- 조건부 분포
	- $p(x \mid z_k = 1) = \mathcal{N}(x \mid \mu_k, \Sigma_k)$
- 두개를 합치면
	- $p(x \mid z) = \prod_{k=1}^{K} \mathcal{N}(x \mid \mu_k, \Sigma_k)^{z_k}$
- 결합 분포(Joint distribution)
	- $p(x) = \sum_{z} p(z) p(x \mid z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)$

### Responsibility(책임도)
- 베이즈 정리를 사용하여 각 가우시안 성분이 데이터 포인트 $x$에 대해 책임지는 정도를 계산
$$\gamma(z_k) = p(z_k = 1 \mid x) = \frac{\pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x \mid \mu_j, \Sigma_j)}$$
- $\pi_k$는 사전 확률(prior probability)로, 데이터가 클러스터 $k$에 속할 확률
- $\gamma(z_k)$는 사후 확률(posterior probability)로, 데이터 $x$가 관측된 후 그것이 클러스터 $k$에 속할 확률
- 종종 **responsibility**라고 불리며, 이는 각 가우시안 성분이 데이터 포인트를 "설명"하는 정도로 나타냄
![[Pasted image 20250208230634.png]]


## Likelihood fuction
- 데이터 집합 $N \times D$으로 나타낼 수 있는 행렬 $X$에 대해 GMM 결합 분포에 로그를 씌워  보면 아래와 같이 표현
$$\ln p(\mathbf{X} \mid \pi, \mu, \Sigma) = \sum_{n=1}^{N} \ln \left\{ \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k) \right\}$$
- 해당식의 최대값을 구하는 것은 쉽지 않은 일인데
- 이것은 로그 안의 k에 대한 합 때문임
- If we set the derivatives of the log likelihood to zero, we will no longer obtain a closed-form solution, as we will see shortly (와닫지가 않아 일단 본문째로 가저옴)
- 정해진 공식을 만들수 없음의 느낌낌
### Singularities(특이점)
- 아래와 같은 혼합 모델이 있을때 모델의 j번째 성분이 특정 데이터 포인트 $x_n$에 정확히 평균을 맞추는 경우를 생각해 보면, 이때 가능도 함수의 항은 다은과 같이 표현
$$\mathcal{N}(x_n \mid x_n, \sigma_j^2 I) = \frac{1}{(2\pi)^{D/2} \sigma_j^D}$$
- 만약에 $\sigma_j \to 0$ 으로 수렴한다면, 이 항은 무한대로 발산하게 됩니다. 이는 로그 가능도 함수가 무한대로 발산하는 결과를 초래한다
- 로그 가능도 최대화가 항상 가능한 것도 아니라는 말씀
- 이러한 Singularities를 피해야하는데, 아래와 같은 방법을 사용 할 수있음
	- Heuristic
		- 가우시안 성분이 데이터 포인트에 '붕괴(collapse)'하는 경우를 탐지하고, 해당 성분의 평균을 무작위 값으로 재설정하거나 공분산을 더 큰 값으로 초기화한 뒤 최적화를 계속 진행합니다.
	- Regularization
		- 정규화 항을 추가하는 것

### Identifiability(식별 가능성)
- 가우시안 혼합 모델에서 $K$개의 성분을 할당하는 방법에는 $K!$개의 동등한 해가 존재
	- 이는 성분의 순서를 바꾸어도 모델의 성능에는 차이가 없기 때문
	- 해석할때 중요한 이슈가 될 수도 있음

## Maximum likelihood
- 잠재 변수(latent variables)를 가진 모델에 대한 Maximum likelihood 해법을 찾는 우아하고 강력한 방법 그것이 바로 **Expectation-Maximization (EM) algorithm**
- 아래 식에서 GMM은 likelihood function가 최대값을 가짐 (미분해서 만들어 진 것으로 보임..)
$$0 = \sum_{n=1}^{N} \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}{\sum_{j} \pi_j \mathcal{N}(x_n \mid \mu_j, \Sigma_j)} \Sigma_k^{-1} (x_n - \mu_k)$$
- 여기서 $\frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}{\sum_{j} \pi_j \mathcal{N}(x_n \mid \mu_j, \Sigma_{j)}}=\gamma(z_{nk})$ (responsibilities)임을 볼수 있다
- 그래서 양변에 $\Sigma_k$ 를 곱하여 정리하면 
$$\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) x_{n}\quad \rightarrow\quad N_k = \sum_{n=1}^{N} \gamma(z_{nk})$$ 
- Nₖ는 cluster k에 할당된 데이터 포인트의 유효 개수로 해석가능하다.

- $\Sigma_k$ 는 다음과 같이 추정될 수있고 (MLE값)

$$ \Sigma_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T$$

- $\pi_k$ 는 
$$\ln p(\mathbf{X} \mid \pi, \mu, \Sigma) + \lambda \left( \sum_{k=1}^{K} \pi_k - 1 \right)$$
$$0 = \sum_{n=1}^{N} \frac{\mathcal{N}(x_n \mid \mu_k, \Sigma_k)}{\sum_{j} \pi_j \mathcal{N}(x_n \mid \mu_j, \Sigma_j)} + \lambda$$
을 통해 아래와 같이 추정 가능
$$\pi_k = \frac{N_k}{N}$$
- 즉, **πₖ**는 component k가 데이터 포인트를 설명하는 평균 responsibility로 나타낼 수 있슴
- 이러한 방법이 EM 알고리즘
	- <mark style="background: #ABF7F7A6;">E-step</mark>: 현재 파라미터를 사용하여 posterior probabilities(γ(zₙₖ))를 계산
	- <mark style="background: #ABF7F7A6;">M-step</mark>: E-step의 결과를 바탕으로 μₖ, Σₖ, πₖ를 업데이트
![[Pasted image 20250209000210.png]]![[Pasted image 20250209000234.png]]
