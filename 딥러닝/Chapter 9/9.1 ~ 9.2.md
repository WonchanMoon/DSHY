우리가 회귀분석을 할 때, 아래와 같이 L2-norm이 정규화항으로 추가된 Ridge 회귀를 수행할 수 있다.
$$\tilde{E}(w) = E(w) + \frac{\lambda}{2} w^\top w$$
이때 $E(w)$는 정규화항이 추가되기 전의 Loss function이고, $\lambda$는 정규화의 강도를 조절하는 값.

정규화를 통한 예측 정확도의 개선은, 편향-분산 트레이드오프의 관점에서 이해할 수 있다. 편향을 조금 얻는 대신 분산을 감소시키는 것이다.(살을 주고 뼈를 취한다... 멋지군요)

이 장에서는 이런 정규화에 대해서 더 깊게 탐구해봅니다.
# 9.1 Inductive Bias
편향이 높은 단순한 모델은 underfitting(과소적합)이 발생하여 일반화 성능이 떨어짐.
편향이 낮은 매우 유연한 모델은 overfitting(과대적합)이 되기 쉬워 일반화 성능이 떨어짐.
### 9.1.1 Inverse problems
대부분의 기계학습 문제는, 역문제이다.
조건부 분포 $p(t|x)$와 입력 데이터 $x$가 주어질 때, 이 분포에 대응되는 값 $t$를 추론하는 것은 매우 간단.
하지만 기계학습에서는 문제를 역으로 해결해야함. 우리가 관측한 일부 데이터(유한한 샘플)로 부터 전체 분포를 추론해야하며, 이는 본질적으로 잘못된 문제. 왜냐하면 우리가 관측한 데이터를 설명했을 가능성이 있는 분포는 무수히 많기 때문.

특정 선택을 다른 선택보다 선호하는 경향을 **inductive bias**라고 한다. 이 inductive bias는 prior knowledge라고 불리기도 하며, 기계학습에서 중요한 역할을 함. 이러한 inductive bias는 기계학습에서 필수적임.
### 9.1.2 No free lunch theorem
Deep Neural Network가 마치 모든 문제를 해결해주는 매우 유연한 모델처럼 보이지만, 이 모델 조차도 Inductive bias가 있다. 예를들어 CNN에서는 객체의 위치가 어디 있든지 간에 상관없이 객체를 인식하는 것과 같은 Inductive bias를 갖고 있다

No free lunch : 공짜 점심은 없다.
> 경제에서는 모든 일에는 대가가 있다는 의미로 사용되는데, 여기서는 다르게 사용한 듯?

No free lunch는, 모든 학습 알고리즘은 모든 가능한 문제에 대해 평균적으로 동등하다는 것. 즉, 특정 모델이나 알고리즘이 일부 문제에서 평균 이상이라면, 다른 문제에서는 평균 이하일 수 밖에 없다. 따라서 문제의 특성에 맞는 적절한 귀납적 편향을 도입해야함.

딥러닝을 실제 문제에 적용하는 기술의 핵심 중 하나는 Inductive bias를 신중히 설계하는데에 있음.,
### 9.1.3 Symmetry and invariance
기계 학습에서는, 입력 변수에 대한 하나 이상의 변환에도 불구하고 예측이 변하지 않아야하는 경우가 있다. 예를 들어, 고양이와 개 분류 문제를 풀 때, 객체의 위치가 변한다고 해서 결과가 달라지면 안된다.(평행이동 불변성, Translation Invariance). 또한 객체의 크기가 달라져도 분류가 변하면 안된다.(크기 불변성, Scale Invariance). 이러한 특징을 활용하여 Inductive bias를 형성하면, 기계학습 모델의 성능을 크게 향상 시킬 수 있다.

신경망의 일반화 성능을 높이고, 특정 변환(평행 이동, 크기 조정, 회전)에 대해 불변성을 확보하기 위해서는 다음과 같은 방법들을 고려해볼 수 있음.
#### 1. 사전 처리
모델에 데이터를 넣기 전에, 사전에 데이터를 미리 변환하는 단계.
모든 데이터에 동일한 방식으로 변환 적용(ex: 이미지의 모든 픽셀값을 0~1사이로 고정, 이미지 크기를 고정, 문장에서 불필요한 용어 제거 등)

사전에 데이터를 변환하여 필요한 불변성을 가지는 특징을 미리 계산
> 내가 잡고 싶은 변환을 미리 데이터에 적용하고, 그 데이터를 모델에 넣는 듯?
#### 2. Loss 함수 정규화 항 추가
Loss 함수에 입력 데이터가 변환 되더라도 출력 값이 변하지 않도록 패널티 항을 추가할 수 있음.
하지만 정규화 항 설계가 어려울 수 있으며, 계산 비용이 높아질 수 있다는 단점이 있음.
#### 3. 데이터 늘리기
데이터 셋을 확장하여, 변환된 데이터를 미리 추가하여 모델 적합.
특히 이미지 분석에서는 변환된 데이터를 생성하는 과정이 직관적이고 간단해서 많이 사용.
![[9.1.1.png]]
구현이 쉽고, 데이터가 많아질 수록 일반화 성능이 좋아지지만, 많은 변환 조합이 필요할 수 있다는 단점이 있음.

##### 모델 학습 과정에서의 활용 : SGD
stochastic gradient descent를 활용해서, 모델을 훈련 시키고자 한다면, 입력 데이터 포인트가 모델에 제공되기 전에 변환을 적용할 수 있다.
##### 모델 학습 과정에서의 활용 : Batch
각 데이터 포인트를 여러 번 복제하고, 각 복제본 마다 독립적으로 변환을 적용할 수 있다.
#### 4. 네트워크 구조
불변성을 신경망 구조 자체에 넣어버리는 방법. 예를 들어, CNN은 평행 이동 불변성을 자연스럽게 구현하도록 설계가 되어있음.
> 자세한건 CNN 공부하면 나온다고 합니다 ~

### 9.1.4 Equivariance
9.1.4 에서는 변환과 관련된 이야기를 하고 있습니다
#### 불변성
입력 데이터가 변환($T$) 되더라도, 신경망의 출력($C$)은 일정하게 유지 되어야함.
$$C(T(I))=C(I)$$
#### 이미지 segmentation
데이터가 변환($T$) 되기 전 후 세그멘테이션($S$)도 동일해야함.
$$S(T(I))=T(S(I))$$
![[9.1.2.png]]

# 9.2 Weight Decay : 가중치 감쇠
### 9.2.1 Consistent regularizers
$$\tilde{E}(w) = E(w) + \frac{\lambda}{2} w^\top w$$
위와 같은 단순 가중치 감쇠는, 데이터를 선형 변환($x\leftarrow ax+b$, 데이터 스케일링, 편향 이동) 했을 때 모델이 기존 데이터와 다른 솔루션을 제공할 수도 있음.
이러한 불변성을 만족시키기 위해서는, 아래와 같은 정규화 항을 고려해야한다.
$$
\Omega(\mathbf{w}) = \frac{1}{2} \sum_k \alpha_k \|\mathbf{w}\|_{k}^{2},\quad \|\mathbf{w}\|_k^2 = \sum_{j \in \mathcal{W}_k} w_j^2
$$

이때 $W_{k}$는 $k$번째 가중치 그룹.
>가중치 그룹?
>사용자가 임의로 설정하는 가중치 그룹이다. 예를 들어 MLP에서는 각 층 단위로 가중치를 그룹화 할 수도 있고, CNN에서는 각 층의 필터 가중치와 편향을 그룹화 할 수도 있다. 이때 각 층 별로 역할과 중요도가 다르기 때문에, 층 별 중요도 강도인 $\alpha_{k}$를 통해 정규화의 강도를 조절한다. 
>이 외에도 편항만을 모아서 가중치 그룹으로 설정하거나 하는 등, 사용자의 목적과 의도에 맞게 가중치 그룹을 형성 할 수 있다.
### 9.2.2 Generalized weight decay
위에서 언급한 일반화된 정규화 항을, 다음과 같이 쓰기도 함.
$$
\Omega(\mathbf{w}) = \frac{\lambda}{2} \sum_{j=1}^M |w_j|^q
$$
이때 $q=2$ 이면, 책에서는 단순한 이차 정규화 항 이라고 하던데, 우리가 아는 Ridge가 되겠죠?
$q=1$이면, Lasso, 그보다 더 작으면 더 강한 가중치 축소를 유도하게 된다.
![[9.2.1.png]]

이와 같은 가중치 항이 추가된 Loss function은 아래와 같이 정의된다.
$$
E(\mathbf{w}) + \frac{\lambda}{2} \sum_{j=1}^M |w_j|^q
$$
이는 $E(\mathbf{w})$를 Loss로 가지고 제약 조건을 갖는 아래 문제와 같은 것으로 볼 수 있다. 
$$
\min E(\mathbf{w})\quad \text{subject to }\sum_{j=1}^M |w_j|^q \leq \eta
$$
아래 그림을 통해, Lasso(왼쪽 그림)와 Ridge(오른쪽 그림)의 차이를 볼 수 있다.
![[9.2.2.png]]