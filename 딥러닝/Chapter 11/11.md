Structured Distributions (구조화된 확률 분포)
**확률은 딥러닝의 가장 중요한 기초 개념 중 하나**
ex. 이진분류에 사용되는 신경망은 다음과 같은 조건부 확률 분포로 설명 가능 ($y$는 신경망 함수)
$$
p(t \mid \mathbf{x}, \mathbf{w}) = y(\mathbf{x}, \mathbf{w})^t \left\{ 1 - y(\mathbf{x}, \mathbf{w}) \right\}^{(1-t)}
$$

LLM, VAE, Diffusion Models와 같은 복잡한 딥러닝 모델들을 설명하고 활용하기 위해 **확률 그래프 모델(probabilistic graphical models)** 을 도입함
# 11.1 Graphical Models
**합의 법칙(sum rule)** 과 **곱의 법칙(product rule)** 으로 알려진 간단한 두가지 방정식으로 확률론이 표현될 수 있음 (아무리 복잡한 확률 계산이라도)

확률분포를 다이어그램으로 표현하면 다음의 효과를 얻을 수 있음 (다이어그램 : 2차원 기하학 모델로 정보를 시각화한 것)
1. 모델 구조의 시각화
2. 모델의 특성 파악 (조건부 독립 같은 특성들)
3. 복잡한 계산을 그래픽적으로 표현

이러한 그래프 모델은 신경망 다이어그램과 비슷하게 노드와 엣지를 가지지만, 그래프는 **확률적 의미**를 가지며 **더 풍부한 의미론적 구조**를 가짐
>신경망은 입력과 출력에 집중하지만, 그래프 모델은 입력이 어떻게 생성되었는지, 출력이 입력과 어떤 확률적 관계를 가지는 지 모델링함
### Directed Graphs
확률 그래프 모델(Probabilistic Graphical Model)에서,
- 노드는 확률 변수(Random Variable)
- 엣지는 변수들 사이의 확률적 관계

방향 그래프 모델 (Directed Graphical Models)
- **화살표로 방향이 표시된 간선**을 가짐
- 확률 변수들 간의 **인과 관계(Causal Relationships)** 를 표현하는 데 유용
- **베이즈 네트워크(Bayesian Networks)** 또는 **Bayes Nets**로 불림

비방향 그래프 모델 (Undirected Graphical Models)
- **간선에 방향이 없음**
- 확률 변수들 간의 **부드러운 제약 조건(Soft Constraints)** 을 표현 (서로 연관은 있지만 특정 방향성이 없음)
- **마르코프 랜덤 필드(Markov Random Fields)** 또는 **비방향 그래프 모델**로 불림

**방향 그래프**와 **비방향 그래프**는 모두 **요소 그래프(Factor Graphs)** 의 특수한 사례임

**비확률적 해석**을 가지는 비방향 그래프도 등장하는데, 이는 **그래프 신경망(Graph Neural Networks)** 과 관련이 있음 (노드가 확률 변수가 아니라 결정적 변수(deterministic variables))
>**결정적 변수**는 특정 입력과 규칙에 따라 값이 항상 고정적으로 결정되는 변수
### Factorization
임의의 결합분포 $p(a,b,c)$를 고려
1. 확률의 곱셈 법칙(Product Rule)
$$p(a,b,c)=p(c|a,b)p(a,b)$$
2. 추가적인 분해
$$p(a,b,c)=p(c|a,b)p(b|a)p(a)$$
해당 분해는 어떠한 결합 분포에 대해서도 성립함

3. 그래프 모델로 표현
![[11.1.1.png]]
이때 $a$는 $b$의 부모이며 $b$는 $a$의 자식이라 할 수 있음
해당 분해를 하면서 **$a,b,c$의 순서를 암묵적으로 선택**했으며 다른 순서였으면 다른 그래프가 생성되었음

확률의 곱셈 법칙을 반복적으로 적용하면 아래와 같이도 표현 가능함
$$
p(x_1, \ldots, x_K) = p(x_K \mid x_1, \ldots, x_{K-1}) \cdots p(x_2 \mid x_1)p(x_1)
$$
이렇게 모든 노드 쌍 사이에 연결이 존재하면 이를 **완전 연결 그래프(fully connected graph)** 라고 부름

- 그래프 -> 확률 표현 예시
![[11.1.2.png]]
$$
p(x_1)p(x_2)p(x_3)p(x_4 \mid x_1, x_2, x_3)p(x_5 \mid x_1, x_3)p(x_6 \mid x_4)p(x_7 \mid x_4, x_5)
$$

어떤 방향 그래프가 주어졌을 때, **모든 노드에 대해 다음의 수식이 만족함**을 알 수 있음 ($pa(k)$는 $x_k$의 부모 노드에 해당하는 변수들의 집합을 의미)
$$
p(x_1, \ldots, x_K) = \prod_{k=1}^K p(x_k \mid \mathrm{pa}(k))
$$

앞으로 다룰 방향 그래프들은 **어떤 노드에서 화살표를 따라 이동해 다시 그 노드로 돌아오는 경로**가 있으면 안됨 (사이클이 없어야 됨)
이러한 방향 그래프를 **방향 비순환 그래프(Directed Acyclic Graph, DAG)** 라고 함
>사이클이 존재하면 의존 관계가 불명확해짐
>따라서 위의 수식도 성립하지 않음

지수족(exponential family)에 속하는 다양한 확률 분포들을 많이 알아봤었음
그 중에서도 **이산 변수와 가우시안 변수는 많이 사용**되므로 이에 대해 알아봄
### Discrete variables
**단일 이산 변수** (1-of-K 표현(원 핫 벡터) : $\mathbf{x}$가 $K$개의 상태를 가질 수 있음)
- $\mathbf{X} = k$일 확률을 한 번에 표현한 것
$$
p(\mathbf{x} \mid \boldsymbol{\mu}) = \prod_{k=1}^{K} \mu_k^{x_k}
$$
- 이때 $\boldsymbol{\mu}$는 확률이므로, $\sum_k \mu_k = 1$ 조건을 가지므로 $K-1$개의 파라미터만 정해지면 됨

**두 개의 이산 변수** ($\mathbf{x}_1, \mathbf{x}_2$)
$$
p(\mathbf{x}_1, \mathbf{x}_2 \mid \boldsymbol{\mu}) = \prod_{k=1}^{K} \prod_{l=1}^{K} \mu_{kl}^{x_{1k} x_{2l}}
$$
- $\sum_k \sum_l \mu_{kl} = 1$ 이므로, $K^2-1$개의 파라미터가 필요함
- 만약 $\mathbf{x}_1, \mathbf{x}_2$이 서로 독립이라면 $2(K-1)$개의 파라미터가 필요함 (선형적)
![[11.1.3.png]]
- 간선을 제거하면(독립이라 가정하면) 파라미터가 크게 줄지만 그만큼 표현력이 제한됨
- 따라서 그래프 구조를 어떻게 설정하느냐에 따라 **파라미터의 수와 표현력을 조절**할 수 있음
>파라미터 수
>완전 연결 그래프 : $K^M-1$
>모두 독립인 그래프 : $M(K-1)$

**중간 수준의 연결**을 가진 그래프 (체인 형태)
![[11.1.4.png]]
- 결합 분포 : $p(x_1, x_2, \ldots, x_M) = p(x_1) \prod_{i=2}^M p(x_i \mid x_{i-1})$
- 파라미터 수 : $p(x_1) : K-1$, 나머지는 $K(K-1)$개씩 $(M-1)$개 존재하므로, 총 $K-1+(M-1)K(K-1)$개
- 완전 연결보다 훨씬 적은 파라미터로 독립 가정보다 풍부한 분포를 표현 가능
![[11.1.5.png]]
- 위와 같이 모두 동일한 파라미터를 가지면 체인이 길어져도 파라미터 수는 $K^2-1$로 고정됨 ($M$에 관계없음)
- 하지만 모든 단계에서 조건부 분포가 동일하다는 가정은 매우 강력하므로 표현력이 제한됨

파라미터의 수를 줄이기 위한 다른 방법으론 **조건부 확률을 특정함수로 매개변수화**시키면 됨
- ex. 로지스틱 시그모이드
![[11.1.6.png]]
- 부모 노드가 모두 이진 변수고 자식도 이진 변수이면 부모의 조합마다 확률을 지정하므로 $2^M$개의 파라미터 필요
- 대신에 아래와 같은 로지스틱 회귀 (시그모이드 함수)의 형태로 두면 파라미터의 수가 선형적으로 증가함 ($M+1$개)
$$
p(y = 1 \mid x_1, \ldots, x_M) = \sigma \left( w_0 + \sum_{i=1}^M w_i x_i \right) = \sigma (\mathbf{w}^\top \mathbf{x})
$$
>그래프의 구조 설정, 파라미터 공유, 함수 기반 파라미터화를 사용해 필요 파라미터의 수를 줄이고 충분한 표현력을 확보할 수 있게 됨
>어떠한 가정을 추가해 모델의 파라미터 수(복잡도)를 줄인다고 생각하면 됨
### Gaussian variables
노드가 가우시안 확률 변수로 이루어진 그래프에 대해 다룸
각 노드들은 부모 노드에 선형적으로 의존하고 결합 분포가 다변수 가우시안 형태가 되어 **linear-Gaussian models**로 불림

$$
p(x_i \mid \text{pa}(i)) = \mathcal{N} \left( x_i \mid \sum_{j \in \text{pa}(i)} w_{ij} x_j + b_i, v_i \right)
$$
- $w_{ij}$와 $b_i$는 평균을 결정하는 파라미터, $v_i$는 분산을 의미

- 모든 노드의 조건부 분포를 곱한게 전체 결합 분포이며, 이는 다변수 가우시안 분포가 됨
$$
\ln p(\mathbf{x}) = \sum_{i=1}^D \ln p(x_i \mid \text{pa}(i)) \\
= -\sum_{i=1}^D \frac{1}{2v_i} \left( x_i - \sum_{j \in \text{pa}(i)} w_{ij} x_j - b_i \right)^2 + \text{const}
$$

- 평균 구하기 : $\mathbb{E}[x_i] = \sum_{j \in \text{pa}(i)} w_{ij} \mathbb{E}[x_j] + b_i$가 성립하므로 재귀적으로 구하면 됨
- 분산 구하기 : $\text{cov}[x_i, x_j] = \sum_{k \in \text{pa}(j)} w_{jk} \text{cov}[x_i, x_k] + I_{ij} v_j$가 성립하므로 재귀적으로 구하면 됨

>파라미터 수
>모두 독립인 그래프 : 각 노드의 평균과 분산 $2D$
>완전 연결 그래프(가장 일반적) : 공분산 행렬 $D(D+1)/2$ + 평균 $D$

**중간 수준의 연결**을 가진 그래프 (체인 형태)
![[11.1.7.png]]
- **공분산 행렬에 제약** ($x_1$과 $x_2$ 사이에 간선이 없음)
- 따라서 다음과 같은 평균과 공분산이 계산됨
$$
\boldsymbol{\mu} = (b_1, b_2 + w_{21} b_1, b_3 + w_{32} b_2 + w_{32} w_{21} b_1)^\top
$$
$$
\Sigma = \begin{pmatrix}
v_1 & w_{21} v_1 & w_{32} w_{21} v_1 \\
w_{21} v_1 & v_2 + w_{21}^2 v_1 & w_{32} (v_2 + w_{21}^2 v_1) \\
w_{32} w_{21} v_1 & w_{32} (v_2 + w_{21}^2 v_1) & v_3 + w_{32}^2 (v_2 + w_{21}^2 v_1)
\end{pmatrix}
$$

노드를 스칼라 변수가 아니라 벡터로 생각하면 다음과 같이 표현됨
$$
p(\mathbf{x}_i \mid \text{pa}(i)) = \mathcal{N} \left( \mathbf{x}_i \mid \sum_{j \in \text{pa}(i)} \mathbf{W}_{ij} \mathbf{x}_j + \mathbf{b}_i, \Sigma_i \right)
$$
- 이때도 **결합분포가 결과적으로 다변량 가우시안**임
### Binary classifier
이진 분류 모델을 방향 그래프로 표현
- 데이터 : $\{(x_n, t_n)\}_{n=1}^N$ , $x_n$은 입력 벡터, $t_n$은 타겟 (0 or 1)
- 파라미터 : $\mathbf{w}$
- 사후 확률 분포 : $p(t \mid x, \mathbf{w}) = y(x, \mathbf{w})^t (1 - y(x, \mathbf{w}))^{(1-t)}$
- 사전 확률 분포 : $p(\mathbf{w} \mid \lambda) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \lambda \mathbf{I})$
- 전체 확률 모형
$$
p(\mathbf{t}, \mathbf{w} \mid \mathbf{X}, \lambda) = p(\mathbf{w} \mid \lambda) \prod_{n=1}^N p(t_n \mid \mathbf{w}, x_n)
$$

- 그래프 모델 표현
![[11.1.8.png]]
- 플레이트(plate) 표기
![[11.1.9.png]]
	박스 내부가 N번 반복된다는 의미
### Parameters and observations
%%그래프 모델에서 확률 변수와 결정론적 파라미터를 어떻게 표현하는지, 관측 변수와 잠재변수가 어떻게 표현되는지에 대한 내용%%

앞선 사진 11.9에서 결정론적 파라미터를 추가하면 아래와 같이 표현됨
![[11.1.10.png]]
- 확률 변수는 원으로 둘러싸고, 결정론적 파라미터는 떠 있는 변수로 표현

관측 변수(Observed Variable)와 잠재 변수(Latent Variable)
![[11.1.11.png]]
- 관측 변수 : 데이터로 값이 주어진 변수로, 색칠해서 표현
- 잠재 변수 : 모델에 존재하지만 관측되지 않은 변수로, 비어있는 원으로 표현

새로운 입력 $\hat{x}$에 대한 예측
$$
p(\hat{t}, \mathbf{t}, \mathbf{w} \mid \hat{\mathbf{x}}, \mathbf{X}, \lambda) = p(\mathbf{w} \mid \lambda) p(\hat{t} \mid \mathbf{w}, \hat{\mathbf{x}}) \prod_{n=1}^N p(t_n \mid \mathbf{w}, \mathbf{x}_n)
$$
![[11.1.13.png]]
- 새로운 입력이 주어지면 모델 파라미터인 $\mathbf{w}$가 확률적 변수이지만 계산 효율성을 위해 MAP로 계산 (베이지안 관점에선 그냥 적분하면 되긴함)

### Bayes' theorem
%%확률적 모델에서 일부 확률 변수에 관측값이 주어졌을 때, 나머지 잠재 확률 변수들의 분포가 어떻게 달라지는지를 설명하는 내용. 이를 추론이라하며 베이즈 정리로 설명%%

어떤 확률 모델에 포함된 확률 변수들 중 일부를 실제 관측값에 맞게 고정하면 다른 잠재 변수들의 분포가 바뀜
이를 **업데이트하는 과정을 추론(inference)** 라고 함

이는 베이즈 정리로 해석할 수 있음
![[11.1.12.png]]
- $p(x,y) = p(x)p(y \mid x)$일 때 (a)로 표현 가능
- 이때 $y$가 주어지면 (b)로 표현되며 $p(x)$는 사전 분포가 되고 사후 분포인 $p(y \mid x)$를 구해야함
- 베이즈 정리를 사용하면 (c)의 형태로 바꿀 수 있음
$$
p(x \mid y) = \frac{p(y \mid x)p(x)}{p(y)}
$$

복잡한 그래프에서의 추론은 결국 **합의 법칙, 곱의 법칙, 베이즈 정리를 반복적으로 활용**하는게 전부임

계산 효율성을 위해 알고리즘이 존재하지만 해당 책에선 다루지 않음 (Bishop(2006) 참고)
# 11.2 Conditional Independence
# 11.3 Sequence Models