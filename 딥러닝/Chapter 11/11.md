Structured Distributions (구조화된 확률 분포)
**확률은 딥러닝의 가장 중요한 기초 개념 중 하나**
ex. 이진분류에 사용되는 신경망은 다음과 같은 조건부 확률 분포로 설명 가능 ($y$는 신경망 함수)
$$
p(t \mid \mathbf{x}, \mathbf{w}) = y(\mathbf{x}, \mathbf{w})^t \left\{ 1 - y(\mathbf{x}, \mathbf{w}) \right\}^{(1-t)}
$$

LLM, VAE, Diffusion Models와 같은 복잡한 딥러닝 모델들을 설명하고 활용하기 위해 **확률 그래프 모델(probabilistic graphical models)** 을 도입함
# 11.1 Graphical Models
**합의 법칙(sum rule)** 과 **곱의 법칙(product rule)** 으로 알려진 간단한 두가지 방정식으로 확률론이 표현될 수 있음 (아무리 복잡한 확률 계산이라도)

확률분포를 다이어그램으로 표현하면 다음의 효과를 얻을 수 있음 (다이어그램 : 2차원 기하학 모델로 정보를 시각화한 것)
1. 모델 구조의 시각화
2. 모델의 특성 파악 (조건부 독립 같은 특성들)
3. 복잡한 계산을 그래픽적으로 표현

이러한 그래프 모델은 신경망 다이어그램과 비슷하게 노드와 엣지를 가지지만, 그래프는 **확률적 의미**를 가지며 **더 풍부한 의미론적 구조**를 가짐
>신경망은 입력과 출력에 집중하지만, 그래프 모델은 입력이 어떻게 생성되었는지, 출력이 입력과 어떤 확률적 관계를 가지는 지 모델링함
### Directed Graphs
확률 그래프 모델(Probabilistic Graphical Model)에서,
- 노드는 확률 변수(Random Variable)
- 엣지는 변수들 사이의 확률적 관계

방향 그래프 모델 (Directed Graphical Models)
- **화살표로 방향이 표시된 간선**을 가짐
- 확률 변수들 간의 **인과 관계(Causal Relationships)** 를 표현하는 데 유용
- **베이즈 네트워크(Bayesian Networks)** 또는 **Bayes Nets**로 불림

비방향 그래프 모델 (Undirected Graphical Models)
- **간선에 방향이 없음**
- 확률 변수들 간의 **부드러운 제약 조건(Soft Constraints)** 을 표현 (서로 연관은 있지만 특정 방향성이 없음)
- **마르코프 랜덤 필드(Markov Random Fields)** 또는 **비방향 그래프 모델**로 불림

**방향 그래프**와 **비방향 그래프**는 모두 **요소 그래프(Factor Graphs)** 의 특수한 사례임

**비확률적 해석**을 가지는 비방향 그래프도 등장하는데, 이는 **그래프 신경망(Graph Neural Networks)** 과 관련이 있음 (노드가 확률 변수가 아니라 결정적 변수(deterministic variables))
>**결정적 변수**는 특정 입력과 규칙에 따라 값이 항상 고정적으로 결정되는 변수
### Factorization
임의의 결합분포 $p(a,b,c)$를 고려
1. 확률의 곱셈 법칙(Product Rule)
$$p(a,b,c)=p(c|a,b)p(a,b)$$
2. 추가적인 분해
$$p(a,b,c)=p(c|a,b)p(b|a)p(a)$$
해당 분해는 어떠한 결합 분포에 대해서도 성립함

3. 그래프 모델로 표현
![[11.1.1.png]]
이때 $a$는 $b$의 부모이며 $b$는 $a$의 자식이라 할 수 있음
해당 분해를 하면서 **$a,b,c$의 순서를 암묵적으로 선택**했으며 다른 순서였으면 다른 그래프가 생성되었음

확률의 곱셈 법칙을 반복적으로 적용하면 아래와 같이도 표현 가능함
$$
p(x_1, \ldots, x_K) = p(x_K \mid x_1, \ldots, x_{K-1}) \cdots p(x_2 \mid x_1)p(x_1)
$$
이렇게 모든 노드 쌍 사이에 연결이 존재하면 이를 **완전 연결 그래프(fully connected graph)** 라고 부름

- 그래프 -> 확률 표현 예시
![[11.1.2.png]]
$$
p(x_1)p(x_2)p(x_3)p(x_4 \mid x_1, x_2, x_3)p(x_5 \mid x_1, x_3)p(x_6 \mid x_4)p(x_7 \mid x_4, x_5)
$$

어떤 방향 그래프가 주어졌을 때, **모든 노드에 대해 다음의 수식이 만족함**을 알 수 있음 ($pa(k)$는 $x_k$의 부모 노드에 해당하는 변수들의 집합을 의미)
$$
p(x_1, \ldots, x_K) = \prod_{k=1}^K p(x_k \mid \mathrm{pa}(k))
$$

앞으로 다룰 방향 그래프들은 **어떤 노드에서 화살표를 따라 이동해 다시 그 노드로 돌아오는 경로**가 있으면 안됨 (사이클이 없어야 됨)
이러한 방향 그래프를 **방향 비순환 그래프(Directed Acyclic Graph, DAG)** 라고 함
>사이클이 존재하면 의존 관계가 불명확해짐
>따라서 위의 수식도 성립하지 않음

지수족(exponential family)에 속하는 다양한 확률 분포들을 많이 알아봤었음
그 중에서도 **이산 변수와 가우시안 변수는 많이 사용**되므로 이에 대해 알아봄
### Discrete variables
**단일 이산 변수** (1-of-K 표현(원 핫 벡터) : $\mathbf{x}$가 $K$개의 상태를 가질 수 있음)
- $\mathbf{X} = k$일 확률을 한 번에 표현한 것
$$
p(\mathbf{x} \mid \boldsymbol{\mu}) = \prod_{k=1}^{K} \mu_k^{x_k}
$$
- 이때 $\boldsymbol{\mu}$는 확률이므로, $\sum_k \mu_k = 1$ 조건을 가지므로 $K-1$개의 파라미터만 정해지면 됨

**두 개의 이산 변수** ($\mathbf{x}_1, \mathbf{x}_2$)
$$
p(\mathbf{x}_1, \mathbf{x}_2 \mid \boldsymbol{\mu}) = \prod_{k=1}^{K} \prod_{l=1}^{K} \mu_{kl}^{x_{1k} x_{2l}}
$$
- $\sum_k \sum_l \mu_{kl} = 1$ 이므로, $K^2-1$개의 파라미터가 필요함
- 만약 $\mathbf{x}_1, \mathbf{x}_2$이 서로 독립이라면 $2(K-1)$개의 파라미터가 필요함 (선형적)
![[11.1.3.png]]
- 간선을 제거하면(독립이라 가정하면) 파라미터가 크게 줄지만 그만큼 표현력이 제한됨
- 따라서 그래프 구조를 어떻게 설정하느냐에 따라 **파라미터의 수와 표현력을 조절**할 수 있음
>파라미터 수
>완전 연결 그래프 : $K^M-1$
>모두 독립인 그래프 : $M(K-1)$

**중간 수준의 연결**을 가진 그래프 (체인 형태)
![[11.1.4.png]]
- 결합 분포 : $p(x_1, x_2, \ldots, x_M) = p(x_1) \prod_{i=2}^M p(x_i \mid x_{i-1})$
- 파라미터 수 : $p(x_1) : K-1$, 나머지는 $K(K-1)$개씩 $(M-1)$개 존재하므로, 총 $K-1+(M-1)K(K-1)$개
- 완전 연결보다 훨씬 적은 파라미터로 독립 가정보다 풍부한 분포를 표현 가능
![[11.1.5.png]]
- 위와 같이 모두 동일한 파라미터를 가지면 체인이 길어져도 파라미터 수는 $K^2-1$로 고정됨 ($M$에 관계없음)
- 하지만 모든 단계에서 조건부 분포가 동일하다는 가정은 매우 강력하므로 표현력이 제한됨

파라미터의 수를 줄이기 위한 다른 방법으론 **조건부 확률을 특정함수로 매개변수화**시키면 됨
- ex. 로지스틱 시그모이드
![[11.1.6.png]]
- 부모 노드가 모두 이진 변수고 자식도 이진 변수이면 부모의 조합마다 확률을 지정하므로 $2^M$개의 파라미터 필요
- 대신에 아래와 같은 로지스틱 회귀 (시그모이드 함수)의 형태로 두면 파라미터의 수가 선형적으로 증가함 ($M+1$개)
$$
p(y = 1 \mid x_1, \ldots, x_M) = \sigma \left( w_0 + \sum_{i=1}^M w_i x_i \right) = \sigma (\mathbf{w}^\top \mathbf{x})
$$
>그래프의 구조 설정, 파라미터 공유, 함수 기반 파라미터화를 사용해 필요 파라미터의 수를 줄이고 충분한 표현력을 확보할 수 있게 됨
>어떠한 가정을 추가해 모델의 파라미터 수(복잡도)를 줄인다고 생각하면 됨
### Gaussian variables
노드가 가우시안 확률 변수로 이루어진 그래프에 대해 다룸
각 노드들은 부모 노드에 선형적으로 의존하고 결합 분포가 다변수 가우시안 형태가 되어 **linear-Gaussian models**로 불림

$$
p(x_i \mid \text{pa}(i)) = \mathcal{N} \left( x_i \mid \sum_{j \in \text{pa}(i)} w_{ij} x_j + b_i, v_i \right)
$$
- $w_{ij}$와 $b_i$는 평균을 결정하는 파라미터, $v_i$는 분산을 의미

- 모든 노드의 조건부 분포를 곱한게 전체 결합 분포이며, 이는 다변수 가우시안 분포가 됨
$$
\ln p(\mathbf{x}) = \sum_{i=1}^D \ln p(x_i \mid \text{pa}(i)) \\
= -\sum_{i=1}^D \frac{1}{2v_i} \left( x_i - \sum_{j \in \text{pa}(i)} w_{ij} x_j - b_i \right)^2 + \text{const}
$$

- 평균 구하기 : $\mathbb{E}[x_i] = \sum_{j \in \text{pa}(i)} w_{ij} \mathbb{E}[x_j] + b_i$가 성립하므로 재귀적으로 구하면 됨
- 분산 구하기 : $\text{cov}[x_i, x_j] = \sum_{k \in \text{pa}(j)} w_{jk} \text{cov}[x_i, x_k] + I_{ij} v_j$가 성립하므로 재귀적으로 구하면 됨

>파라미터 수
>모두 독립인 그래프 : 각 노드의 평균과 분산 $2D$
>완전 연결 그래프(가장 일반적) : 공분산 행렬 $D(D+1)/2$ + 평균 $D$

**중간 수준의 연결**을 가진 그래프 (체인 형태)
![[11.1.7.png]]
- **공분산 행렬에 제약** ($x_1$과 $x_2$ 사이에 간선이 없음)
- 따라서 다음과 같은 평균과 공분산이 계산됨
$$
\boldsymbol{\mu} = (b_1, b_2 + w_{21} b_1, b_3 + w_{32} b_2 + w_{32} w_{21} b_1)^\top
$$
$$
\Sigma = \begin{pmatrix}
v_1 & w_{21} v_1 & w_{32} w_{21} v_1 \\
w_{21} v_1 & v_2 + w_{21}^2 v_1 & w_{32} (v_2 + w_{21}^2 v_1) \\
w_{32} w_{21} v_1 & w_{32} (v_2 + w_{21}^2 v_1) & v_3 + w_{32}^2 (v_2 + w_{21}^2 v_1)
\end{pmatrix}
$$

노드를 스칼라 변수가 아니라 벡터로 생각하면 다음과 같이 표현됨
$$
p(\mathbf{x}_i \mid \text{pa}(i)) = \mathcal{N} \left( \mathbf{x}_i \mid \sum_{j \in \text{pa}(i)} \mathbf{W}_{ij} \mathbf{x}_j + \mathbf{b}_i, \Sigma_i \right)
$$
- 이때도 **결합분포가 결과적으로 다변량 가우시안**임
### Binary classifier
이진 분류 모델을 방향 그래프로 표현
- 데이터 : $\{(x_n, t_n)\}_{n=1}^N$ , $x_n$은 입력 벡터, $t_n$은 타겟 (0 or 1)
- 파라미터 : $\mathbf{w}$
- 사후 확률 분포 : $p(t \mid x, \mathbf{w}) = y(x, \mathbf{w})^t (1 - y(x, \mathbf{w}))^{(1-t)}$
- 사전 확률 분포 : $p(\mathbf{w} \mid \lambda) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \lambda \mathbf{I})$
- 전체 확률 모형
$$
p(\mathbf{t}, \mathbf{w} \mid \mathbf{X}, \lambda) = p(\mathbf{w} \mid \lambda) \prod_{n=1}^N p(t_n \mid \mathbf{w}, x_n)
$$

- 그래프 모델 표현
![[11.1.8.png]]
- 플레이트(plate) 표기
![[11.1.9.png]]
	박스 내부가 N번 반복된다는 의미
### Parameters and observations
%%그래프 모델에서 확률 변수와 결정론적 파라미터를 어떻게 표현하는지, 관측 변수와 잠재변수가 어떻게 표현되는지에 대한 내용%%

앞선 사진 11.9에서 결정론적 파라미터를 추가하면 아래와 같이 표현됨
![[11.1.10.png]]
- 확률 변수는 원으로 둘러싸고, 결정론적 파라미터는 떠 있는 변수로 표현

관측 변수(Observed Variable)와 잠재 변수(Latent Variable)
![[11.1.11.png]]
- 관측 변수 : 데이터로 값이 주어진 변수로, 색칠해서 표현
- 잠재 변수 : 모델에 존재하지만 관측되지 않은 변수로, 비어있는 원으로 표현

새로운 입력 $\hat{x}$에 대한 예측
$$
p(\hat{t}, \mathbf{t}, \mathbf{w} \mid \hat{\mathbf{x}}, \mathbf{X}, \lambda) = p(\mathbf{w} \mid \lambda) p(\hat{t} \mid \mathbf{w}, \hat{\mathbf{x}}) \prod_{n=1}^N p(t_n \mid \mathbf{w}, \mathbf{x}_n)
$$
![[11.1.13.png]]
- 새로운 입력이 주어지면 모델 파라미터인 $\mathbf{w}$가 확률적 변수이지만 계산 효율성을 위해 MAP로 계산 (베이지안 관점에선 그냥 적분하면 되긴함)

### Bayes' theorem
%%확률적 모델에서 일부 확률 변수에 관측값이 주어졌을 때, 나머지 잠재 확률 변수들의 분포가 어떻게 달라지는지를 설명하는 내용. 이를 추론이라하며 베이즈 정리로 설명%%

어떤 확률 모델에 포함된 확률 변수들 중 일부를 실제 관측값에 맞게 고정하면 다른 잠재 변수들의 분포가 바뀜
이를 **업데이트하는 과정을 추론(inference)** 라고 함

이는 베이즈 정리로 해석할 수 있음
![[11.1.12.png]]
- $p(x,y) = p(x)p(y \mid x)$일 때 (a)로 표현 가능
- 이때 $y$가 주어지면 (b)로 표현되며 $p(x)$는 사전 분포가 되고 사후 분포인 $p(y \mid x)$를 구해야함
- 베이즈 정리를 사용하면 (c)의 형태로 바꿀 수 있음
$$
p(x \mid y) = \frac{p(y \mid x)p(x)}{p(y)}
$$

복잡한 그래프에서의 추론은 결국 **합의 법칙, 곱의 법칙, 베이즈 정리를 반복적으로 활용**하는게 전부임

계산 효율성을 위해 알고리즘이 존재하지만 해당 책에선 다루지 않음 (Bishop(2006) 참고)
# 11.2 Conditional Independence
조건부 독립으로, 확률 분포에서 중요한 개념임
**여러 변수간의 의존관계를 단순화**할 수 있음
- ex. $p(a \mid b, c) = p(a \mid c)$ : $a$는 $c$가 주어졌을 때 $b$와 조건부 독립
  따라서 다음이 성립함
$$
\begin{align*}
p(a, b \mid c) &= p(a \mid b, c)p(b \mid c) \\
               &= p(a \mid c)p(b \mid c)
\end{align*}
$$
이는 **다음과 같은 표기법**으로 간단히 표현 가능 ($c$가 주어졌을 때 $a$와 $b$는 조건부 독립)
$$
a \perp\!\!\!\perp b \mid c
$$
조건부 독립은 머신러닝에서 확률 모델을 설계할 때에도 유용함
	1. 모델 구조 단순화 
	2. 계산 효율성 증가
그래프 모델의 경우 **조건부 독립을 그래프에서 직접 확인 가능** (계산 없이)
이를 확인하는 일반적인 프레임워크를 **d-separation**이라고 함
### 11.2.1 Three example graphs
방향성 그래프에서 조건부 독립 속성을 이해하기 위해 노드가 3개인 간단한 예제들을 볼 예정
#### 예제 1 :  $p(a, b, c) = p(a \mid c)p(b \mid c)p(c)$
![[11.2.1.png]]
모든 변수가 관측되지 않은 경우 : **$a$와 $b$는 독립이 아님**
	$a$와 $b$가 독립인지 확인하려면 $c$를 제거(marginalize)해야함
	$p(a, b) = \sum_c p(a \mid c)p(b \mid c)p(c)$
	일반적으로 $p(a,b)$는 $p(a)p(b)$가 아니므로 독립이 아님 ($a \not\perp\!\!\!\perp b \mid \emptyset$)

![[11.2.2.png]]
변수 $c$가 주어진 경우 : **$a$와 $b$는 독립임**
	$\begin{align*} p(a, b \mid c) &= \frac{p(a, b, c)}{p(c)} \\ &= p(a \mid c)p(b \mid c) \end{align*}$
	즉, $c$가 주어진 경우 $a$와 $b$가 조건부 독립임 ($a \perp\!\!\!\perp b \mid c$)
- 그래프적 해석 ($c$가 tail-to-tail 노드)
  $c$를 관측하면 $a$와 $b$를 연결하는 경로가 차단되므로 조건부 독립이 성립함
#### 예제 2 : $p(a, b, c) = p(a)p(c \mid a)p(b \mid c)$
![[11.2.3.png]]
모든 변수가 관측되지 않은 경우 : **$a$와 $b$는 독립이 아님**
	marginalize $c$ : $p(a, b) = p(a) \sum_c p(c \mid a)p(b \mid c)$
	일반적으로 $p(a,b)$는 $p(a)p(b)$가 아니므로 독립이 아님 ($a \not\perp\!\!\!\perp b \mid \emptyset$)
![[11.2.4.png]]
변수 $c$가 주어진 경우 : **$a$와 $b$는 독립임**
	$\begin{align*}p(a, b \mid c) &= \frac{p(a, b, c)}{p(c)} \\&=\frac{p(a)p(c \mid a)p(b \mid c)}{p(c)} \\&= p(a \mid c)p(b \mid c)\end{align*}$
	즉, $c$가 주어진 경우 $a$와 $b$가 조건부 독립임 ($a \perp\!\!\!\perp b \mid c$)
- 그래프적 해석 ($c$가 head-to-tail 노드)
  $c$를 관측하면 $c$가 경로를 차단하므로 조건부 독립이 성립함.
#### 예제 3 : $p(a, b, c) = p(a)p(b)p(c \mid a, b)$
![[11.2.5.png]]
모든 변수가 관측되지 않은 경우 : **$a$와 $b$는 독립임**
	marginalize $c$ : $p(a, b) = \sum_c p(a)p(b)p(c \mid a, b)$
	$p(a,b)=p(a)p(b)$를 만족하게 되므로 조건부 독립임 ($a \perp\!\!\!\perp b \mid c$)
![[11.2.6.png]]
변수 $c$가 주어진 경우 : **$a$와 $b$는 독립이 아님**
	$\begin{align*}p(a, b \mid c) &= \frac{p(a, b, c)}{p(c)} \\&= \frac{p(a)p(b)p(c \mid a, b)}{p(c)}\end{align*}$
	이는 일반적으로 $p(a \mid c)p(b\mid c)$로 인수분해되지 않으므로 조건부 독립이 아님 ($a \not\perp\!\!\!\perp b \mid c$)
- 그래프적 해석 ($c$가 head-to-head 노드)
  $c$가 관측되지 않은 경우 경로를 차단하여 $a$와 $b$를 독립적으로 만듦
### 11.2.2 Explaining away
예제 3을 제대로 이해하기 위해 있는 장
![[11.2.7.png]]
$B$ : 배터리의 상태 (0 or 1)
$F$ : 연료탱크의 상태 (0 or 1)
$G$ : 전기연료 게이지의 상태 (0 or 1)
- 사전 확률 : $\begin{align*}p(B = 1) &= 0.9 \\p(F = 1) &= 0.9\end{align*}$
- 게이지가 full 일때 확률값들
  $\begin{align*}p(G = 1 \mid B = 1, F = 1) &= 0.8 \\p(G = 1 \mid B = 1, F = 0) &= 0.2 \\p(G = 1 \mid B = 0, F = 1) &= 0.2 \\p(G = 1 \mid B = 0, F = 0) &= 0.1\end{align*}$
#### 2번째 그림 ($G$가 관측되었을 때 $G=0$)
$p(G = 0) = \sum_{B \in \{0, 1\}} \sum_{F \in \{0, 1\}} p(G = 0 \mid B, F)p(B)p(F) = 0.315$
$p(G = 0 \mid F = 0) = \sum_{B \in \{0, 1\}} p(G = 0 \mid B, F = 0)p(B) = 0.81$
$p(F = 0 \mid G = 0) = \frac{p(G = 0 \mid F = 0)p(F = 0)}{p(G = 0)} \approx 0.257$
즉, $G=0$으로 관측되면 $F=0$일 확률이 더 올라감 (0.1에서 0.257)
#### 3번째 그림 (추가적으로 $B=0$으로 관측되었음)
$p(F = 0 \mid G = 0, B = 0) = \frac{p(G = 0 \mid B = 0, F = 0)p(F = 0)}{\sum_{F \in \{0, 1\}} p(G = 0 \mid B = 0, F)p(F)} \approx 0.111$
값은 조금 낮아졌지만 사전확률보단 높은 값을 가짐

위와 같은 현상을 **explain away**라고 함
	2번째 : 연료게이지가 비었다는 관측은 연료 탱크가 실제로 비어있을 확률을 증가
	3번째 : 배터리가 방전되었다는 관측은 연료 게이지가 비었다는 관측을 설명(expalin away)함
	즉, 베터리가 방전된 상태가 연료 게이지가 비었다는 결과를 충분히 설명할 수 있으므로, 연료 탱크가 실제로 비어있을 가능성이 줄어듦
	$G$ 대신 $G$의 하위 노드가 관측되어도 동일한 효과를 가짐 (배터리와 연료 탱크 상태가 의존적)
	다르게 말하면, 자식 노드가 관측되지 않으면 부모 노드의 관측이 다른 부모 노드에 영향을 미치지 못함
### 11.2.3 D-separation
d-separation : 방향성 비순환 그래프(Directed Acyclic Graph, DAG)에서 특정 조건부 독립 관계가 성립하는지 판단하는 도구

- 세 가지 집합 $A$, $B$, $C$가 주어졌을 때, $A \perp\!\!\!\perp B \mid C$가 성립하는지 확인하려면
    1. 모든 가능한 경로를 확인 ($A$의 노드에서 $B$의 노드로 가는 경로)
    2. 경로가 차단(blocked)되는 조건
        - (a) 경로의 화살표가 **머리-꼬리(head-to-tail)** 또는 **꼬리-꼬리(tail-to-tail)** 형태로 만나는 경우, 그 노드가 **집합 $C$에 포함됨**
        - (b) 경로의 화살표가 **머리-머리(head-to-head)** 형태로 만나는 경우, 그 노드나 그 노드의 후손(descendant)이 **집합 $C$에 포함되지 않음**
- 모든 경로가 차단되었다면, $A$와 $B$는 $C$에 의해 d-분리(d-separated)되었으며, $A \perp\!\!\!\perp B \mid C$가 성립함

![[11.2.8.png]]
$a$에서 $b$로 가는 경로에 $f$와 $e$가 존재함
#### 예제 (a)
$f$ : tail-to-tail 노드이며 관측되지 않음 -> 경로가 차단되지 않음
$e$ : head-to-head 노드이며 자식 노드가 $C$에 포함됨 -> 경로가 차단되지 않음
따라서 $a \not\perp\!\!\!\perp b \mid c$
#### 예제 (b)
$f$ : tail-to-tail 노드이며 관측됨 -> 경로가 차단됨
$e$ : head-to-head 노드이며, 해당 노드나 자식 노드가 $C$에 포함되지 않음 -> 경로가 차단됨
따라서 $a \perp\!\!\!\perp b \mid f$
#### 매개변수 노드
항상 관측된 노드로 간주함
부모 노드가 없고 tail-to-tail 형태로 경로에 존재하므로 경로를 항상 차단함
#### i.i.d 데이터
훈련 데이터와 매개변수, 예측값이 존재할 때, 매개변수는 훈련 데이터와 예측값 사이의 tail-to-tail 노드임
따라서 매개변수가 주어지면 예측값과 훈련 데이터는 서로 독립임
### 11.2.4 Naive Bayes
분류 모델로, 조건부 독립 가정을 사용해 모델 구조를 단순화함
데이터 $\mathbf{x}$를 관찰하고, 이를 $K$개의 클래스 중 하나로 할당하는 문제
각 클래스 $C_k$에 대해 조건부 확률 $p(\mathbf{x} \mid C_k)$과 사전확률 $p(C_k)$를 정의함

핵심 가정
	클래스 $C_k$가 주어졌을 때, 입력 변수 $\mathbf{x} = \left( \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)} \right)$의 분포가 독립적으로 분해됨
	$p(\mathbf{x} \mid C_k) = \prod_{l=1}^{L} p(\mathbf{x}^{(l)} \mid C_k)$

![[11.2.9.png]]
즉, 클래스 $C_k$는 모든 $\mathbf{x}^{(l)}$와 tail-to-tail 관계를 가지므로 $C_k$가 주어지면 서로 다른 $\mathbf{x}$는 조건부 독립
만약 $C_k$를 marginalize하면 경로가 차단되지 않으므로 일반적으로 $p(\mathbf{x})$가 $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)}$에 대해 분리되지 않음

학습 방법
	라벨이 있는 데이터 셋이 주어짐 $\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}$
	1. 각 클래스 $C_k$에 대해 해당 라벨 데이터로 개별적으로 모델 학습
	2. 클래스의 사전 확률 $p(C_k)$는 각 클래스의 데이터 비율로 설정
	분류 시 베이즈 정리로 확률을 계산
	$p(C_k \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid C_k)p(C_k)}{p(\mathbf{x})}$
	$p(\mathbf{x} \mid C_k)$는 가정에 따라 계산되고, $p(\mathbf{x})$는 아래와 같이 계산됨
	$p(\mathbf{x}) = \sum_{k=1}^{K} p(\mathbf{x} \mid C_k)p(C_k)$
#### 예제 : 2차원 데이터
![[11.2.10.png]]
각 클래스에 대한 조건부 확률은 가우시안 분포로 가정

$p(\mathbf{x} \mid C_k) = p(x_1 \mid C_k)p(x_2 \mid C_k)$이며 $p(\mathbf{x}) = \sum_{k=1}^{K} p(\mathbf{x} \mid C_k)p(C_k)$이므로, 가우시안 혼합 모델이 됨

장점
	1. 고차원 데이터에서 유용 : 입력 차원이 높아도 독립적인 분포로 분해함
	2. 다양한 데이터 타입 처리 가능 : 입력이 이산이거나 연속이거나 모두 가능

한계와 실용성
	강한 조건부 독립 가정 : 클래스 조건부 확률를 부정확하게 표현할 수 있음
	하지만 독립 가정이 부정확해도 실질적인 성능은 우수할 수 있음
	결정 경계가 조건부 독립 가정의 완전성에 민감하지 않기 때문
### 11.2.5 Generative models
많은 머신러닝 문제들은 inverse problem으로 볼 수 있음
inverse problem (역문제) : 관측된 데이터를 이용해 이를 생성한 근본적인 원인이나 과정을 추론하려는 문제
ex.
	물체의 이미지 생성 과정 (순방향 문제)
	물체의 클래스가 특정 분포에서 선택되고, 위치, 방향 등이 다른 분포에서 선택되고, 이 정보로 이미지가 생성
	이미지에서 물체 정보 추론 (역문제)
	이미지에 나타난 물체의 클래스, 위치, 크기, 방향을 추정
#### 판별 모델 (Discriminative Model)
역문제를 해결하기 위한 하나의 접근법
ex. CNN 기반 모델
특징
	직접적으로 역문제를 해결하려 함
	라벨이 존재하는 데이터가 충분하다면 높은 정확도를 보임
	라벨이 없는 데이터는 사용이 불가능하며, 라벨링 작업에 많은 시간이 소요됨
#### 생성 모델 (Generative Model)
데이터를 생성하는 과정을 모델링하고, 이를 역으로 추론하는 접근법
ex. 물체의 클래스, 위치, 크기가 독립으로 가정한 방향성 그래프 모델
![[11.2.11.png]]
이미지가 주어지지 않았으면 head-to-head 구조로 나머지 변수들이 독립임 
이미지가 관측되면 변수들이 더이상 독립이 아니게 됨 (직관적으로, 이미지가 주어지면 위치나 크기와 같은 유용한 정보를 얻음)
특징
	학습 후 가상 데이터 생성이 가능함
	라벨이 없는 데이터 활용 가능

복잡한 joint distribution을 단순한 요소로부터 구성하기 위해 Hidden variable이 쓰이기도 함
ex. VAE, Diffusion Models
### 11.2.6 Markov blanket
마코프 경계(Markov Boundary)라고도 불림
복잡한 방향성 그래프에서 중요한 역할
Markov blanket : 특정 노드를 제외한 나머지 변수들로부터 특정 노드를 조건부 독립으로 만들어주는 최소한의 변수 집합
$$
\begin{align*}
p(\mathbf{x}_i \mid \mathbf{x}_{\{j \neq i\}}) 
&= \frac{p(\mathbf{x}_1, \ldots, \mathbf{x}_D)}{\int p(\mathbf{x}_1, \ldots, \mathbf{x}_D) \, d\mathbf{x}_i} \\
&= \frac{\prod_k p(\mathbf{x}_k \mid \mathrm{pa}(k))}{\int \prod_k p(\mathbf{x}_k \mid \mathrm{pa}(k)) \, d\mathbf{x}_i}
\end{align*}
$$
수식에서 $\mathbf{x}_k$가 $\mathbf{x}_i$에 의존하지 않으면 $p(\mathbf{x}_k \mid \mathrm{pa}(k))$ 소거되므로 다음의 3가지로만 조건부 분포가 결정됨
1. 부모 노드 : 특정 노드에 직접 영향을 미치는 노드
2. 자식 노드 : 특정 노드의 영향을 받는 노드
3. 공동 부모 노드들 : 특정 노드의 자식 노드의 다른 부모 노드들 (explaining away 효과 때문)
![[11.2.12.png]]
역할
	Markov blanket에 포함된 변수만 알면 특정 노드를 나머지 노드들로부터 분리 가능
	그래프 분석에서 변수 간 관계를 간결하게 표현하고, 계산 복잡도를 줄이는 데 도움

#### GPT 예시 1 : 의료 데이터
- 노드 $\mathbf{x}_i$ : 환자의 특정 질병 여부.
- 부모 : 환자의 유전자 정보(질병에 영향을 미침).
- 자식 : 진단 검사 결과(질병 여부로부터 파생).
- 공동 부모 : 환경 요인(질병과 진단 검사 결과에 모두 영향을 미침).
#### GPT 예시 2 : 소셜 네트워크
- 노드 $\mathbf{x}_i$ : 특정 사용자의 행동.
- 부모 : 사용자가 팔로우하는 사람들.
- 자식 : 사용자의 게시물.
- 공동 부모 : 게시물의 공동 작성자.
### 11.2.7 Graphs as filters
![[11.2.13.png]]방향성 그래프를 필터로 이해할 수 있음 : 그래프를 통과한 분포는 조건부 확률 분해와 조건부 독립 관계를 동시에 만족
방향성 그래프는 확률분포를 조건부 확률의 곱으로 분해하는 특징이 있음
또한 d-separation을 통해 특정 조건부 독립 관계를 알 수 있음
#### 필터 1 : 분해 (factorization) 필터
그래프에 따라 분해된 조건부 확률의 곱을 만족하는 분포만 통과
이 필터를 통과한 분포들은 $\mathcal{DF}$ (directed factorization)으로 부름
#### 필터 2 : 조건부 독립 필터
그래프에서 d-separation을 통해 얻어진 모든 조건부 독립 관계를 만족하는 분모만 통과

위 두 필터의 조건은 서로 동등함
즉, 그래프가 표현할 수 있는 분포는 두가지 조건을 동시에 만족함

그래프에 따른 $\mathcal{DF}$
	1. 완전 연결 그래프 : 모든 분포
	2. 완전히 분리된 그래프 : 완전히 분리된 분포 (모두 독립)
	3. 중간 복잡도의 그래프 : 그래프가 표현할 수 있는 분포
# 11.3 Sequence Models
sequence data : 순서가 있는 데이터
ex. 텍스트, 단백질 서열, 시계열

- 자기회귀 모델 (autoregressive model), 완전 연결 모델
![[11.2.14.png]]
$$
p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = \prod_{n=1}^{N} p(\mathbf{x}_n \mid \mathbf{x}_1, \ldots, \mathbf{x}_{n-1})
$$
모든 이전 변수들이 다음 변수의 조건부 분포에 영향을 미침
위 표현이 일반적이지만, 가정이 포함되지 않으면 실제 데이터에 대해 학습이 어렵고 과적합 가능성 존재

- 독립 모델 (Independent Model)
![[11.2.15.png]]
모든 변수 간 독립을 가정하며, 시퀸스의 순서 정보를 완전히 무시함

- 마코프 모델 (Markov Models)
![[11.2.16.png]]
$$
p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = p(\mathbf{x}_1) \prod_{n=2}^{N} p(\mathbf{x}_n \mid \mathbf{x}_{n-1})
$$
각 변수가 바로 이전 변수에만 의존한다고 가정
즉, $\mathbf{x}_n$은 바로 이전 변수 $\mathbf{x}_{n-1}$외의 모든 이전 변수와 조건부 독립

- 2차 마코프 모델 (Second-Order Markov Model)
![[11.2.17.png]]
각 변수가 두 개의 이전 변수에 의존한다고 가정
$$
p(\mathbf{x}_1, \ldots, \mathbf{x}_N) = p(\mathbf{x}_1)p(\mathbf{x}_2 \mid \mathbf{x}_1) \prod_{n=3}^{N} p(\mathbf{x}_n \mid \mathbf{x}_{n-1}, \mathbf{x}_{n-2})
$$
$M$차로 확장 가능한데 커질수록 파라미터의 수가 지수적으로 증가하므로, 큰 $M$은 비효율적
### 11.3.1 Hidden variables
마코프 모델의 한계를 극복하기 위해 hidden variables를 도입해 시퀸스 데이터를 모델링함
관측값마다 대응되는 숨겨진 변수를 도입해 모델의 잠재 구조를 설명함

- 상태 공간 모델 (State-Space Model)
![[11.2.18.png]]
hidden variable끼리 마코프 체인을 형성 : $\mathbf{z}_{n+1} \perp\!\!\!\perp \mathbf{z}_{n-1} \mid \mathbf{z}_n$
관측값은 해당 시점의 hidden variable에만 의존 : $p(\mathbf{x}_n \mid \mathbf{z}_n)$
joint distribution은 다음과 같음
$$
p(\mathbf{x}_1, \ldots, \mathbf{x}_N, \mathbf{z}_1, \ldots, \mathbf{z}_N) 
= p(\mathbf{z}_1) \left[ \prod_{n=2}^{N} p(\mathbf{z}_n \mid \mathbf{z}_{n-1}) \right] 
\left[ \prod_{n=1}^{N} p(\mathbf{x}_n \mid \mathbf{z}_n) \right]
$$
관측값들 사이에는 항상 hidden variable이 존재하므로 서로 조건부 독립성을 가지지 않음
즉, 다음 관측값을 예측하려면 모든 이전 관측값에 의존함

은닉 변수가 이산이면 숨겨진 마코프 모델 (Hidden Markov Model, HMM)
은닉 변수와 관측값 모두 가우시안이면 선형 동적 시스템 (Linear Dynamical System, LDS)
	칼만필터로도 알려짐

모델의 유연성을 위해 $p(\mathbf{x}_n \mid \mathbf{z}_n)$를 DNN으로 대체 가능함
