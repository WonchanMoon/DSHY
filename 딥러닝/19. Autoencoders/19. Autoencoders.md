표현 학습의 한가지 방법으로, 입력과 유사한 출력을 생성하도록 학습됨
크게 2가지 부분으로 나눌 수 있음
1. 인코더(encoder): 입력 $x$를 hidden representation $z(x)$로 변환
2. 디코더(decoder): hidden representation $z(x)$을 다시 출력 $y(z)$로 변환

오토인코더가 단순한 복사 이상의 의미를 가지기 위해서 아래와 같은 제약을 도입함
- $z$의 차원을 $x$의 차원보다 작게 제한. 또는 $z$가 희소 표현을 가지도록 함
- 입력에 노이즈를 주거나 변형시키고 이를 복원하는 방식으로 학습시킴

해당 장에서는 **결정론적 오토인코더(deterministic autoencoder)** 를 다룬 후, 이를 확장하여 **확률적 모델(stochastic model)** 로 일반화
확률적 오토인코더 모델은 **변분 오토인코더(Variational Autoencoder, VAE)** 라고 하며, 이는 비선형 잠재 변수 모델(nonlinear latent variable model)을 학습하는 방법 중 하나임
## 19.1 Deterministic Autoencoders
PCA는 오토인코더의 단순한 형태:
	입력 벡터를 선형 변환을 통해 더 낮은 차원의 공간으로 사영
	해당 결과를 다시 선형 변환으로 원래 공간에서 근사적으로 복원

이때 신경망의 비선형성을 활용해 잠재 공간이 선형 부분 공간이 아닌 형태를 가지게 만드는 **비선형 PCA(Nonlinear PCA)** 를 정의할 수 있음
	입력과 동일한 수(차원)의 출력을 가지는 신경망을 사용
	입력과 출력 간의 재구성 오류(reconstruction error)를 최소화하도록 가중치 최적화

현대 딥러닝에서는 이런 단순한 형태의 오토인코더가 직접적으로 사용되지 않음
	1. 잠재 공간에서 의미론적으로 유의미한 표현을 제공하지 못함
	2. 데이터 분포로부터 직접적으로 새로운 예제를 생성하는 능력이 부족

하지만 VAE와 같은 강력한 생성 모델의 개념적 기초를 제공하는 역할을 하기 때문에 알아둬야함
### 19.1.1 Linear autoencoders
![[19.1.1.png]]
입력 개수 $D$, 출력 유닛 개수 $D$, 은닉 유닛 개수 $M$ < $D$

해당 신경망은 **단순히 입력을 그대로 출력**하는 것
은닉층의 유닛 수가 입력보다 작기 때문에 모든 입력을 완벽히 복원하는 건 불가능함
따라서 입력과 복원된 출력 간 불일치를 최소화하는 방향으로 학습 (MSE)
$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \|\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{x}_n\|^2
$$
	$x_n$: 입력 벡터
	$y(x_n, w)$: 신경망을 통해 복원된 출력
	$N$: 전체 데이터 샘플 수

만약 은닉 유닛이 선형 활성화 함수를 가지면 위 오차 함수는 유일한 전역 최소값을 가짐
해당 최적값에서 네트워크는 데이터를 $M$차원 부분 공간으로 사영하는 역할
$M$차원 부분 공간은 입력 데이터의 상위 $M$개의 주성분에 의해 정의됨

>선형 오토인코더는 PCA와 동일한 역할을 수행

- 은닉층에서 비선형 활성화 함수를 사용하면?
	**Bourlard & Kamp (1988)** 의 연구에 따르면, 비선형 활성화 함수를 사용하더라도 최소 오차 해가 여전히 주성분 부분 공간과 동일함
	따라서 비선형 활성화 함수를 사용하는 2 레이어 NN은 PCA 이상의 이점을 제공하지 않음

- PCA는 SVD를 활용하는게 더 효과적임
	1. 유한한 시간내에 정확한 해를 보장
	2. 고유값과 직교하는 고유벡터를 순서대로 생성
### 19.1.2 Deep autoencoders
![[19.1.2.png]]
- 하지만 비선형 계층이 추가적으로 포함되면 상황이 달라짐
	출력층: 선형
	2번째 은닉층: $M$개의 유닛을 가지며, 선형일 수 있음
	1, 3번째 레이어: 시그모이드 비선형 활성화 함수 사용
	해당 네트워크는 두개의 연속적인 함수로 볼 수 있음 $F_1, F_2$

- 1번째 함수 $F_1$
	기존의 $D$차원 데이터를 $M$차원의 잠재 공간으로 사영
	1번째 은닉층이 비선형 활성화 함수로 해당 매핑은 보다 일반적인 형태를 가짐
- 2번째 함수 $F_2$
	$M$차원 잠재 공간에서 다시 $D$차원 입력 공간으로 매핑

위 과정은 기하학적 의미를 가지고 있음 (아래 그림 참고)
![[19.1.3.png]]

위 네트워크는 사실상 비선형 PCA(Nonlinear PCA)를 수행함
	선형 변환에 국한되지 않는다는 장점
	표준 PCA를 특수한 경우로 포함함
	오차 함수가 더이상 이차 함수가 아니기 때문에 **계산량이 큰 비선형 최적화 기법을 사용해야함** (국소 최소값에 수렴할 수도 있음)
	잠재 공간의 차원 수를 미리 결정해야함
### 19.1.3 Sparse autoencoders
신경망에서 은닉층의 뉴런 개수를 제한하는 대신, 내부 표현에 제약을 가하는 또 다른 방법이 존재 
$\rightarrow$ **정규화 항을 추가**해 희소 표현을 유도함 (중요한 특징만을 학습하게 유도)

가장 간단한 방법은 L1 정규화를 사용하는 것
$$
\tilde{E}(\mathbf{w}) = E(\mathbf{w}) + \lambda \sum_{k=1}^{K} |z_k|
$$
	$E(\mathbf{w})$: 기존의 오차 함수
	$K$: 해당 은닉층의 뉴런 개수
	$z_k$: 은닉층 뉴런의 활성화 값
	$\lambda$: 정규화 강도 조절 파라미터

L1 정규화 항은 모든 은닉층 뉴런의 활성화 값 $z_k$의 절대값 합을 최소화하도록 유도해, 뉴런이 가능한 적게 활성화되도록 강제하는 효과

기존의 정규화는 가중치에 적용되는데 반해, 여기선 **은닉층 뉴런의 활성화 값에 직접 적용**된다는 점에서 차이

정규화 항이 포함된 오차 함수는 자동 미분을 사용해서 기울기 계산 가능함 ([subgradient](https://ko.wikipedia.org/wiki/%ED%95%98%EB%B0%A9%EB%AF%B8%EB%B6%84)) $\rightarrow$ 기존 학습 방식과 동일하게 최적화
### 19.1.4 Denoising autoencoders
기존의 단순한 오토인코더(simple autoencoder)에서 잠재 공간의 차원을 제한해 입력을 그대로 복사하는 방식을 학습하는 것을 방지했음

노이즈 제거 오토인코더(Denoising autoencoders)는 데이터 내부의 구조를 학습하도록 모델을 강제하는 또 다른 접근법
	1. 기존의 입력 벡터 $\mathbf{x}_n$에 노이즈를 추가해 $\tilde{\mathbf{x}}_n$을 만들고 이를 오토인코더 입력으로 사용
	2. 오토인코더의 출력이 기존의 입력 벡터 $\mathbf{x}_n$에 최대한 가깝도록 학습함
$$
E(\mathbf{w}) = \sum_{n=1}^{N} \|\mathbf{y}(\tilde{\mathbf{x}}_n, \mathbf{w}) - \mathbf{x}_n\|^2
$$
	$\mathbf{x}_n$: 기존 입력 벡터
	$\tilde{\mathbf{x}}_n$: 노이즈가 추가된 입력 벡터
	$\mathbf{y}(\tilde{\mathbf{x}}_n, \mathbf{w})$: 오토인코더가 복원한 출력
	$N$: 전체 데이터 샘플 수

모델이 **노이즈를 제거하는 방법을 학습하며 데이터의 구조를 더 깊이 이해**하도록 함
- 노이즈를 추가하는 방법은 대표적으로 2가지가 있음
	1. 랜덤하게 일부 입력 변수를 0으로 설정: 일부 정보가 손실되도록 함. $\nu$를 0~1사이로 조절하며 1에 가까울수록 많은 입력이 0으로 변함
	2. 평균이 0인 독립적인 가우시안 노이즈 추가: $\tilde{\mathbf{x}}_n = {\mathbf{x}}_n + \text{Noise}$, 노이즈의 크기는 분산으로 조절

- 입력이 이미지라고 가정하면
	인접한 픽셀들은 높은 상관관계를 가지므로, 모델이 이를 학습하면 노이즈가 추가된 픽셀을 원본으로 복원할 수 있음
	따라서 네트워크가 **데이터의 구조적 특성을 활용해 손실된 정보를 복원**하는 방법을 학습함

- 노이즈 제거 오토인코더의 학습은 Score Matching과 밀접한 관련이 있음
	Score Function $\mathbf{s}(\mathbf{x}) = \nabla_{\mathbf{x}} \ln p(\mathbf{x})$
	즉, 확률 밀도 함수 $p(\mathbf{x})$가 높은 방향을 가리키는 벡터
	노이즈 제거 오토인코더의 각 데이터 포인트에서 왜곡된 벡터 $\tilde{\mathbf{x}}_n - {\mathbf{x}}_n$를 보정하는 방향으로 학습하는 과정과 유사함
	즉, **노이즈 제거 과정은 데이터 분포의 구조를 학습하는 과정과 동일**한 목표
![[19.1.4.png]]

노이즈 제거 오토인코더는 확산 모델(Diffusion Model)과 연결되며 이후에 깊이 있는 내용이 나옴
### 19.1.5 Masked autoencoders
BERT와 같은 트랜스포머 기반 모델은 입력을 일부 마스킹해 잠재 정보의 복원을 학습하는 방법임
이를 이미지에 적용하려고 한 연구가 **마스킹 오토인코더(Masked Autoencoder, MAE)**
DNN을 사용해 **손상된 입력 이미지를 기반으로 원본 이미지를 복원**하는 방식으로 동작
개념적으로 노이즈 제거 오토인코더와 비슷하지만 **손상의 형태가 마스킹 또는 이미지의 일부를 제거**하는 방식으로 이루어짐
이 기법은 일반적으로 Vision Transformer(ViT) 아키텍처와 결합되어 사용됨
![[19.1.5.png]]
이 경우 마스킹 과정은 단순히 랜덤하게 선택된 토큰만 인코더에 전달하는 방식으로 구현됨

- BERT와의 차이점
	이미지는 언어와 달리 **일부 픽셀을 제거하더라도 전체 의미가 크게 손상되지 않음** 
	$\rightarrow$ 따라서 BERT는 15%정도로 마스킹했다면 MAE는 75% 마스킹이 일반적
	
	BERT에서 마스킹 토큰은 "MASK"인데 MAE에선 입력에서 제외시킴 
	$\rightarrow$ 랜덤하게 선택된 일부 패치들만 가지고 원본을 복원해야해서 계산량을 크게 줄임

- MAE의 디코더는 트랜스포머 기반으로 설계되는데, 트랜스포머의 출력과 입력의 차원이 동일해야하기 때문에 **인코더 출력에서 디코더 입력으로 변환하는 과정이 필요**함
	1. 마스킹된 패치를 다시 추가
	2. 각 패치에 위치 정보를 추가

디코더의 출력 후에 선형 레이어를 통해 픽셀 공간으로 변형해 원본 이미지를 복원하도록 학습함 (MSE loss)
![[19.1.6.png]]

MAE의 궁극적인 목표는 이미지 복원이 아니라, **추론에 사용할 수 있는 강력한 내부 표현을 학습**하는 것
	다운스트림 태스크(분류, 객체 탐지 등)에서 활용될 일반화된 표현을 학습
	이를 위해 학습 후엔 인코더만을 활용함
	즉, 전체 이미지를 인코더에 입력하고 새로운 출력 레이어를 추가해 원하는 태스크에 맞게 fine-tuning함
## 19.2 Variational Autoencoders
- latent-variable 모델의 likelihood function
$$
p(\mathbf{x} \mid \mathbf{w}) = \int p(\mathbf{x} \mid \mathbf{z}, \mathbf{w}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}
$$
	$p(\mathbf{x} \mid \mathbf{z}, \mathbf{w})$: DNN에 의해 정의됨
	해당 식은 잠재변수 $z$에 대한 적분을 계산할 수 없음
Variational autoencoder는 이 문제를 해결하기 위해, 학습 과정에서 위 likelihood function에 대한 근사를 사용함

- VAE의 핵심 아이디어
	1. 근사를 위해 **Evidence Lower Bound** 사용 (EM 알고리즘과 밀접함)
	2. 인코더 네트워크를 통한 **amortized inference**
	3. 인코더 모델 학습을 위한 **reparameterization trick**을 사용 (해당 방법 덕분에 그래디언트를 통해 학습할 수 있음)

- 생성 모델
	$\mathbf{x}\in \mathbb{R}^D$에 대해 조건부 분포 $p(\mathbf{x}\mid \mathbf{z},\mathbf{w})$는 DNN $g(\mathbf{z},\mathbf{w})$의 출력으로 정해짐
	잠재 변수 $\mathbf{z}\in \mathbb{R}^M$은 다음 분포를 따름
	$p(\mathbf{z}) = \mathcal{N}(\mathbf{z}\mid \mathbf{0}, \mathbf{I})$

- ELBO 유도
	임의의 분포 $q(\mathbf{z})$에 대해 다음의 관계가 성립함
$$
\ln p(\mathbf{x} \mid \mathbf{w}) = \mathcal{L}(\mathbf{w}) + \mathrm{KL} \left( q(\mathbf{z}) \,\|\, p(\mathbf{z} \mid \mathbf{x}, \mathbf{w}) \right)
$$
	$\mathcal{L}$: evidence lower bound (ELBO)
	$\mathrm{KL}$: Kullback-Leibler divergence
	
	ELBO는 아래와 같음
$$
\mathcal{L}(\mathbf{w}) = \int q(\mathbf{z}) \ln \left\{ \frac{p(\mathbf{x} \mid \mathbf{z}, \mathbf{w}) p(\mathbf{z})}{q(\mathbf{z})} \right\} \, \mathrm{d}\mathbf{z}
$$
	
	KL divergence는 아래와 같음
$$
\mathrm{KL} \left( q(\mathbf{z}) \,\|\, p(\mathbf{z} \mid \mathbf{x}, \mathbf{w}) \right) = - \int q(\mathbf{z}) \ln \left\{ \frac{p(\mathbf{z} \mid \mathbf{x}, \mathbf{w})}{q(\mathbf{z})} \right\} \, \mathrm{d}\mathbf{z}
$$
	KL divergence는 항상 0 이상이므로 $\ln p(\mathbf{x} \mid \mathbf{w}) \geq \mathcal{L}$
	**log likelihood 자체는 계산이 어렵지만 ELBO는 몬테카를로 추정을 통해 근사적으로 계산할 수 있음**
![[VAE(ELBO).jpg]]

- 데이터셋으로 확장
	데이터 $\mathcal{D} = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\}$가 모델 분포 $p(x)$로부터 독립적으로 샘플링되었다고 가정
	데이터셋 전체에 대한 log likelihood는 아래와 같음
$$
\ln p(\mathcal{D} \mid \mathbf{w}) = \sum_{n=1}^{N} \mathcal{L}_n + \sum_{n=1}^{N} \mathrm{KL} \left( q_n(\mathbf{z}_n) \,\|\, p(\mathbf{z}_n \mid \mathbf{x}_n, \mathbf{w}) \right)
$$
$$
\mathcal{L}_n = \int q_n(\mathbf{z}_n) \ln \left\{ \frac{p(\mathbf{x}_n \mid \mathbf{z}_n, \mathbf{w}) p(\mathbf{z}_n)}{q_n(\mathbf{z}_n)} \right\} \, \mathrm{d}\mathbf{z}_n
$$
	즉, 각 입력 $\mathbf{x}_n$마다 잠재 변수 $\mathbf{z}_n$이 존재하고, 이에 대한 분포 $q_n(\mathbf{z}_n)$을 최적화할 수 있음
	위 수식은 분포 $q_n(\mathbf{z}_n)$에 상관없이 성립하므로, **ELBO를 최대화하는 분포 또는 KL divergence를 최소화하는 분포를 선택**할 수 있음
	
	만약 $q_n(\mathbf{z}_n)=p(\mathbf{z}_n\mid \mathbf{x}_n, \mathbf{w})$라면 KL divergence가 0이 되어 ELBO와 log likelihood는 같아짐 (데이터에 대한 posterior를 설정할 수 있는 경우임)
	아래는 posterior가 어떤 모양을 가지는지 직관적으로 보여줌
	![[19.2.1.png]]
	
	각 데이터 $\mathbf{x}_n$에 대한 정확한 posterior $p(\mathbf{z}_n\mid \mathbf{x}_n, \mathbf{w})$는 베이즈 정리를 통해 아래와 같이 주어짐
$$
p(\mathbf{z}_n \mid \mathbf{x}_n, \mathbf{w}) = \frac{p(\mathbf{x}_n \mid \mathbf{z}_n, \mathbf{w}) \, p(\mathbf{z}_n)}{p(\mathbf{x}_n \mid \mathbf{w})}
$$
	분자는 deep 생성 모델로 쉽게 계산되지만, 분모가 likelihood로 계산이 불가능함
	$\rightarrow$ 따라서 **posterior를 근사할 방법이 필요**함
	
	각 $q_n(\mathbf{z}_n)$에 대해 parameterized 모델을 도입해 수치적으로 최적화시킬 수 있지만 계산 비용이 매우 큼
	
	이를 해결하기 위해 **인코더 네트워크를 도입해 근사적인 추론을 수행**함 (이게 amortized inference로, **하나의 인코더가 모든 $q_n(\mathbf{z}_n)$을 통합적으로 근사하는 방식**임)
	- %%GPT 비유 ㅋㅋ%%
		✅ 비유로 이해해 보기
		- 전통적 VI: 문제집 1000문제를 전부 손으로 풀기
		- Amortized Inference: 풀이 공식(네트워크)을 하나 만들어서 1000문제에 똑같이 적용하기
### 19.2.1 Amortized inference
VAE는 각 데이터 포인트 $\mathbf{x}_n$에 대해 개별적으로 posterior $p(\mathbf{z}_n \mid \mathbf{x}_n, \mathbf{w})$를 계산하는 대신, 인코더 네트워크 (encoder network)라고 불리는 하나의 NN을 학습해 한번에 근사함

이를 amortized inference라고 하며, 인코더 네트워크인 $q(\mathbf{z} \mid \mathbf{x}, \mathbf{\phi})$가 필요함 ($\phi$는 인코더 신경망의 파라미터)

목표 함수는 ELBO로 주어지는데, **네트워크 파라미터 $w$와 인코더 파라미터 $\phi$에 의존**함
$\rightarrow$ **ELBO를 최대화하도록 그래디언트 기반 최적화를 수행**

- VAE는 2개의 신경망으로 구성됨 (다른 파라미터지만 한번에 학습)
	1. 인코더 네트워크 $q(\mathbf{z} \mid \mathbf{x}, \mathbf{\phi})$
		- 입력 데이터 벡터 $\mathbf{x}$를 받아 잠재 공간으로 매핑
	2. 디코더 네트워크 $p(\mathbf{x}\mid \mathbf{z}, \mathbf{w})$ (기존의 생성 모델)
		- 잠재 벡터 $\mathbf{z}$를 받아 데이터 공간으로 다시 매핑
	
	해당 구조는 오토인코더와 비슷하지만, **잠재 공간 상에서 확률 분포가 정의**된다는 점이 다름

- **인코더는 보통 다음의 정규분포를 가정**함
$$
q(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\phi}) = \prod_{j=1}^{M} \mathcal{N} \left( z_j \mid \mu_j(\mathbf{x}, \boldsymbol{\phi}), \sigma_j^2(\mathbf{x}, \boldsymbol{\phi}) \right)
$$
	이는 각 잠재 변수 $\mathbf{z}_j$에 대해, 평균과 분산을 출력하는 인코더 신경망을 의미
	$\mu_j(\mathbf{x}, \boldsymbol{\phi})$: 실수 범위를 가지므로 선형 활성화 함수 가능
	$\sigma_j^2(\mathbf{x}, \boldsymbol{\phi})$: 분산은 항상 0이상이므로 출력층에 $\exp$와 같은 비음수 활성화 함수 사용
- **ELBO가 최대화 되는 방향으로 $\phi$와 $\mathbf{w}$를 함께 학습** (SGD)
	개념적으로는 아래의 그림과 같이 EM알고리즘처럼 번갈아 최적화할 수 있음
	![[19.2.2.png]]
	하지만 EM과 달리 **$\mathbf{w}$가 고정되고 $\phi$를 최적화해도 KL divergence가 0으로 수렴하지 않음**
	인코더 신경망은 posterior를 예측할 수 있는 완벽한 모델이 아니기 때문 (ELBO와 log likelihood 사이에는 항상 잔차가 존재) 
		1. 실제 posterior $p(\mathbf{z}\mid \mathbf{x})$는 일반적으로 정규분포의 곱으로 표현되지 않음
		2. 아무리 큰 신경망이어도 표현 능력에 한계가 존재
		3. 학습 과정은 본질적으로 근사해서 최적화하는 것
	따라서 EM 알고리즘과 ELBO 최적화는 아래와 같은 관계를 가짐
		VAE는 동시에 최적화하므로 EM과는 다른 양상
	![[19.2.3.png]]
### 19.2.2 The reparameterization trick
- 현재 상태에선 **ELBO를 계산하는게 여전히 불가능**함
	ELBO가 잠재 변수들 $\{\mathbf{z}_n\}$에 대한 적분을 포함하고 있고, 적분을 할 함수는 디코더 네트워크 때문에 잠재 변수에 대해 매우 복잡한 의존성을 가지기 때문

- 데이터 포인트 $\mathbf{x}_n$에 대한 ELBO는 다음과 같음
$$
\begin{align*}
\mathcal{L}_n(\mathbf{w}, \boldsymbol{\phi}) &= \int q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi}) \ln \left\{ \frac{p(\mathbf{x}_n | \mathbf{z}_n, \mathbf{w}) p(\mathbf{z}_n)}{q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi})} \right\} \, \mathrm{d}\mathbf{z}_n \\
&= \int q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi}) \ln p(\mathbf{x}_n | \mathbf{z}_n, \mathbf{w}) \, \mathrm{d}\mathbf{z}_n - \mathrm{KL}(q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi}) \| p(\mathbf{z}_n))
\end{align*}
$$

- 두번째 항인 **KL divergence는 다음과 같이 2개의 가우시안 분포 사이의 거리**로 생각할 수 있음 (계산 가능)
$$
\mathrm{KL}\left(q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi}) \| p(\mathbf{z}_n)\right) 
= \frac{1}{2} \sum_{j=1}^{M} \left\{ 1 + \ln \sigma_j^2(\mathbf{x}_n) - \mu_j^2(\mathbf{x}_n) - \sigma_j^2(\mathbf{x}_n) \right\}
$$
- 첫번째 항은 **몬테카를로 방식으로 근사**할 수 있음
$$
\int q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi}) \ln p(\mathbf{x}_n | \mathbf{z}_n, \mathbf{w}) \, \mathrm{d}\mathbf{z}_n 
\simeq \frac{1}{L} \sum_{l=1}^{L} \ln p(\mathbf{x}_n | \mathbf{z}_n^{(l)}, \mathbf{w})
$$
	$\{\mathbf{z}_n^{(l)}\}$: $q(\mathbf{z}_n | \mathbf{x}_n, \boldsymbol{\phi})$에서 샘플링한 값들
	해당 근사방법은 $\mathbf{w}$에 대해서는 미분이 되지만 $\phi$에 대해서는 미분이 안됨 ($\{\mathbf{z}_n^{(l)}\}$은 샘플링되어 고정된 값이기 때문)
	아래 그림은 해당 상황을 시각적으로 보여줌
	![[19.2.4.png]]
	위와 같은 문제를 막기 위해 Reparameterization trick을 사용
- **Reparameterization trick**
	$\epsilon \sim \mathcal{N}(0,I)$일 때, $z =\sigma \cdot \epsilon + \mu$와 같이 표현하면 $z \sim \mathcal{N}(\mu,\sigma^2)$가 됨
	이를 몬테카를로 추정에 적용하면 아래와 같음 %%책 오타%%
$$
z_{nj}^{(l)} = \sigma_j(\mathbf{x}_n, \boldsymbol{\phi})\epsilon_{nj}^{(l)} + \mu_j(\mathbf{x}_n, \boldsymbol{\phi})
$$
	$\mu_j(\mathbf{x}_n, \boldsymbol{\phi}), \sigma_j(\mathbf{x}_n, \boldsymbol{\phi})$: encoder의 출력
	$\epsilon_{nj}^{(l)}\sim \mathcal{N}(0,1)$: 샘플링된 노이즈
	![[19.2.5.png]]
- 최종적으로 VAE의 전체 손실 함수는 다음과 같음
$$
\mathcal{L} = \sum_{n} \left\{ \frac{1}{L} \sum_{l=1}^{L} \log p(\mathbf{x}_n \mid \mathbf{z}_n^{(l)}, \mathbf{w}) 
- \mathrm{KL}(q(\mathbf{z}_n \mid \mathbf{x}_n, \boldsymbol{\phi}) \| p(\mathbf{z}_n)) \right\}
$$
	$\mathbf{z}^{(l)}_n$은 $z_{nj}^{(l)} = \sigma_{nj} \epsilon^{(l)} + \mu_{nj}$로 구성됨 ($\mu_{nj}=\mu_j(\mathbf{x}_n, \phi)$, $\sigma_{nj}=\sigma_j(\mathbf{x}_n, \phi)$)
	보통 $L=1$으로, noisy하게 추정하지만 SGD자체가 noisy하므로 문제x (오히려 학습이 효율적임)

- 학습 과정을 요약하면 아래와 같음
	1. 각 배치의 데이터 포인트에 대해, 인코더 네크워크를 통해 foward propagation을 수행해 잠재 변수의 평균과 분산을 구함
	2. reparameterization trick을 사용해 해당 분포에서 샘플을 뽑음
	3. 디코더 네트워크로 foward해서 ELBO를 계산
	4. 디코더 파라미터 $\mathbf{w}$와 인코더 파라미터 $\phi$에 대해 gradient를 계산해 파라미터 업데이트
![[19.2.6.png]]

- 학습 이후에 생성 과정
	인코더는 버리고, 잠재 공간 $p(z)$에서 샘플링하고 해당 샘플을 디코더에 통과시켜 데이터 공간의 샘플을 생성

- 모델 평가
	log-likelihood를 계산할 수 없기 때문에, **ELBO를 근사치로 사용**
	$q(\mathbf{z} \mid \hat{\mathbf{x}}, \boldsymbol{\phi})$로부터 샘플링하면 $p(\mathbf{z})$에서 샘플링하는거보다 더 정확한 추정 가능

- VAE 변형 모델
	1. 이미지의 경우 인코더는 convolution layer, 디코더는 transpose convolution으로 구성 가능
	2. conditional VAE (cVAE): 조건 변수 $c$를 추가 입력으로 받음 (ex. 물체 이미지를 생성 시 클래스를 지정할 수 있음)

- ELBO의 역할과 문제점
	**ELBO의 역할**: 인코더 분포 $q(\mathbf{z} \mid {\mathbf{x}}, \boldsymbol{\phi})$와 prior $p(\mathbf{z})$를 가까워지게 정규화함
	**문제점 1**: 너무 같아지면 $\mathbf{x}$에 대한 정보가 사라짐 (posterior collapse)
	**문제점 2**: 1번과 반대로 두 분포가 너무 다르면 학습 데이터에서 재구성은 매우 정확하지만 샘플링해서 만든 출력이 품질이 낮을 수 있음
	
	**해결책: KL 항 앞에 조절 계수 $\beta$를 도입**
	재구성이 나쁘면 $\beta$를 줄이고, 샘플링 결과가 나쁘면 다시 키움
	초기엔 작게 두고 점차 증가시키는 annealing schedule도 적용 가능

- 디코더 확장
	현재까지는 디코더가 가우시안 분포의 평균을 출력한다고 가정했지만, **분산까지 출력하거나 더 복잡한 분포의 다양한 파라미터를 출력하도록 모델을 확장**할 수 있음