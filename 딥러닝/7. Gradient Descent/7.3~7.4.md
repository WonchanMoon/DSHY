# 7.3 Convergence

**Learning Rate**(학습률)(보통 $\eta$로 작성)은 수렴속도를 결정하는 parameter로써, 너무 크게 설정하면, 에러 표면의 골자기에서 진동하거나 발산할 위험이 있고, 반면 너무 작게 설정하면 수렴속도가 느려짐
다음 식을 통해 보통 업데이트
$$\alpha_i^\mathrm{new}=(1-\eta\lambda_i)\alpha_i^\mathrm{old}$$
여기서 $\lambda_{i}$ 는 에러함수의 Hessian 행렬의 고유값을 나타냄 (고유값이 여기서 왜나옴?)

결국 수렴을하기 위해서는 다음 조건을 만족해야함
$$|1-\eta\lambda_i|<1$$
따라서 학습률의 상한은 고유값의 가장 큰 값이 결정하게됨
$$\eta < \frac{2}{\lambda_{\max}}$$

상한($\frac{2}{\lambda_{\max}}$)으로  $\eta$를 설정했을때 곡률 비율에 의해 수렴속도가 제한된다. 따라서
$\left(1-\frac{2\lambda_{\min}}{\lambda_{\max}}\right)$ 의 값이 1에 가까워 질수록 수렴 속도가 느려진다.


## Momentum
$$w^{(t)}=w^{(t-1)}+\Delta w^{(t)}$$
수렴속도를 개선하고 진동을 줄이기 위해 모멘텀이라는 개념을 도입한다.
모멘텀읜 쉽게 관성효과를 추가하는 방법으로, 다른 고유값에 따른 진동을 줄이고 수렴 속도를 높이는데 사용된다.
$$\Delta w^{(\tau-1)}=-\eta\nabla E(w^{(\tau-1)})+\mu\Delta w^{(\tau-2)}$$

- $\eta$:학습률
- $\mu{:}$모멘텀 계수(일반적으로$0\leq\mu\leq1$사이의 값)
- $\Delta w^{( \tau - 1) }$:현재 단계에서의 가중치 변화량.

이러한 식을 사용하면 어떠한 효과가 있냐?
- 낮은 곡률에서는 원래 에러 표면이 평탕하여 경사가 크게 변하지 않는데 
$$\begin{aligned}
\Delta\mathbf{w} & =-\eta\nabla E\{1+\mu+\mu^{2}+\ldots\} \\
 & =-\frac{\eta}{1-\mu}\nabla E
\end{aligned}$$
다음과 같이 학습률을 $\eta \rightarrow \frac{\eta}{1-\mu}$ 까지 증가 시킬 수 있다
- 높은 곡률의 경우에는 경사가 급격하게 변해 진동하기가 쉬운데, 관성이 이전 단계의 진동을 상쇄시킴으로 학습률을 줄일 수 있다. 결론적으로 안정적으로 수렴하게 도와줌
- 결론적으로 곡률이 낮은 평면에서는 학습률을 키우고, 곡률이 높은곳에서는 학습률을 낮추는 전략
- 비등방성(여러방향의 곡률의 차이가 많이나는 경우) 에러 표면에서도 효과적으로 작동

### Nesterov Momentem
기존 모멘텀 전략에서 업데이트 순서를 변경하여 성능을 더 향상시키는 방법
원래는  현재 위치에서 기울기를 계산한뒤 모멘텀을 추가했는데,
반대로 이전 모멘텀에 따라 우선적으로 가중치 만큼 이동한 후, 해당 위치에서 기울기를 업데이트하는 방식
$$\Delta\mathbf{w}^{(\tau-1)}=\mu\Delta\mathbf{w}^{(\tau-2)}-\eta\nabla E
\begin{pmatrix}
\mathbf{w}^{(\tau-1)}+\mu\Delta\mathbf{w}^{(\tau-2)}
\end{pmatrix}$$
## Learning rate schedule
- 학습률의 초기에는 큰값, 후반부에는 작은 값을 주는 전략
- 거듭제곱
$$\eta(\tau)=\eta(0)\cdot(1+\tau/s)^c$$
- 지수 감소
$$\eta(\tau)=\eta(0)\cdot c^{\tau/s}$$
c: 감소비율 (0<c<1), s: 감소 속도를 조정

## RMSProp 알고리즘
학습률을 적응적으로 저정하는 방법으로, 특정 파라미터가 큰 변화를 겪는 경우 학습률이 안정적으로 개선된다
### 전략
- 기울기의 제곱 합을 누적하여 학습률을 조정
- 높은 곡률에서는 학습률 감소, 낮은 곡률에서는 학습률을 증가
$$\begin{gathered}
r_{i}^{(\tau)}=\beta r_{i}^{(\tau-1)}+(1-\beta)\left(\frac{\partial E(\mathbf{w})}{\partial w_{i}}\right)^{2} \\
w_{i}^{(\tau)}=w_{i}^{(\tau-1)}-\frac{\eta}{\sqrt{r_{i}^{\tau}}+\delta}\left(\frac{\partial E(\mathbf{w})}{\partial w_{i}}\right)
\end{gathered}$$
- $\beta$: 가중 평균을 위한 모멘텀 상수, $\delta$ : 0이 되는 것을 방지하기위한 아주작은값

## Adam Optimization
모멘텀 전략과 RMSProp전략을 결합한 방식
$$\begin{aligned}
 & s_{i}^{(\tau)}=\beta_{1}s_{i}^{(\tau-1)}+(1-\beta_{1})\left(\frac{\partial E(\mathbf{w})}{\partial w_{i}}\right) \\
 & r_{i}^{(\tau)}=\beta_{2}r_{i}^{(\tau-1)}+(1-\beta_{2})\left(\frac{\partial E(\mathbf{w})}{\partial w_{i}}\right)^{2} \\
 & \widehat{s}_{i}^{(\tau)}=\frac{s_{i}^{(\tau)}}{1-\beta_{1}^{\tau}} \\
 & \widehat{r}_{i}^{\tau}=\frac{r_i^\tau}{1-\beta_2^\tau} \\
 & w_{i}^{(\tau)}=w_{i}^{(\tau-1)}-\eta\frac{\widehat{s_{i}}^{\tau}}{\sqrt{\widehat{r_{i}^{\tau}}}+\delta}.
\end{aligned}$$
- $s_{i}$ 기울기 모멘텀, $\beta$를 2개 사용
- 3, 4번째 단계는 편향 보정단계라고함
	- 학습 초기 단계에서는 기울기의 모멘텀($s_i$ )와 제곱 평균($r_i$)이 아직 충분히 축적되지 않았기 때문에, 계산된 값이 실제 값보다 작아지는 편향이 발생한다고 함 이를 보정하기 위한 장치
	- 홧수 가증가혀 $\tau$ 가 커짐에 따라 $\beta^{\tau}$는 0에 가까워져, 평균값을 얻을 수 있음
	- 초기의 낮은 값이 실제 관찰치와 더 잘 일치하도록 하기 위함

# Normalization

- 다양한 입력 변수들이 매우 다른 범위를 가질때, 경사하강법 훈련을 훨씬 어렵게 만듬
- 보통 표준화를 진행하여 스케일링을 진행
- 훈련 전에 한번만 수행
- train, test 데이터에도 동일하게 진행해야함

### Batch Nomalization
- 배치 정규화는 심층 신경망에서 각 은닉층 변수들의 범위가 크게 달라지는 문제를 해결하기 위한 기법.
- 가중치를 업데이트 할때마데 정규화를 해줄 필요성이 있음
- 기울기가 너무 커지거나, 너무 작아지는 현상을 해결하기위한 목적
- 정규화된 값을 다시 재조정하기위해 다음 식을 통해 변환
$$\tilde{a}_{ni}=\gamma_i\widehat{a}_{ni}+\beta_i$$
- $\gamma, \beta$ 역시 학습의 대상 
-  훈련이 끝난 이후 새로운 데이터를 예측할때, 당연히 미니배치는 없음으로 전체 데이터를 통해 $\mu, \sigma^2$ 를 전체 훈련 데이터를 통해 계산해야하는데, 이는 비효울 적임으로 이동 평균을 활용한다.
$$\begin{gathered}
\overline{\mu}_{i}^{(\tau)}=\alpha\overline{\mu}_{i}^{(\tau-1)}+(1-\alpha)\mu_{i} \\
\overline{\sigma}_{i}^{(\tau)}=\alpha\overline{\sigma}_{i}^{(\tau-1)}+(1-\alpha)\sigma_{i}
\end{gathered}$$

- 배치 정규화는 학습을 안정적이고 빠르게 만들어 주지만 왜그런진 아무도 모르는게 뽀인트

### Layer Normalization
- 네트워크 각 레이어에서 계산된 출력값, 즉 각 데이터 샘플의 활성화값을 정규화하는 것
- 일반적으로 순차적 데이터나 RNN과 같이 순서에 민감한 모델에 적용(해당 모델에서는 배치 정규화가 잘 안된다고함)