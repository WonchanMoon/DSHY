##### \[ICDM'20] Dynamic Graph Collaborative Filtering
### Intuitive Idea
- user/item 노드의 **collaborative information을 명시적으로 고려**하자 (second-order)
	- JODIE는 노드의 second-order를 명시적으로 고려하지 않음
		- sequence-based 모델들도 마찬가지
>JODIE + second order information (collaborative information)
### Motivation
- 기존의 그래프 기반 추천 시스템은 정적인 유저-아이템 관계에 집중하여 추천을 수행하지만, 실제 추천 상황에서는 유저의 선호와 아이템 특성이 시간에 따라 변화함
  $\rightarrow$ 시간적으로 변화하는 관계를 모델링할 수 있는 **동적 추천 시스템이 필요**함
- sequence-based 모델들은 user/item 노드의 collaborative information을 직접 활용하지 않음
### Technical Idea
- 3가지 노드 임베딩 벡터 업데이트 메커니즘을 도입해, collabrative, sequential 관계를 반영
	1. zero-order ‘inheritance’
	2. first-order ‘propagation’
	3. second-order ‘aggregation’
- 가장 최신 임베딩을 사용해 추천에 활용
### Preliminary
#### Dynamic Recommendation
- $U, V$: 유저/아이템 집합
- $I$: 총 상호작용 수
- $S_i=(u_i,v_i,t_i,\mathbf{f}_i)$: $i$번째 유저-아이템 상호작용
	- $i \in \{1,2,\cdots, I\}$
	- $u_i\in U, v_i \in V$
	- $t_i$: time stamp
	- $\mathbf{f}_i$: 상호작용 feature으로, 유저 feature $\mathbf{f}_u$와 아이템 feature $\mathbf{f}_v$를 포함 %%코드 상 같은 값임%%

>현재 상호작용과 과거 기록으로부터 유저, 아이템의 표현을 학습한 다음, 유저가 미래에 상호작용할 가능성이 가장 높은 아이템을 예측
#### Dynamic Graph
- $t$시점까지의 유저-아이템 상호작용으로 만들어진 dynamic graph $\mathcal{G}_t=(\mathcal{V}_t, \mathcal{E}_t)$
	- $\mathcal{V}_t, \mathcal{E}_t$: 노드/엣지 집합
- 추천 상황에서
	- $\mathcal{V}_t$는 모든 유저/아이템을 포함하며, $\mathcal{E}_t$는 $t$시점 이전의 모든 유저-아이템 상호작용의 집합
	- Bipartite graph이며 모든 엣지는 유저 노드와 아이템 노드 사이에 존재
	- $\mathbf{h}^t_u, \mathbf{h}^t_v \in \mathbb{R}^d$: $t$시점의 유저, 아이템 노드의 임베딩 벡터
	- $\mathbf{h}^{t-}_u, \mathbf{h}^{t-}_v \in \mathbb{R}^d$: $t$시점 이전의 가장 최신 유저, 아이템 노드의 임베딩 벡터
- 초기 그래프 $\mathcal{G}_{t_0}=(\mathcal{V}_{t_0}, \mathcal{E}_{t_0})$는 시점 $t_0$에서의 그래프의 상태이며, 고립된 노드 집합이거나 동적 그래프의 스냅샷일 수 있음 (초기 임베딩은 feature 벡터이거나 무작위로 초기화) %%논문은 랜덤%%
- 새로운 상호작용이 그래프에 추가되면 DGCF 모델을 통해 유저와 아이템 임베딩 $\mathbf{h}^t_u, \mathbf{h}^t_v$이 업데이트됨
### Formulation %%사견%%
- Input:
	- 시간에 따라 변화하는 유저-아이템 상호작용 데이터
	- 이를 dynamic bipartite graph $\mathcal{G}_t$로 생각할 수 있음
- Output:
	- 학습된 유저/아이템 임베딩 벡터
	- 이를 통해 미래에 유저가 상호작용할 가능성이 높은 아이템을 예측하는 것이 목표
### Method
#### 0. Overview
![[DGCF_1.png]]1. 새로운 유저-아이템 상호작용에 대해 **임베딩 업데이트 메커니즘을 활용**해 유저/아이템 노드 임베딩을 동시에 업데이트
1. **projection 함수**를 통해 미래의 유저/아이템 노드 임베딩을 예측
2. 예측된 아이템 노드 임베딩과 다른 모든 아이템 노드 임베딩 간의 **L2 거리를 계산**하고, 해당 값이 가장 작은 아이템들을 유저에게 추천함
#### 1. 임베딩 업데이트
- Zero-order 'inheritance'
	- **노드의 이전 상태를 상속받는 형태**
	$$
\begin{align*}
\hat{\mathbf{h}}_u^t &= \theta_u \left( \mathbf{W}_0^u \mathbf{h}_u^{t-} + \mathbf{w}_0 \Delta t_u + \mathbf{W}_0^f \mathbf{f}_u \right) \\
\hat{\mathbf{h}}_v^t &= \theta_v \left( \mathbf{W}_0^v \mathbf{h}_v^{t-} + \mathbf{w}_0 \Delta t_v + \mathbf{W}_0^f \mathbf{f}_v \right)
\end{align*}
	$$
		- $\hat{\mathbf{h}}_u^t, \hat{\mathbf{h}}_v^t$: 유저 $u$와 아이템 $v$의 $t$시점 노드 임베딩 벡터
		- $\mathbf{h}_u^{t-}, \mathbf{h}_v^{t-}$: 유저 $u$와 아이템 $v$의 $t$시점 직전 상호작용 시점 노드 임베딩 벡터
		- $\Delta t_u, \Delta t_v$: 유저 $u$와 아이템 $v$의 $t$시점 직전 상호작용 시점으로부터 현재 $t$시점까지의 시간 변화량 %%scalar로 예상됨%%
		- $\mathbf{f}_u, \mathbf{f}_v$: 유저 $u$와 아이템 $v$의 feature 벡터 (해당 벡터들은 $\mathbf{f}_t$로부터 생성됨)
		- $\mathbf{W}_0 \in \mathbb{R}^{d\times d}, \mathbf{w}_0\in \mathbb{R}^d$: 파라미터 행렬/벡터
		- $\theta_u, \theta_v$: 활성화 함수 (논문은 identity map)
- First-order 'propagation'
	- 유저가 상호작용하는 아이템은 어느정도 해당 유저의 최근 관심사를 반영
	- 반대로, 특정 아이템에 관심을 가지는 유저를 보면 아이템이 어떤 속성을 가졌는지 알 수 있음
	- 따라서, **first-order 이웃 정보를 사용해 노드 임베딩을 학습하는건 필수적!** (현재 상호작용항에만 국한됨)
	$$
	\begin{align*}
\bar{\mathbf{h}}_u^t &= \phi_u \left( \mathbf{W}_1^u \mathbf{h}_v^{t-} + \mathbf{W}_1^f \mathbf{f}_v \right) \\
\bar{\mathbf{h}}_v^t &= \phi_v \left( \mathbf{W}_1^v \mathbf{h}_u^{t-} + \mathbf{W}_1^f \mathbf{f}_u \right)
\end{align*}
	$$
		- $\mathbf{W}_1\in \mathbb{R}^{d \times d}$: 파라미터 행렬
		- $\phi_u, \phi_v$: 활성화 함수 (논문은 identity map)
		- 즉, $t$시점 상호작용의 feature가 유저/아이템 노드 임베딩 벡터를 업데이트하는 데 전파됨 (유저 $u$와 아이템 $v$의 feature 벡터가 $\mathbf{f}_t$로부터 생성되기 때문) 
		  %%코드 보니까 구분 없이 $\mathbf{f}_t$를 사용함%%
- Second-order 'aggregation'
	- 어떤 유저가 과거에 많은 아이템을 샀고 현재 새 아이템을 사려고 할 때, 새 아이템과 이전에 구매했던 아이템들과의 collaborative relation이 존재한다고 추정할 수 있음 (반대로 유저도 동일하게 고려될 수 있음)
	- 따라서 **second-order 이웃 정보를 집계해 노드 임베딩을 학습!** (aggregation)
	$$
	\begin{align*}
\tilde{\mathbf{h}}_u^t &= \zeta_u \left( \mathbf{h}_u^{t-}, \mathbf{h}_{u_1}^{t-}, \mathbf{h}_{u_2}^{t-}, \ldots, \mathbf{h}_{u_n}^{t-} \right) \\
\tilde{\mathbf{h}}_v^t &= \zeta_v \left( \mathbf{h}_v^{t-}, \mathbf{h}_{v_1}^{t-}, \mathbf{h}_{v_2}^{t-}, \ldots, \mathbf{h}_{v_m}^{t-} \right)
\end{align*}
	$$
		- $v_i\in \mathcal{H}^u_v,\quad \text{where}\ \mathcal{H}^u_v=\{v_1,v_2, \ldots, v_m\}$: 유저 $u$와 이전에 상호작용했던 아이템들의 집합 ($v$는 현재 상호작용의 아이템)
		- $u_i\in \mathcal{H}^v_u,\quad \text{where}\ \mathcal{H}^v_u=\{u_1,u_2, \ldots, u_n\}$: 아이템 $v$와 이전에 상호작용했던 유저들의 집합 ($u$는 현재 상호작용의 유저)
		- 데이터셋이 크면 second-order aggregation의 계산 비용이 커지므로 고정된 이웃 수를 사용 (aggregator size) %%코드보니까 sh로 다 다르게 해놓은듯%%
		- 대부분의 상황에서 사용될 수 있는 계산 복잡도를 위해 second-order 정보까지만 사용 (+ oversmoothing의 문제도 방지)
		- $\zeta_u, \zeta_v$: aggregator 함수 
			- Mean aggregator
				- **이웃 정보를 평균내서 집계하자!**
			$$
\begin{align*}
\tilde{\mathbf{h}}_u^t &= \mathbf{h}_u^{t-} + \frac{1}{|\mathcal{H}_u^v|} \sum_{u_i \in \mathcal{H}_u^v} \mathbf{W}_u^m \mathbf{h}_{u_i}^{t-} \\
\tilde{\mathbf{h}}_v^t &= \mathbf{h}_v^{t-} + \frac{1}{|\mathcal{H}_v^u|} \sum_{v_i \in \mathcal{H}_v^u} \mathbf{W}_v^m \mathbf{h}_{v_i}^{t-}
\end{align*}
			$$
				- $\mathbf{W}^m \in \mathcal{R}^{d\times d}$: aggregation 파라미터 행렬
			
			- LSTM aggregator
				- **LSTM을 활용해 순서정보를 반영해 집계하자!**
			$$
			\begin{align*}
\tilde{\mathbf{h}}_u^t &= \mathbf{h}_u^{t-} + \text{LSTM}_u \left( \mathbf{h}_{u_1}^{t-}, \mathbf{h}_{u_2}^{t-}, \ldots, \mathbf{h}_{u_n}^{t-} \right) \\
\tilde{\mathbf{h}}_v^t &= \mathbf{h}_v^{t-} + \text{LSTM}_v \left( \mathbf{h}_{v_1}^{t-}, \mathbf{h}_{v_2}^{t-}, \ldots, \mathbf{h}_{v_m}^{t-} \right)
\end{align*}
			$$
			
			- Graph Attention aggregator (GAT 모델에 영감을 받음)
				- **중요한 이웃 노드에는 큰 가중치를 줘서 집계하자!**
			$$
			\begin{align*}
\tilde{\mathbf{h}}_u^t &= \sum_{u_i \in \mathcal{H}_u^v} \alpha_{ui} \mathbf{h}_{u_i}^{t-} \\
\alpha_{ui} &= \frac{\exp\left(\text{LeakyRelu}(\mathbf{W}_w [\mathbf{h}_u^{t-} \, \| \, \mathbf{h}_{u_i}^{t-}])\right)}
{\sum_{u_i \in \mathcal{H}_u^v} \exp\left(\text{LeakyRelu}(\mathbf{W}_w [\mathbf{h}_u^{t-} \, \| \, \mathbf{h}_{u_i}^{t-}])\right)} \\
\\
\tilde{\mathbf{h}}_v^t &= \sum_{v_i \in \mathcal{H}_v^u} \alpha_{vi} \mathbf{h}_{v_i}^{t-} \\
\alpha_{vi} &= \frac{\exp\left(\text{LeakyRelu}(\mathbf{W}_w [\mathbf{h}_v^{t-} \, \| \, \mathbf{h}_{v_i}^{t-}])\right)}
{\sum_{v_i \in \mathcal{H}_v^u} \exp\left(\text{LeakyRelu}(\mathbf{W}_w [\mathbf{h}_v^{t-} \, \| \, \mathbf{h}_{v_i}^{t-}])\right)}
\end{align*}
			$$
				- $\alpha$: attention 가중치로, 이웃 노드와 중심 노드와의 중요도를 계산해 softmax로 비율로 나타낸 값 (즉, 이웃 노드에 부여된 중요도)
				- $\mathbf{W}_w\in \mathbb{R}^{2d}$: 가중치 행렬 %%왜 여기만 가중치라는 용어를 사용했는지 모르겠음%%
				- $\|$: concatenation 연산
		![[DGCF_2.png]]

- Fusion information
	- **3가지 업데이트 메커니즘을 융합해서 업데이트하자!**
$$
\begin{align*}
\mathbf{h}_u^t &= \mathbf{F}_u \left( \mathbf{W}_u^{zero} \hat{\mathbf{h}}_u^t + \mathbf{W}_u^{first} \bar{\mathbf{h}}_u^t + \mathbf{W}_u^{second} \tilde{\mathbf{h}}_u^t \right) \\
\mathbf{h}_v^t &= \mathbf{F}_v \left( \mathbf{W}_v^{zero} \hat{\mathbf{h}}_v^t + \mathbf{W}_v^{first} \bar{\mathbf{h}}_v^t + \mathbf{W}_v^{second} \tilde{\mathbf{h}}_v^t \right)
\end{align*}
$$
	- $\mathbf{F}_u, \mathbf{F}_v$: fusion 함수 (일반적으로 sigmoid 사용)
	- $\mathbf{W}^{zero}, \mathbf{W}^{first}, \mathbf{W}^{second}\in \mathbb{R}^{d\times d}$: 3가지 업데이트 메커니즘의 영향도를 조절하는 파라미터 행렬
#### 2. 추천
- Evolution formula
	- $t$시점 이후 미래 시점 $t^+$에서 유저 $u$의 노드 임베딩 벡터 projection 방법
		- JODIE와 마찬가지로 유저 노드 임베딩 벡터의 변화가 연속적인 공간 안에서 smooth할 것이라 가정함 (JODIE와 다르게 MLP를 추가함) $\rightarrow$ **시간 간격에 기반한 미래 임베딩 추정**
$$
\hat{\mathbf{h}}_u^{t^+} = \text{MLP}_u \left( \mathbf{h}_u^t \odot \left( \mathbf{1} + \mathbf{w}_t (t^+ - t) \right) \right)
$$
		- $\odot$: 원소별 곱
		- $\mathbf{w}_t\in \mathbb{R}^d$: time-context 파라미터 벡터로, 시간 간격 (scalar)을 벡터로 변환시킴
		- $\mathbf{1}\in\mathbb{R}^d$: 모든 요소가 1인 벡터
		- $\text{MLP}$: Multi-Layer Perceptron
	
	- $t$시점 이후 미래 시점 $t^+$에서 아이템 $v$의 노드 임베딩 벡터 projection 방법
		- **해당 미래 시점 $t^+$에서 상호작용할 유저의 노드 임베딩 벡터와 유저 feature, 아이템 feature에 의존** %%코드에서는 feature가 one-hot 벡터들이며 concat%%
$$
\hat{\mathbf{h}}_v^{t^+} = \text{MLP}_v \left( \mathbf{W}_2 \hat{\mathbf{h}}_u^{t^+} + \mathbf{W}_3 \mathbf{f}_u + \mathbf{W}_4 \mathbf{f}_v \right)
$$
		- $\mathbf{W}_2, \mathbf{W}_3, \mathbf{W}_4$: 가중치 행렬
- Loss function (evolutionary loss)
	- 모델이 만든 임베딩인 $\mathbf{h}^t_v, \mathbf{h}^t_u$와 추정된 ground truth 임베딩인 $\hat{\mathbf{h}}^{t+}_v, \hat{\mathbf{h}}^{t+}_u$를 MSE
	- 오버 피팅을 방지하기 위해 최근 노드 임베딩과 현재 노드 임베딩이 연관이 있도록 함 (짧은 시간 내에는 유저/아이템의 속성이 stable할 거라는 가정)
$$

\mathcal{L} = \sum_{(u, v, t, f) \in \{S_i\}_{i=0}^{I}} \big( 
\left\| \hat{\mathbf{h}}_v^{t^+} - \mathbf{h}_v^t \right\|_2 
+ \lambda_u \left\| \mathbf{h}_u^t - \mathbf{h}_u^{t-} \right\|_2 \\
+ \alpha_v \left\| \mathbf{h}_v^t - \mathbf{h}_v^{t-} \right\|_2 
\big)

$$
		- $\{S_i\}_{i=0}^{I}$: chronological한 순서로 정렬된 상호작용들
		- $\lambda_u,\alpha_v$: smooth 계수 (업데이트 과정을 통해 유저/아이템 임베딩이 너무 멀리 떨어지지 않게 방지) %%둘 다 1%%

- 유저에게 추천을 하기 위해
	- loss 함수로 얻어진 예측된 아이템 임베딩과 모든 다른 최신 아이템 임베딩 간의 $L2$ 거리를 계산
	- 가장 가까운 Top-$k$ 아이템들이 유저에 대한 추천 결과

- BPR loss와 비교해서
	- evolutionary loss는 **시간 정보를 고려**하기 때문에 dynamic recommendation에 더 적합
	- 따라서 해당 loss로 모델링된 유저, 아이템의 변화하는 궤적은 다음 아이템을 **더욱 정확하게 추천**해줄 수 있음

#### 3. 최적화와 학습
- RNN처럼 back-propagation through time (BPTT) 알고리즘을 사용해 학습
- 모델 파라미터는 Adam optimizer로 최적화
- JODIE와 같은 방법으로 배치 생성 (t-batch) $\rightarrow$ 학습 속도 빠르게!
### Experiment
#### Settings
- 데이터셋: 
	- Reddit: 유저와 아이템(= subreddits) 사이의 상호작용(= post)
		- 10,000명의 가장 활동적인 유저와 1,000개의 가장 활동적인 subreddit, 672,447개의 1달간 상호작용
		- post의 텍스트를 LIWC 방법으로 feature 벡터로 변환해 사용
	- Wikipedia edits: 유저(= 편집자)와 아이템(= 편집 페이지) 사이의 상호작용(= 편집)
		- 최소 5개의 편집을 한 편집자와 가장 많이 편집된 1,000개의 편집 페이지, 157,474개의 1달간 상호작용
		- 편집 텍스트를 LIWC 방법으로 feature 벡터로 변환해 사용
	- LastFM: 유저와 아이템(= 곡) 사이의 상호작용(= 듣기)
		- 1,000명의 유저와 1,000개의 가장 많이 들은 곡들, 1,293,103개의 1달간 상호작용
		- 해당 상호작용은 feature가 존재하지 않음
	- 모든 데이터셋은 chronological하게 정렬되어 8:1:1로 train:valid:test를 나눔
- 베이스라인: LSTM, Time-LSTM, RRN, CTDNE, DeepCoevolve, Jodie
- 성능 지표: MRR, Recall@10
- 파라미터 설정:
	- 임베딩 벡터 차원: 128
	- 임베딩 벡터 초기화: 평균 0, 분산 1인 정규 분포에서 랜덤 샘플링
	- 유저와 아이템의 feature: 원-핫 벡터
	- 옵티마이저: Adam (lr= 1e-3)
	- L2 정규화 하이퍼파라미터: 1e-3
	- loss 함수의 $\lambda, \alpha$: 모두 1
	- 베이스라인들의 하이퍼파라미터는 모두 논문에서 기본으로 쓰인 값 사용
#### Q1. 성능 비교
![[DGCF_3.png]]
- 6개의 베이스라인보다 **모두 훨씬 뛰어난 성능**을 보임 (특히, LastFM)
  $\rightarrow$ DGCF가 그래프에서 collaborative information을 명시적으로 고려했기 때문 %%second-order aggregation 말하는 것으로 이해%%
#### Q2. Ablation Study
- DGCF-0: zero-order inheritance 제거
- DGCF-1: first-order propagation 제거
- DGCF-2: second-order aggregation 제거
- DGCF: aggregator 함수로 graph attention을 사용한 DGCF
![[DGCF_4.png]]
- Wiki랑 LastFM에서는 collaborative information이 중요!
- Reddit은 Wiki와 LastFM에 비해 collaborative information이 효과가 없음
- ablation 모델들과 비교해 DGCF가 모든 데이터셋에 대해 최고 성능을 달성 
  $\rightarrow$ **모든 모듈의 효과성을 입증**
#### Q3. Aggregator 함수
![[DGCF_5.png]]
- 모든 데이터셋에서 **graph attention이 가장 뛰어난 성능**을 보임
  $\rightarrow$ 많은 second-order 이웃 노드들 중에서 효과적인 collaborative information을 명시적으로 고르기 때문 (Mean과 LSTM은 모든 이웃들을 동등하게 취급)
#### Q4. 하이퍼파라미터 Study
![[DGCF_6.png]]
- graph attention aggregator의 aggregation size를 20, 40, 60, 80, 100, 120 중에서 선택한 결과, **작을수록 높은 성능**을 보이고 클수록 성능이 낮아짐
  $\rightarrow$ second-order collaborative information의 중복 때문일수도 있음
### Contribution
- dynamic recommendation에서 쓰일 수 있는, novel한 프레임워크를 제안함 (dynamic graph를 기반으로) (**dynamic graph와 추천 시스템을 결합한 첫 시도**)
- 3가지 dynamic 노드 업데이트 메커니즘을 통해 노드 임베딩과 표현들을 학습함
- 실험을 통해 베이스라인들보다 뛰어남을 보임