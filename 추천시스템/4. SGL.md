##### Self-supervised Graph Learning for Recommendation (SIGIR'21)
### 논문의 제목 설명(단어의 의미들 등)
- Self-supervised Graph Learning: 논문에서 새롭게 제시하는 그래프 학습 패러다임
	- Self-Supervised Learning (SSL): 자기 지도 학습으로, 라벨이 없는 데이터를 활용해 의미 있는 표현을 학습하는 기법. 대표적으로 대조 학습(Contrastive Learning)이 있고, 본 논문에서도 이를 활용함

Recommendation: 유저의 선호를 예측해 적절한 아이템을 추천하는 시스템

>추천시스템에서 자기 지도 학습을 활용해 유저-아이템 그래프에서의 표현 학습을 개선
### Problem formulation (풀고자 하는 문제, input, output, 현실에 어떻게 적용될지)
- 풀고자 하는 문제
	**기존의 지도 학습 추천 모델을 보완**하여, 그래프 기반 추천의 표현 학습을 향상
- input
	유저-아이템 상호작용 그래프
- output
	노드의 학습된 임베딩
	이를 통해 유저별 Top-K 추천 아이템 리스트를 만들 수 있음
- 현실에 어떻게 적용될지
	1. 유저-아이템 그래프 구축:
	   유저와 아이템을 노드, 유저의 아이템 클릭/구매 기록을 엣지로 설정
	2. 데이터 증강:
	   Edge Dropout, Node Dropout, Random Walk 중 하나를 통해 그래프 구조를 변형
	3. 학습:
	   기존의 지도 학습 손실인 LightGCN의 BPR 손실에 InfoNCE 손실을 더해, 대조 학습(Contrastive Learning)을 보조 학습으로 사용
	4. 학습된 유저-아이템 임베딩을 통해 추천 리스트 생성:
	   대조 학습을 사용했기 때문에 long-tail 아이템들의 추천 성능이 향상됨
	   즉, 데이터가 부족하거나 노이즈가 많은 환경에서도 강건한 추천이 가능
### 해결하고자하는 문제 (뭐가 문제고 뭘 해결해야한다고 주장?)
GCN 기반 추천 시스템은 다음과 같은 문제점 3가지가 존재
1. 희소한 지도 신호 (Sparse Supervision Signal)
	지도 신호: 유저-아이템 상호작용 데이터
	전체 아이템 중에서 **유저가 실제로 상호작용한 아이템은 극히 일부**이며, **고품질의 임베딩 표현을 학습하는데 충분하지 않음**
	ex. Yelp2018은 density가 0.130%, Amazon-Book은 0.062%, Alibaba-iFashion은 0.007%

2. 편향된 데이터 분포 (Skewed Data Distribution)
	일부 높은 차수(high-dgree) 아이템은 많은 사용자와 상호작용하지만, **long-tail 아이템은 매우 적은 상호작용**만이 존재
	높은 차수의 아이템은 이웃 노드 집계 과정과 지도 손실을 계산할 때 더 자주 등장해 표현 학습에 더 큰 영향을 미침
	즉, **GCN 기반 추천 모델은 높은 차수의 아이템에 편향되기 쉬우며, long-tail 아이템의 추천 성능이 희생됨**

3. 상호작용 데이터 내 노이즈 (Noises in Interactions)
	대부분의 유저 피드백은 명시적(explicit) 피드백 보단 암묵적(implicit) 피드백으로 이루어짐
	ex. 좋아요/싫어요, 평점 보단 클릭, 조회수 등이 훨씬 많음
	
	이때 **암묵적 피드백은 많은 노이즈**가 포함될 수 있음
	ex. 유저가 잘못 클릭한 아이템을 실제로는 싫어할 수 있음
	
	특히, GCN의 이웃 노드 집계 방식에서 **노이즈가 포함됨 상호작용도 학습에 반영되어 학습이 불안정할 수 있음**
### 문제를 해결하기 위해 낸 아이디어
자기 지도 학습(Self-supervised Learning)을 보조 학습으로 활용
>Self-supervised Graph Learning (SGL)
>데이터 증강(Data Augmentation) 후, 같은 노드의 2가지 다른 view에 대해 대조 학습(Contrastive Learning)하는 방식
>즉, 기존의 지도 학습 손실에 대조 학습 손실을 더해 유저/아이템 임베딩이 더 나은 표현을 학습하도록 함

![[SGL_1.png]]#### 해결책
1. 희소한 지도 신호 (Sparse Supervision Signal)
>노드 자체 식별(Node Self-discrimination) 기법을 통해 추가적인 보조 지도 신호(Auxiliary Supervision Signal)를 제공
2. 편향된 데이터 분포 (Skewed Data Distribution)
>데이터 증강 연산자(Augmentation Operators), 특히 엣지 드롭아웃(Edge Dropout)을 활용하여 고차 노드의 연결을 의도적으로 제거해, 고차 노드의 영향력을 감소 (편향 문제 완화)
3. 상호작용 데이터 내 노이즈 (Noises in Interactions)
>여러 개의 노드 뷰(Multiple Views)를 생성하고 학습해 같은 노드의 변형된 표현은 가깝게, 불필요한 노이즈는 제거하는 방향으로 학습
>상호작용 노이즈(Interaction Noise)에 대한 강건성(Robustness) 향상
#### 1. 데이터 증강
각 노드에 대해 2개의 뷰(view)를 생성
- 노드 드롭아웃(Node Dropout): 그래프에서 일부 노드를 $\rho$ 확률로 제거
$$
s_1(\mathcal{G}) = (\mathbf{M'} \odot \mathcal{V}, \mathcal{E}), \quad s_2(\mathcal{G}) = (\mathbf{M''} \odot \mathcal{V}, \mathcal{E})
$$
	$\mathcal{G}$: 그래프
	$\mathcal{V}, \mathcal{E}$: 노드와 엣지 집합
	$\mathbf{M'}, \mathbf{M''}$: 마스킹 벡터로, (1,0,0,1,0)처럼 표현되며 0이 있는 인덱스의 **노드를 제거**하는 효과

- 엣지 드롭아웃(Edge Dropout) -> 그래프에서 일부 엣지를 $\rho$ 확률로 제거
$$
s_1(\mathcal{G}) = (\mathcal{V}, \mathbf{M}_1 \odot \mathcal{E}), \quad s_2(\mathcal{G}) = (\mathcal{V}, \mathbf{M}_2 \odot \mathcal{E})
$$
	$\mathcal{G}$: 그래프
	$\mathcal{V}, \mathcal{E}$: 노드와 엣지 집합
	$\mathbf{M_1}, \mathbf{M_2}$: 마스킹 벡터로, (1,0,0,1,0)처럼 표현되며 0이 있는 인덱스의 **엣지를 제거**하는 효과
- 랜덤 워크(Random Walk): 무작위로 선택된 경로를 기반으로 새로운 그래프 구조 생성
$$
s_1(\mathcal{G}) = (\mathcal{V}, \mathbf{M}_1^{(l)} \odot \mathcal{E}), \quad s_2(\mathcal{G}) = (\mathcal{V}, \mathbf{M}_2^{(l)} \odot \mathcal{E})
$$
	$\mathcal{G}$: 그래프
	$\mathcal{V}, \mathcal{E}$: 노드와 엣지 집합
	$\mathbf{M^{(l)}_1}, \mathbf{M^{(l)}_2}$: 마스킹 벡터로, 레이어 $l$ 마다의 **엣지를 제거**하는 효과

데이터 증강을 위해 **모델 파라미터를 따로 추가하지 않았다**는 점은 언급할 가치가 있음
#### 2. 대조 학습
**동일한 노드의 서로 다른 뷰 간의 유사성을 극대화하고, 다른 노드들의 뷰와의 유사성은 최소화**
- 특정 노드의 서로 다른 뷰 (positive pairs): $\{(\mathbf{z}'_u, \mathbf{z}''_u) \mid u \in \mathcal{U} \}$
- 다른 노드들의 뷰 (negative pairs): $\{(\mathbf{z}'_u, \mathbf{z}''_v) \mid u, v \in \mathcal{U}, u \neq v \}$


- InfoNCE를 사용하며 유저의 contrastive loss는 다음과 같음
$$
\mathcal{L}_{ssl}^{user} = \sum_{u \in \mathcal{U}} -\log \frac{\exp(s(\mathbf{z}'_u, \mathbf{z}''_u) / \tau)}
{\sum_{v \in \mathcal{U}} \exp(s(\mathbf{z}'_u, \mathbf{z}''_v) / \tau)}
$$
	$s$: 두 벡터간 유사도 계산 함수 ex. 코사인 유사도 함수
	$\tau$: 하이퍼파라미터로, softmax 내의 temperature로 알려져 있음. 유사도 값의 스케일을 조절
	**$\tau$가 클수록 유사도끼리의 차이가 줄어 부정 샘플들이 비교적 균등하게 고려됨**
	**$\tau$가 작을수록 유사도끼리의 차이가 커져 부정 샘플 중 유사도가 높은 샘플(hard negative samples)가 큰 영향을 미침**
	분자는 긍정 샘플과의 유사도를 나타내며 크도록 학습됨
	분모는 부정 샘플들과의 유사도가 포함되어 작아지도록 학습됨

- 아이템의 contrastive loss도 위와 같이 계산되며 SSL의 목적 함수는 다음과 같음
$$
\mathcal{L}_{ssl} = \mathcal{L}_{ssl}^{user} + \mathcal{L}_{ssl}^{item}
$$

- 기존의 추천 학습의 손실과 합쳐서 동시에 학습하도록 함 (SSL로 pretrain할수도 있는데 실험 참고)
$$
\mathcal{L} = \mathcal{L}_{main} + \lambda_1 \mathcal{L}_{ssl} + \lambda_2 \|\Theta\|_2^2
$$
	$\mathcal{L}_{main}$: 기존의 지도 학습 손실 함수 ex. BPR loss
	$\lambda_1, \lambda_2$: 각 항의 강도를 조절하는 하이퍼파라미터
	$\Theta$: 모델 파라미터 집합 ($\mathcal{L}_{main}$만 해당. $\mathcal{L}_{SSL}$은 추가 파라미터가 없으니까)
### 그 문제를 해결했다고 실험을 통해 빈틈없이 증명하는지
- 총 3가지 실험:
	1. RQ1: 다른 SOTA CF 모델들과 성능 비교
	2. RQ2: CF에서 SSL이 어떤 이점이 있는가?
	3. RQ3: 다른 설정들은 SGL에 어떤 영향을 주는가? (ablation study)
- 실험 설정:
	데이터셋: Yelp2018, Amazon-Book, Alibaba-iFashion
	데이터 비율: 학습 7, 검증 1, 테스트 2
	평가 지표: Recall@20, NDCG@20
	비교 모델들: NGCF, LightGCN, Multi-VAE, DNN+SSL
	사용 모델들: SGL-ND, SGL-ED, SGL-RW (각각 Node Dropout, Edge Dropout, Random Walk). 모두 LightGCN을 토대로 구현됨
	모두 같은 하이퍼 파라미터: Xavier 초기화, Adam optimizer(lr = 0.001), 배치 사이즈 2048
	SGL은 $\lambda_1: \{0.005, 0.01, 0.05, 0.1, 0.5, 1.0\}, \tau: \{0.1, 0.2, 0.5, 1.0\}, \rho: \{0, 0.1, 0.2, \cdots, 0.5\}$에서 튜닝
#### RQ1-1: Performance Comparison - LightGCN vs SGL-ND, SGL-ED, SGL-RW

![[SGL_2.png]]
- 대부분의 경우, 3가지 SGL이 LightGCN을 크게 앞섬 $\rightarrow$ **추천 task에서 SSL로 보완하는게 뛰어난 효과를 가짐**
- SGL-ED > SGL-RW > SGL-ND 순으로 성능이 좋음 $\rightarrow$ **ED스러운 연산이 그래프의 고유한 패턴을 잘 잡음**
- dense한 데이터셋(Yelp2018, Amazon-Book)에서 SGL-ED > SGL-RW $\rightarrow$ **낮은 차수 노드들의 연결을 ED가 더 잘 막기 때문**
- SGL-ND는 레이어가 깊어질수록 상대적으로 성능이 불안정함 $\rightarrow$ **높은 차수의 노드를 drop하면 그래프의 구조가 크게 바뀌기 때문**
- sparse한 데이터셋(Amazon-Book과 Alibaba-iFashion)에서 성능 향상이 큼 $\rightarrow$ **SGL이 representation learning을 도와줬기 때문**
- SGL은 레이어가 깊어질수록 성능이 증가함 $\rightarrow$ 서로 다른 노드들 간 대조 학습이 oversmoothing 문제를 해결 + overfitting 방지
#### RQ1-2: Performance Comparison - SOTAs vs SGL-ED

![[SGL_3.png]]
- SGL-ED가 제일 성능이 좋음 $\rightarrow$ **SSL을 추천시스템에 적용하는게 합리적이고 효율적임**
- DNN+SSL이 Amazon-Book에서는 가장 강력한 baseline인데 SGL-ED보단 낮은 성능 $\rightarrow$ **ID 임베딩에 직접 SSL을 적용하는건 그래프 표현에 적용하는 것보다 최적이 아님**
- 유의성 검정을 통해 0.05 유의수준에서 SGL-ED가 **가장 강력한 베이스라인보다 성능이 향상**됨을 확인
#### RQ2-1: Benefits of SGL - Long-tail Recommendation

![[SGL_4.png]]
GroupID가 높을수록 높은 차수 아이템을 가짐(인기 있는 아이템들)
각 그룹은 같은 수의 상호작용을 가짐

- SGL이 LightGCN에 비해 sparse한 아이템을 보다 정확하게 추천함 $\rightarrow$ SSL을 활용한 보조 학습이 **sparse한 아이템에 대해 더 나은 표현을 학습**하는데 도움이 됨을 입증
#### RQ2-2: Benefits of SGL - Training Efficiency

![[SGL_5.png]]
- SGL이 LightGCN보다 훨씬 빠르게 수렴 (SGL은 18, 16번째 epoch, LightGCN은 720, 700번째 epoch이 최고 성능) $\rightarrow$ **InfoNCE 손실을 사용해 다수의 부정 샘플들(negative samples)로부터 표현 학습** + **적절한 $\tau$로 어려운 부정 샘플들을 활용해 의미있는 그래디언트 제공**
- BPR 손실보다 recall이 먼저 좋아짐 $\rightarrow$ 랭킹 task와 BPR 손실 간에 간격이 존재 (후속 연구)
#### RQ2-3: Benefits of SGL - Robustness to Noisy Interactions

![[SGL_6.png]]
유저-아이템 간의 부정적인 상호작용(negative interactions)을 5%, 10%, 15%, 20% 추가
테스트는 그대로 유지 (adversarial examples 추가)

- SGL의 성능 저하폭이 LightGCN보다 낮으며, 노이즈 비율이 증가할수록 성능 차이가 뚜렷해짐 $\rightarrow$ **SGL이 서로 다른 증강된 노드들의 뷰들을 비교하면서 유용한 패턴을 추출하고 특정 엣지에 대한 의존도를 줄임**
- Amazon-Book에선 SGL에 20% 노이즈를 준 상태에서도, 노이즈가 없는 LightGCN보다 우수함 $\rightarrow$ **SGL이 LightGCN보다 더 강력하고 강건한 모델**
- SGL이 Yelp2018에서 더욱 강건함 $\rightarrow$ Amazon-Book이 더 희소해 노이즈의 영향을 크게 받았을 것
#### RQ3-1: Study of SGL - Effect of Temperature $\tau$

![[SGL_7.png]]
- $\tau$를 증가시키면 성능이 저하되며, 성능이 수렴하기까지 더 많은 epoch이 필요함 $\rightarrow$ **어려운 부정 샘플과 쉬운 부정 샘플을 효과적으로 구분하는 능력이 부족해지기 때문**
- $\tau$를 너무 작게 설정하면 모델 성능이 저하됨 $\rightarrow$ **소수의 어려운 부정 샘플들이 과도하게 강조되어 다른부정 샘플들은 제대로 학습되지 않음**

>$\tau$ 값을 0.1~1.0 범위에서 신중하게 조절할 것을 권장
#### RQ3-2: Study of SGL - Effect of Pre-training

![[SGL_8.png]]
SGL-pre: SSL만을 사전학습 후 지도학습으로 모델을 fine-tune

- SGL-pre가 LightGCN보다 우수 $\rightarrow$ **SSL이 LightGCN의 초기 임베딩을 개선**
- SGL-ED가 SGL-pre보다 우수 $\rightarrow$ **지도 학습과 SSL을 통한 보조 학습이 상호 보완적으로 표현을 향상시킴**
#### RQ3-3: Study of SGL - Effect of Negatives
 Table 5 참고
 SGL-ED-batch: 노드 타입을 구별하고 배치 내에서 유저는 유저끼리, 아이템은 아이템끼리 부정 샘플 취급함
 SGL-ED-merge: 노드 타입을 구별하지 않고 배치 내 모든 노드를 부정 샘플로 간주함

- SGL-ED-batch가 SGL-ED-merge보다 더 나은 성능 $\rightarrow$ **유저와 아이템을 구분하는게 더 효과적**
- SGL-ED-batch가 전체 공간에서 부정 샘플링한 SGL-ED와 거의 비슷한 성능 $\rightarrow$ **SSL을 배치 단위로 학습하면 효율적일 것**

### 시사점
- 기존 그래프 기반 추천은 지도 학습 방식(Supervised Learning)으로 수행했지만, 본 연구는 **자기지도 학습(Self-supervised Learning, SSL)을 적용**하여 새로운 연구 방향을 제시
- 기존 SSL 연구들은 일반 그래프를 대상으로 했지만, 본 연구는 **이분 그래프(Bipartite Graph)를 고려**하는 새로운 접근법을 제시